{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "Mean Absolute Error (MAE): 4.62401640535372\n",
      "Mean Squared Error (MSE): 42.475506174392336\n",
      "Root Mean Squared Error (RMSE): 6.517323543786386\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "\n",
    "# Fetch historical data from Yahoo Finance\n",
    "ticker = 'TSLA'  # Tesla stock ticker\n",
    "start_date = '2020-01-01'  # Start date of historical data\n",
    "end_date = '2022-01-01'  # End date of historical data\n",
    "\n",
    "data = yf.download(ticker, start=start_date, end=end_date)\n",
    "\n",
    "# Calculate moving averages\n",
    "data['MA_5'] = data['Close'].rolling(window=5).mean()\n",
    "data['MA_10'] = data['Close'].rolling(window=10).mean()\n",
    "data['MA_20'] = data['Close'].rolling(window=20).mean()\n",
    "\n",
    "# Prepare the data\n",
    "X = data[['Open', 'Volume', 'MA_5', 'MA_10', 'MA_20']]  # Include moving averages as features\n",
    "# save as csv\n",
    "X.to_csv('tesla.csv')\n",
    "y = data['Close']  # Target column\n",
    "\n",
    "# Fill missing values with mean\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X = imputer.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "Mean Absolute Error (MAE): 4.289542806120915\n",
      "Mean Squared Error (MSE): 39.075672281367645\n",
      "Root Mean Squared Error (RMSE): 6.251053693687781\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "import talib\n",
    "\n",
    "# Fetch historical data from Yahoo Finance\n",
    "ticker = 'TSLA'  # Tesla stock ticker\n",
    "start_date = '2019-01-01'  # Start date of historical data\n",
    "end_date = '2023-06-01'  # End date of historical data\n",
    "\n",
    "data = yf.download(ticker, start=start_date, end=end_date)\n",
    "\n",
    "# Calculate moving averages\n",
    "data['MA_5'] = data['Close'].rolling(window=5).mean()\n",
    "data['MA_10'] = data['Close'].rolling(window=10).mean()\n",
    "data['MA_20'] = data['Close'].rolling(window=20).mean()\n",
    "\n",
    "# Calculate RSI\n",
    "rsi_period = 14  # RSI calculation period\n",
    "data['RSI'] = talib.RSI(data['Close'], timeperiod=rsi_period)\n",
    "\n",
    "# Prepare the data\n",
    "X = data[['Open', 'Volume', 'MA_5', 'MA_10', 'MA_20', 'RSI']]  # Include RSI as a feature\n",
    "# save as csv\n",
    "X.to_csv('tesla.csv')\n",
    "y = data['Close']  # Target column\n",
    "\n",
    "# Handle missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X = imputer.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "Mean Absolute Error (MAE): 4.4904829373199675\n",
      "Mean Squared Error (MSE): 48.55235299073955\n",
      "Root Mean Squared Error (RMSE): 6.967951850489464\n",
      "[262.7364754]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fn/kl8cvg_x4659hhy0mdqymlmw0000gp/T/ipykernel_18703/36570073.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X['MACD'] = macd\n",
      "/var/folders/fn/kl8cvg_x4659hhy0mdqymlmw0000gp/T/ipykernel_18703/36570073.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X['MACD_Signal'] = macdsignal\n",
      "/var/folders/fn/kl8cvg_x4659hhy0mdqymlmw0000gp/T/ipykernel_18703/36570073.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X['MACD_Histogram'] = macdhist\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "import talib\n",
    "\n",
    "# Fetch historical data from Yahoo Finance\n",
    "ticker = 'TSLA'  # Tesla stock ticker\n",
    "start_date = '2019-01-01'  # Start date of historical data\n",
    "end_date = '2023-06-23'  # End date of historical data\n",
    "\n",
    "data = yf.download(ticker, start=start_date, end=end_date)\n",
    "\n",
    "# Calculate moving averages\n",
    "data['MA_5'] = data['Close'].rolling(window=5).mean()\n",
    "data['MA_10'] = data['Close'].rolling(window=10).mean()\n",
    "data['MA_20'] = data['Close'].rolling(window=20).mean()\n",
    "\n",
    "# Calculate RSI\n",
    "rsi_period = 14  # RSI calculation period\n",
    "data['RSI'] = talib.RSI(data['Close'], timeperiod=rsi_period)\n",
    "\n",
    "# Calculate MACD\n",
    "macd, macdsignal, macdhist = talib.MACD(data['Close'])\n",
    "\n",
    "# Prepare the data\n",
    "X = data[['Open', 'Volume', 'MA_5', 'MA_10', 'MA_20', 'RSI']]\n",
    "X['MACD'] = macd\n",
    "X['MACD_Signal'] = macdsignal\n",
    "X['MACD_Histogram'] = macdhist\n",
    "\n",
    "X.to_csv('tesla.csv')\n",
    "y = data['Close']  # Target column\n",
    "\n",
    "# Handle missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X = imputer.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "\n",
    "# predict this 2023-06-22,250.77000427246094,166425500,262.99199829101565,255.9549987792969,230.50799942016602,74.55671818997965,21.78733996274059,19.354082532247062,2.433257430493526\n",
    "sample = model.predict([[260.6100427246094,170876500,262.99199829101565,255.9549987792969,230.50799942016602,74.55671818997965,21.78733996274059,19.354082532247062,2.433257430493526]])\n",
    "print(sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "Predicted price for the next day: 258.73132885483517\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "import talib\n",
    "\n",
    "# Fetch historical data from Yahoo Finance\n",
    "ticker = 'TSLA'  # Tesla stock ticker\n",
    "start_date = '2019-01-01'  # Start date of historical data\n",
    "end_date = '2023-07-01'  # End date of historical data\n",
    "\n",
    "data = yf.download(ticker, start=start_date, end=end_date)\n",
    "\n",
    "# Calculate moving averages\n",
    "data['MA_5'] = data['Close'].rolling(window=5).mean()\n",
    "data['MA_10'] = data['Close'].rolling(window=10).mean()\n",
    "data['MA_20'] = data['Close'].rolling(window=20).mean()\n",
    "\n",
    "# Calculate RSI\n",
    "rsi_period = 14  # RSI calculation period\n",
    "data['RSI'] = talib.RSI(data['Close'], timeperiod=rsi_period)\n",
    "\n",
    "# Prepare the data\n",
    "X = data[['Open', 'Volume', 'MA_5', 'MA_10', 'MA_20', 'RSI']].iloc[:-1]  # Adjusted for predicting next day's price\n",
    "y = data['Close'].shift(-1)[:-1]  # Adjusted for predicting next day's price\n",
    "\n",
    "# Handle missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X = imputer.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions for the next day's price\n",
    "latest_data = X[-1].reshape(1, -1)  # Reshape the data to match the model's input shape\n",
    "next_day_prediction = model.predict(latest_data)\n",
    "print(\"Predicted price for the next day:\", next_day_prediction[0])\n",
    "\n",
    "# Save the data in a CSV file\n",
    "data.to_csv('tesla_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "Mean Absolute Error (MAE): 4.286388186144539\n",
      "Mean Squared Error (MSE): 37.98815460204785\n",
      "Root Mean Squared Error (RMSE): 6.163453139437976\n",
      "Predicted price for the next day: 416.697571995914\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "import talib\n",
    "\n",
    "# Fetch historical data from Yahoo Finance\n",
    "ticker = 'NVDA'  # Tesla stock ticker\n",
    "start_date = '2019-01-01'  # Start date of historical data\n",
    "end_date = '2023-07-03'  # End date of historical data\n",
    "\n",
    "data = yf.download(ticker, start=start_date, end=end_date)\n",
    "\n",
    "# Calculate moving averages\n",
    "data['MA_5'] = data['Close'].rolling(window=5).mean()\n",
    "data['MA_10'] = data['Close'].rolling(window=10).mean()\n",
    "data['MA_20'] = data['Close'].rolling(window=20).mean()\n",
    "\n",
    "# Calculate RSI\n",
    "rsi_period = 14  # RSI calculation period\n",
    "data['RSI'] = talib.RSI(data['Close'], timeperiod=rsi_period)\n",
    "\n",
    "# Prepare the data\n",
    "X = data[['Open', 'Volume', 'MA_5', 'MA_10', 'MA_20', 'RSI']].iloc[:-1]  # Adjusted for predicting next day's price\n",
    "y = data['Close'].shift(-1)[:-1]  # Adjusted for predicting next day's price\n",
    "\n",
    "# Handle missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X = imputer.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "\n",
    "# Make predictions for the next day's price\n",
    "latest_data = X[-1].reshape(1, -1)  # Reshape the data to match the model's input shape\n",
    "next_day_prediction = model.predict(latest_data)\n",
    "print(\"Predicted price for the next day:\", next_day_prediction[0])\n",
    "\n",
    "# Save the data in a CSV file\n",
    "data.to_csv('nvidia_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "Predicted price for the next day: 201.6316158463586\n",
      "Train Set - Mean Absolute Error (MAE): 6.573327434196004\n",
      "Train Set - Mean Squared Error (MSE): 91.47892603176547\n",
      "Train Set - Root Mean Squared Error (RMSE): 9.564461617454768\n",
      "Test Set - Mean Absolute Error (MAE): 6.847398953174948\n",
      "Test Set - Mean Squared Error (MSE): 96.98906564669882\n",
      "Test Set - Root Mean Squared Error (RMSE): 9.848302678466926\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "import talib\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Fetch historical data from Yahoo Finance\n",
    "ticker = 'TSLA'  # Tesla stock ticker\n",
    "start_date = '2019-01-01'  # Start date of historical data\n",
    "end_date = '2023-06-01'  # End date of historical data\n",
    "\n",
    "data = yf.download(ticker, start=start_date, end=end_date)\n",
    "\n",
    "# Calculate moving averages\n",
    "data['MA_5'] = data['Close'].rolling(window=5).mean()\n",
    "data['MA_10'] = data['Close'].rolling(window=10).mean()\n",
    "data['MA_20'] = data['Close'].rolling(window=20).mean()\n",
    "\n",
    "# Calculate RSI\n",
    "rsi_period = 14  # RSI calculation period\n",
    "data['RSI'] = talib.RSI(data['Close'], timeperiod=rsi_period)\n",
    "\n",
    "# Prepare the data\n",
    "X = data[['Open', 'Volume', 'MA_5', 'MA_10', 'MA_20', 'RSI']].iloc[:-1]  # Adjusted for predicting next day's price\n",
    "y = data['Close'].shift(-1)[:-1]  # Adjusted for predicting next day's price\n",
    "\n",
    "# Handle missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X = imputer.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions for the next day's price\n",
    "latest_data = X[-1].reshape(1, -1)  # Reshape the data to match the model's input shape\n",
    "next_day_prediction = model.predict(latest_data)\n",
    "print(\"Predicted price for the next day:\", next_day_prediction[0])\n",
    "\n",
    "# Evaluate the model\n",
    "predictions_train = model.predict(X_train)\n",
    "predictions_test = model.predict(X_test)\n",
    "\n",
    "mae_train = mean_absolute_error(y_train, predictions_train)\n",
    "mse_train = mean_squared_error(y_train, predictions_train)\n",
    "rmse_train = np.sqrt(mse_train)\n",
    "\n",
    "mae_test = mean_absolute_error(y_test, predictions_test)\n",
    "mse_test = mean_squared_error(y_test, predictions_test)\n",
    "rmse_test = np.sqrt(mse_test)\n",
    "\n",
    "print(\"Train Set - Mean Absolute Error (MAE):\", mae_train)\n",
    "print(\"Train Set - Mean Squared Error (MSE):\", mse_train)\n",
    "print(\"Train Set - Root Mean Squared Error (RMSE):\", rmse_train)\n",
    "print(\"Test Set - Mean Absolute Error (MAE):\", mae_test)\n",
    "print(\"Test Set - Mean Squared Error (MSE):\", mse_test)\n",
    "print(\"Test Set - Root Mean Squared Error (RMSE):\", rmse_test)\n",
    "\n",
    "\n",
    "\n",
    "# Save the data in a CSV file\n",
    "data.to_csv('tesla_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "Predicted price for the next day: 202.2122584429046\n",
      "Train Set - Mean Absolute Error (MAE): 6.645599040889766\n",
      "Train Set - Mean Squared Error (MSE): 90.54410288934328\n",
      "Train Set - Root Mean Squared Error (RMSE): 9.51546650928599\n",
      "Test Set - Mean Absolute Error (MAE): 6.8954445416797325\n",
      "Test Set - Mean Squared Error (MSE): 94.56958501388908\n",
      "Test Set - Root Mean Squared Error (RMSE): 9.7246894559101\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "from talib import RSI, MACD\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Fetch historical data from Yahoo Finance\n",
    "ticker = 'TSLA'  # Tesla stock ticker\n",
    "start_date = '2019-01-01'  # Start date of historical data\n",
    "end_date = '2023-06-01'  # End date of historical data\n",
    "\n",
    "data = yf.download(ticker, start=start_date, end=end_date)\n",
    "\n",
    "# Calculate moving averages\n",
    "data['MA_5'] = data['Close'].rolling(window=5).mean()\n",
    "data['MA_10'] = data['Close'].rolling(window=10).mean()\n",
    "data['MA_20'] = data['Close'].rolling(window=20).mean()\n",
    "\n",
    "# Calculate RSI\n",
    "rsi_period = 14  # RSI calculation period\n",
    "data['RSI'] = RSI(data['Close'], timeperiod=rsi_period)\n",
    "\n",
    "# Calculate MACD\n",
    "macd, macd_signal, _ = MACD(data['Close'])\n",
    "data['MACD'] = macd\n",
    "\n",
    "# Prepare the data\n",
    "X = data[['Open', 'Volume', 'MA_5', 'MA_10', 'MA_20', 'RSI', 'MACD']].iloc[:-1]  # Adjusted for predicting next day's price\n",
    "y = data['Close'].shift(-1)[:-1]  # Adjusted for predicting next day's price\n",
    "\n",
    "# Handle missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X = imputer.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions for the next day's price\n",
    "latest_data = X[-1].reshape(1, -1)  # Reshape the data to match the model's input shape\n",
    "next_day_prediction = model.predict(latest_data)\n",
    "print(\"Predicted price for the next day:\", next_day_prediction[0])\n",
    "\n",
    "# Evaluate the model\n",
    "predictions_train = model.predict(X_train)\n",
    "predictions_test = model.predict(X_test)\n",
    "\n",
    "mae_train = mean_absolute_error(y_train, predictions_train)\n",
    "mse_train = mean_squared_error(y_train, predictions_train)\n",
    "rmse_train = np.sqrt(mse_train)\n",
    "\n",
    "mae_test = mean_absolute_error(y_test, predictions_test)\n",
    "mse_test = mean_squared_error(y_test, predictions_test)\n",
    "rmse_test = np.sqrt(mse_test)\n",
    "\n",
    "print(\"Train Set - Mean Absolute Error (MAE):\", mae_train)\n",
    "print(\"Train Set - Mean Squared Error (MSE):\", mse_train)\n",
    "print(\"Train Set - Root Mean Squared Error (RMSE):\", rmse_train)\n",
    "print(\"Test Set - Mean Absolute Error (MAE):\", mae_test)\n",
    "print(\"Test Set - Mean Squared Error (MSE):\", mse_test)\n",
    "print(\"Test Set - Root Mean Squared Error (RMSE):\", rmse_test)\n",
    "\n",
    "# Save the data in a CSV file\n",
    "data.to_csv('tesla_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "Predicted price for the next day: 203.06641370485553\n",
      "Train Set - Mean Absolute Error (MAE): 6.60241081755458\n",
      "Train Set - Mean Squared Error (MSE): 87.23357688624135\n",
      "Train Set - Root Mean Squared Error (RMSE): 9.339891695637661\n",
      "Test Set - Mean Absolute Error (MAE): 6.740538018404317\n",
      "Test Set - Mean Squared Error (MSE): 92.76886151994653\n",
      "Test Set - Root Mean Squared Error (RMSE): 9.631659333673847\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "from talib import RSI, MACD, STOCH\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Fetch historical data from Yahoo Finance\n",
    "ticker = 'TSLA'  # Tesla stock ticker\n",
    "start_date = '2019-01-01'  # Start date of historical data\n",
    "end_date = '2023-06-01'  # End date of historical data\n",
    "\n",
    "data = yf.download(ticker, start=start_date, end=end_date)\n",
    "\n",
    "# Calculate moving averages\n",
    "data['MA_5'] = data['Close'].rolling(window=5).mean()\n",
    "data['MA_10'] = data['Close'].rolling(window=10).mean()\n",
    "data['MA_20'] = data['Close'].rolling(window=20).mean()\n",
    "\n",
    "# Calculate RSI\n",
    "rsi_period = 14  # RSI calculation period\n",
    "data['RSI'] = RSI(data['Close'], timeperiod=rsi_period)\n",
    "\n",
    "# Calculate MACD\n",
    "macd, macd_signal, _ = MACD(data['Close'])\n",
    "data['MACD'] = macd\n",
    "\n",
    "# Calculate Stochastic Oscillator\n",
    "slowk, slowd = STOCH(data['High'], data['Low'], data['Close'])\n",
    "data['SlowK'] = slowk\n",
    "data['SlowD'] = slowd\n",
    "\n",
    "# Prepare the data\n",
    "X = data[['Open', 'Volume', 'MA_5', 'MA_10', 'MA_20', 'RSI', 'MACD', 'SlowK', 'SlowD']].iloc[:-1]  # Adjusted for predicting next day's price\n",
    "y = data['Close'].shift(-1)[:-1]  # Adjusted for predicting next day's price\n",
    "\n",
    "# Handle missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X = imputer.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions for the next day's price\n",
    "latest_data = X[-1].reshape(1, -1)  # Reshape the data to match the model's input shape\n",
    "next_day_prediction = model.predict(latest_data)\n",
    "print(\"Predicted price for the next day:\", next_day_prediction[0])\n",
    "\n",
    "# Evaluate the model\n",
    "predictions_train = model.predict(X_train)\n",
    "predictions_test = model.predict(X_test)\n",
    "\n",
    "mae_train = mean_absolute_error(y_train, predictions_train)\n",
    "mse_train = mean_squared_error(y_train, predictions_train)\n",
    "rmse_train = np.sqrt(mse_train)\n",
    "\n",
    "mae_test = mean_absolute_error(y_test, predictions_test)\n",
    "mse_test = mean_squared_error(y_test, predictions_test)\n",
    "rmse_test = np.sqrt(mse_test)\n",
    "\n",
    "print(\"Train Set - Mean Absolute Error (MAE):\", mae_train)\n",
    "print(\"Train Set - Mean Squared Error (MSE):\", mse_train)\n",
    "print(\"Train Set - Root Mean Squared Error (RMSE):\", rmse_train)\n",
    "print(\"Test Set - Mean Absolute Error (MAE):\", mae_test)\n",
    "print(\"Test Set - Mean Squared Error (MSE):\", mse_test)\n",
    "print(\"Test Set - Root Mean Squared Error (RMSE):\", rmse_test)\n",
    "\n",
    "# Save the data in a CSV file\n",
    "data.to_csv('tesla_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "Predicted price for the next day: 265.34280208659004\n",
      "Train Set - Mean Absolute Error (MAE): 6.7343096945883705\n",
      "Train Set - Mean Squared Error (MSE): 89.54498467965786\n",
      "Train Set - Root Mean Squared Error (RMSE): 9.462821179735876\n",
      "Test Set - Mean Absolute Error (MAE): 6.374130600595427\n",
      "Test Set - Mean Squared Error (MSE): 76.22374624251286\n",
      "Test Set - Root Mean Squared Error (RMSE): 8.730621183083874\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "from talib import RSI, MACD, STOCH, OBV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Fetch historical data from Yahoo Finance\n",
    "ticker = 'TSLA'  # Tesla stock ticker\n",
    "start_date = '2019-01-01'  # Start date of historical data\n",
    "end_date = '2023-07-05'  # End date of historical data\n",
    "\n",
    "data = yf.download(ticker, start=start_date, end=end_date)\n",
    "\n",
    "# Calculate moving averages\n",
    "data['MA_5'] = data['Close'].rolling(window=5).mean()\n",
    "data['MA_10'] = data['Close'].rolling(window=10).mean()\n",
    "data['MA_20'] = data['Close'].rolling(window=20).mean()\n",
    "\n",
    "# Calculate RSI\n",
    "rsi_period = 14  # RSI calculation period\n",
    "data['RSI'] = RSI(data['Close'], timeperiod=rsi_period)\n",
    "\n",
    "# Calculate MACD\n",
    "macd, macd_signal, _ = MACD(data['Close'])\n",
    "data['MACD'] = macd\n",
    "\n",
    "# Calculate Stochastic Oscillator\n",
    "slowk, slowd = STOCH(data['High'], data['Low'], data['Close'])\n",
    "data['SlowK'] = slowk\n",
    "data['SlowD'] = slowd\n",
    "\n",
    "# Calculate On-Balance Volume (OBV)\n",
    "obv = OBV(data['Close'], data['Volume'])\n",
    "data['OBV'] = obv\n",
    "\n",
    "# Prepare the data\n",
    "X = data[['Open', 'Volume', 'MA_5', 'MA_10', 'MA_20', 'RSI', 'MACD', 'SlowK', 'SlowD', 'OBV']].iloc[:-1]  # Adjusted for predicting next day's price\n",
    "y = data['Close'].shift(-1)[:-1]  # Adjusted for predicting next day's price\n",
    "\n",
    "# Handle missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X = imputer.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions for the next day's price\n",
    "latest_data = X[-1].reshape(1, -1)  # Reshape the data to match the model's input shape\n",
    "next_day_prediction = model.predict(latest_data)\n",
    "print(\"Predicted price for the next day:\", next_day_prediction[0])\n",
    "\n",
    "# Evaluate the model\n",
    "predictions_train = model.predict(X_train)\n",
    "predictions_test = model.predict(X_test)\n",
    "\n",
    "mae_train = mean_absolute_error(y_train, predictions_train)\n",
    "mse_train = mean_squared_error(y_train, predictions_train)\n",
    "rmse_train = np.sqrt(mse_train)\n",
    "\n",
    "mae_test = mean_absolute_error(y_test, predictions_test)\n",
    "mse_test = mean_squared_error(y_test, predictions_test)\n",
    "rmse_test = np.sqrt(mse_test)\n",
    "\n",
    "print(\"Train Set - Mean Absolute Error (MAE):\", mae_train)\n",
    "print(\"Train Set - Mean Squared Error (MSE):\", mse_train)\n",
    "print(\"Train Set - Root Mean Squared Error (RMSE):\", rmse_train)\n",
    "print(\"Test Set - Mean Absolute Error (MAE):\", mae_test)\n",
    "print(\"Test Set - Mean Squared Error (MSE):\", mse_test)\n",
    "print(\"Test Set - Root Mean Squared Error (RMSE):\", rmse_test)\n",
    "\n",
    "# Save the data in a CSV file\n",
    "data.to_csv('tesla_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "Mean Absolute Error (MAE): 6.6317438477661685\n",
      "Mean Squared Error (MSE): 91.23602315957167\n",
      "Root Mean Squared Error (RMSE): 9.55175497799078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sami/anaconda3/envs/tf/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.829e+04, tolerance: 1.077e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Lasso, LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "from talib import RSI, MACD, STOCH, OBV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Fetch historical data from Yahoo Finance\n",
    "ticker = 'TSLA'  # Tesla stock ticker\n",
    "start_date = '2019-01-01'  # Start date of historical data\n",
    "end_date = '2023-06-01'  # End date of historical data\n",
    "\n",
    "data = yf.download(ticker, start=start_date, end=end_date)\n",
    "\n",
    "# Calculate moving averages\n",
    "data['MA_5'] = data['Close'].rolling(window=5).mean()\n",
    "data['MA_10'] = data['Close'].rolling(window=10).mean()\n",
    "data['MA_20'] = data['Close'].rolling(window=20).mean()\n",
    "\n",
    "# Calculate RSI\n",
    "rsi_period = 14  # RSI calculation period\n",
    "data['RSI'] = RSI(data['Close'], timeperiod=rsi_period)\n",
    "\n",
    "# Calculate MACD\n",
    "macd, macd_signal, _ = MACD(data['Close'])\n",
    "data['MACD'] = macd\n",
    "\n",
    "# Calculate Stochastic Oscillator\n",
    "slowk, slowd = STOCH(data['High'], data['Low'], data['Close'])\n",
    "data['SlowK'] = slowk\n",
    "data['SlowD'] = slowd\n",
    "\n",
    "# Calculate On-Balance Volume (OBV)\n",
    "obv = OBV(data['Close'], data['Volume'])\n",
    "data['OBV'] = obv\n",
    "\n",
    "# Prepare the data\n",
    "X = data[['Open', 'Volume', 'MA_5', 'MA_10', 'MA_20', 'RSI', 'MACD', 'SlowK', 'SlowD', 'OBV']].iloc[:-1]  # Adjusted for predicting next day's price\n",
    "y = data['Close'].shift(-1)[:-1]  # Adjusted for predicting next day's price\n",
    "\n",
    "# Handle missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X = imputer.fit_transform(X)\n",
    "\n",
    "# Convert X back to a DataFrame\n",
    "X = pd.DataFrame(X, columns=['Open', 'Volume', 'MA_5', 'MA_10', 'MA_20', 'RSI', 'MACD', 'SlowK', 'SlowD', 'OBV'])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Perform feature selection using Lasso regularization\n",
    "lasso_model = Lasso(alpha=0.01)  # Adjust the regularization strength as needed\n",
    "lasso_model.fit(X_train, y_train)\n",
    "\n",
    "# Retrieve the selected features based on non-zero coefficients\n",
    "selected_features = X.columns[lasso_model.coef_ != 0]\n",
    "\n",
    "# Retrain the model using the selected features\n",
    "X_train_selected = X_train[selected_features]\n",
    "X_test_selected = X_test[selected_features]\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_selected, y_train)\n",
    "\n",
    "# Make predictions for the next day's price using the model with selected features\n",
    "next_day_prediction = model.predict(X_test_selected)\n",
    "\n",
    "# Evaluate the model\n",
    "mae = mean_absolute_error(y_test, next_day_prediction)\n",
    "mse = mean_squared_error(y_test, next_day_prediction)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "\n",
    "# Save the data in a CSV file\n",
    "data.to_csv('tesla_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "Mean Absolute Error (MAE): 6.374130446514479\n",
      "Mean Squared Error (MSE): 76.22374587816533\n",
      "Root Mean Squared Error (RMSE): 8.730621162217803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sami/anaconda3/envs/tf/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.054e+04, tolerance: 1.102e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/sami/anaconda3/envs/tf/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=4.44771e-19): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "from talib import RSI, MACD, STOCH, OBV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Fetch historical data from Yahoo Finance\n",
    "ticker = 'TSLA'  # Tesla stock ticker\n",
    "start_date = '2019-01-01'  # Start date of historical data\n",
    "end_date = '2023-07-05'  # End date of historical data\n",
    "\n",
    "data = yf.download(ticker, start=start_date, end=end_date)\n",
    "\n",
    "# Calculate moving averages\n",
    "data['MA_5'] = data['Close'].rolling(window=5).mean()\n",
    "data['MA_10'] = data['Close'].rolling(window=10).mean()\n",
    "data['MA_20'] = data['Close'].rolling(window=20).mean()\n",
    "\n",
    "# Calculate RSI\n",
    "rsi_period = 14  # RSI calculation period\n",
    "data['RSI'] = RSI(data['Close'], timeperiod=rsi_period)\n",
    "\n",
    "# Calculate MACD\n",
    "macd, macd_signal, _ = MACD(data['Close'])\n",
    "data['MACD'] = macd\n",
    "\n",
    "# Calculate Stochastic Oscillator\n",
    "slowk, slowd = STOCH(data['High'], data['Low'], data['Close'])\n",
    "data['SlowK'] = slowk\n",
    "data['SlowD'] = slowd\n",
    "\n",
    "# Calculate On-Balance Volume (OBV)\n",
    "obv = OBV(data['Close'], data['Volume'])\n",
    "data['OBV'] = obv\n",
    "\n",
    "# Prepare the data\n",
    "X = data[['Open', 'Volume', 'MA_5', 'MA_10', 'MA_20', 'RSI', 'MACD', 'SlowK', 'SlowD', 'OBV']].iloc[:-1]  # Adjusted for predicting next day's price\n",
    "y = data['Close'].shift(-1)[:-1]  # Adjusted for predicting next day's price\n",
    "\n",
    "# Store column names\n",
    "column_names = X.columns\n",
    "\n",
    "# Handle missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X = imputer.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Perform feature selection using Lasso regularization\n",
    "lasso_model = Lasso(alpha=0.01)  # Adjust the regularization strength as needed\n",
    "lasso_model.fit(X_train, y_train)\n",
    "\n",
    "# Retrieve the selected features based on non-zero coefficients\n",
    "selected_features = column_names[lasso_model.coef_ != 0]\n",
    "\n",
    "# Retrain the model using the selected features with Ridge regularization\n",
    "ridge_model = Ridge(alpha=0.01)  # Adjust the regularization strength as needed\n",
    "ridge_model.fit(X_train[:, lasso_model.coef_ != 0], y_train)\n",
    "\n",
    "# Make predictions for the next day's price using the model with selected features\n",
    "next_day_prediction = ridge_model.predict(X_test[:, lasso_model.coef_ != 0])\n",
    "\n",
    "# Evaluate the model\n",
    "mae = mean_absolute_error(y_test, next_day_prediction)\n",
    "mse = mean_squared_error(y_test, next_day_prediction)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "\n",
    "# Save the data in a CSV file\n",
    "data.to_csv('tesla_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "Epoch [10/5000], Loss: 108703032.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sami/anaconda3/envs/tf/lib/python3.10/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/sami/anaconda3/envs/tf/lib/python3.10/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([27])) that is different to the input size (torch.Size([27, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/5000], Loss: 180453200.0000\n",
      "Epoch [30/5000], Loss: 1349870208.0000\n",
      "Epoch [40/5000], Loss: 251008448.0000\n",
      "Epoch [50/5000], Loss: 69850848.0000\n",
      "Epoch [60/5000], Loss: 1159621504.0000\n",
      "Epoch [70/5000], Loss: 217812848.0000\n",
      "Epoch [80/5000], Loss: 8038053376.0000\n",
      "Epoch [90/5000], Loss: 58322540.0000\n",
      "Epoch [100/5000], Loss: 187621696.0000\n",
      "Epoch [110/5000], Loss: 10721693.0000\n",
      "Epoch [120/5000], Loss: 22000968.0000\n",
      "Epoch [130/5000], Loss: 189485.3594\n",
      "Epoch [140/5000], Loss: 1104123.7500\n",
      "Epoch [150/5000], Loss: 73894.9219\n",
      "Epoch [160/5000], Loss: 5111653.0000\n",
      "Epoch [170/5000], Loss: 959506.4375\n",
      "Epoch [180/5000], Loss: 246566.7344\n",
      "Epoch [190/5000], Loss: 2193210.0000\n",
      "Epoch [200/5000], Loss: 17572050.0000\n",
      "Epoch [210/5000], Loss: 12656896.0000\n",
      "Epoch [220/5000], Loss: 2643118.2500\n",
      "Epoch [230/5000], Loss: 116015.4766\n",
      "Epoch [240/5000], Loss: 2881622.2500\n",
      "Epoch [250/5000], Loss: 94162.7578\n",
      "Epoch [260/5000], Loss: 12777.9277\n",
      "Epoch [270/5000], Loss: 12749.1299\n",
      "Epoch [280/5000], Loss: 12634.9551\n",
      "Epoch [290/5000], Loss: 12551.3828\n",
      "Epoch [300/5000], Loss: 12414.8809\n",
      "Epoch [310/5000], Loss: 12236.6572\n",
      "Epoch [320/5000], Loss: 12020.6992\n",
      "Epoch [330/5000], Loss: 11765.1738\n",
      "Epoch [340/5000], Loss: 11752.1602\n",
      "Epoch [350/5000], Loss: 11144.2598\n",
      "Epoch [360/5000], Loss: 10809.8799\n",
      "Epoch [370/5000], Loss: 10469.3271\n",
      "Epoch [380/5000], Loss: 10162.1904\n",
      "Epoch [390/5000], Loss: 9911.2949\n",
      "Epoch [400/5000], Loss: 9725.3057\n",
      "Epoch [410/5000], Loss: 9603.3105\n",
      "Epoch [420/5000], Loss: 9534.7012\n",
      "Epoch [430/5000], Loss: 9508.9805\n",
      "Epoch [440/5000], Loss: 9504.0273\n",
      "Epoch [450/5000], Loss: 9508.6328\n",
      "Epoch [460/5000], Loss: 9515.3848\n",
      "Epoch [470/5000], Loss: 9521.0859\n",
      "Epoch [480/5000], Loss: 9525.0674\n",
      "Epoch [490/5000], Loss: 9527.6113\n",
      "Epoch [500/5000], Loss: 9529.1758\n",
      "Epoch [510/5000], Loss: 9530.1299\n",
      "Epoch [520/5000], Loss: 9530.6885\n",
      "Epoch [530/5000], Loss: 9530.9971\n",
      "Epoch [540/5000], Loss: 9531.1670\n",
      "Epoch [550/5000], Loss: 9531.2588\n",
      "Epoch [560/5000], Loss: 9531.2998\n",
      "Epoch [570/5000], Loss: 9531.3281\n",
      "Epoch [580/5000], Loss: 9531.3418\n",
      "Epoch [590/5000], Loss: 9531.3281\n",
      "Epoch [600/5000], Loss: 9531.3193\n",
      "Epoch [610/5000], Loss: 9531.3145\n",
      "Epoch [620/5000], Loss: 9531.3115\n",
      "Epoch [630/5000], Loss: 9531.3076\n",
      "Epoch [640/5000], Loss: 9531.3076\n",
      "Epoch [650/5000], Loss: 9531.3066\n",
      "Epoch [660/5000], Loss: 9531.3057\n",
      "Epoch [670/5000], Loss: 9531.3057\n",
      "Epoch [680/5000], Loss: 9531.3057\n",
      "Epoch [690/5000], Loss: 9531.3057\n",
      "Epoch [700/5000], Loss: 9531.3057\n",
      "Epoch [710/5000], Loss: 9531.3057\n",
      "Epoch [720/5000], Loss: 9531.3057\n",
      "Epoch [730/5000], Loss: 9531.3057\n",
      "Epoch [740/5000], Loss: 9531.3057\n",
      "Epoch [750/5000], Loss: 9531.3057\n",
      "Epoch [760/5000], Loss: 9531.3057\n",
      "Epoch [770/5000], Loss: 9531.3057\n",
      "Epoch [780/5000], Loss: 9531.3057\n",
      "Epoch [790/5000], Loss: 9531.3057\n",
      "Epoch [800/5000], Loss: 9531.3057\n",
      "Epoch [810/5000], Loss: 9531.3057\n",
      "Epoch [820/5000], Loss: 9531.3057\n",
      "Epoch [830/5000], Loss: 9531.3057\n",
      "Epoch [840/5000], Loss: 9531.3057\n",
      "Epoch [850/5000], Loss: 9531.3057\n",
      "Epoch [860/5000], Loss: 9531.3057\n",
      "Epoch [870/5000], Loss: 9531.3057\n",
      "Epoch [880/5000], Loss: 9531.3057\n",
      "Epoch [890/5000], Loss: 9531.3057\n",
      "Epoch [900/5000], Loss: 9531.3057\n",
      "Epoch [910/5000], Loss: 9531.3057\n",
      "Epoch [920/5000], Loss: 9531.3057\n",
      "Epoch [930/5000], Loss: 9531.3057\n",
      "Epoch [940/5000], Loss: 9531.3057\n",
      "Epoch [950/5000], Loss: 9531.3057\n",
      "Epoch [960/5000], Loss: 9531.3057\n",
      "Epoch [970/5000], Loss: 9531.3057\n",
      "Epoch [980/5000], Loss: 9531.3057\n",
      "Epoch [990/5000], Loss: 9531.3057\n",
      "Epoch [1000/5000], Loss: 9531.3057\n",
      "Epoch [1010/5000], Loss: 9531.3057\n",
      "Epoch [1020/5000], Loss: 9531.3057\n",
      "Epoch [1030/5000], Loss: 9531.3057\n",
      "Epoch [1040/5000], Loss: 9531.3057\n",
      "Epoch [1050/5000], Loss: 9531.3057\n",
      "Epoch [1060/5000], Loss: 9531.3057\n",
      "Epoch [1070/5000], Loss: 9531.3057\n",
      "Epoch [1080/5000], Loss: 9531.3057\n",
      "Epoch [1090/5000], Loss: 9531.3057\n",
      "Epoch [1100/5000], Loss: 9531.3057\n",
      "Epoch [1110/5000], Loss: 9531.3057\n",
      "Epoch [1120/5000], Loss: 9531.3057\n",
      "Epoch [1130/5000], Loss: 9531.3057\n",
      "Epoch [1140/5000], Loss: 9531.3057\n",
      "Epoch [1150/5000], Loss: 9531.3057\n",
      "Epoch [1160/5000], Loss: 9531.3057\n",
      "Epoch [1170/5000], Loss: 9531.3057\n",
      "Epoch [1180/5000], Loss: 9531.3057\n",
      "Epoch [1190/5000], Loss: 9531.3057\n",
      "Epoch [1200/5000], Loss: 9531.3057\n",
      "Epoch [1210/5000], Loss: 9531.3057\n",
      "Epoch [1220/5000], Loss: 9531.3057\n",
      "Epoch [1230/5000], Loss: 9531.3057\n",
      "Epoch [1240/5000], Loss: 9531.3057\n",
      "Epoch [1250/5000], Loss: 9531.3057\n",
      "Epoch [1260/5000], Loss: 9531.3057\n",
      "Epoch [1270/5000], Loss: 9531.3057\n",
      "Epoch [1280/5000], Loss: 9531.3057\n",
      "Epoch [1290/5000], Loss: 9531.3057\n",
      "Epoch [1300/5000], Loss: 9531.3057\n",
      "Epoch [1310/5000], Loss: 9531.3057\n",
      "Epoch [1320/5000], Loss: 9531.3057\n",
      "Epoch [1330/5000], Loss: 9531.3057\n",
      "Epoch [1340/5000], Loss: 9531.3057\n",
      "Epoch [1350/5000], Loss: 9531.3057\n",
      "Epoch [1360/5000], Loss: 9531.3057\n",
      "Epoch [1370/5000], Loss: 9531.3057\n",
      "Epoch [1380/5000], Loss: 9531.3057\n",
      "Epoch [1390/5000], Loss: 9531.3057\n",
      "Epoch [1400/5000], Loss: 9531.3057\n",
      "Epoch [1410/5000], Loss: 9531.3057\n",
      "Epoch [1420/5000], Loss: 9531.3057\n",
      "Epoch [1430/5000], Loss: 9531.3057\n",
      "Epoch [1440/5000], Loss: 9531.3057\n",
      "Epoch [1450/5000], Loss: 9531.3057\n",
      "Epoch [1460/5000], Loss: 9531.3057\n",
      "Epoch [1470/5000], Loss: 9531.3057\n",
      "Epoch [1480/5000], Loss: 9531.3057\n",
      "Epoch [1490/5000], Loss: 9531.3057\n",
      "Epoch [1500/5000], Loss: 9531.3057\n",
      "Epoch [1510/5000], Loss: 9531.3057\n",
      "Epoch [1520/5000], Loss: 9531.3057\n",
      "Epoch [1530/5000], Loss: 9531.3057\n",
      "Epoch [1540/5000], Loss: 9531.3057\n",
      "Epoch [1550/5000], Loss: 9531.3057\n",
      "Epoch [1560/5000], Loss: 9531.3057\n",
      "Epoch [1570/5000], Loss: 9531.3057\n",
      "Epoch [1580/5000], Loss: 9531.3057\n",
      "Epoch [1590/5000], Loss: 9531.3057\n",
      "Epoch [1600/5000], Loss: 9531.3057\n",
      "Epoch [1610/5000], Loss: 9531.3057\n",
      "Epoch [1620/5000], Loss: 9531.3057\n",
      "Epoch [1630/5000], Loss: 9531.3057\n",
      "Epoch [1640/5000], Loss: 9531.3057\n",
      "Epoch [1650/5000], Loss: 9531.3057\n",
      "Epoch [1660/5000], Loss: 9531.3057\n",
      "Epoch [1670/5000], Loss: 9531.3057\n",
      "Epoch [1680/5000], Loss: 9531.3057\n",
      "Epoch [1690/5000], Loss: 9531.3057\n",
      "Epoch [1700/5000], Loss: 9531.3057\n",
      "Epoch [1710/5000], Loss: 9531.3057\n",
      "Epoch [1720/5000], Loss: 9531.3057\n",
      "Epoch [1730/5000], Loss: 9531.3057\n",
      "Epoch [1740/5000], Loss: 9531.3057\n",
      "Epoch [1750/5000], Loss: 9531.3057\n",
      "Epoch [1760/5000], Loss: 9531.3057\n",
      "Epoch [1770/5000], Loss: 9531.3057\n",
      "Epoch [1780/5000], Loss: 9531.3057\n",
      "Epoch [1790/5000], Loss: 9531.3057\n",
      "Epoch [1800/5000], Loss: 9531.3057\n",
      "Epoch [1810/5000], Loss: 9531.3057\n",
      "Epoch [1820/5000], Loss: 9531.3057\n",
      "Epoch [1830/5000], Loss: 9531.3057\n",
      "Epoch [1840/5000], Loss: 9531.3057\n",
      "Epoch [1850/5000], Loss: 9531.3057\n",
      "Epoch [1860/5000], Loss: 9531.3057\n",
      "Epoch [1870/5000], Loss: 9531.3057\n",
      "Epoch [1880/5000], Loss: 9531.3057\n",
      "Epoch [1890/5000], Loss: 9531.3057\n",
      "Epoch [1900/5000], Loss: 9531.3057\n",
      "Epoch [1910/5000], Loss: 9531.3057\n",
      "Epoch [1920/5000], Loss: 9531.3057\n",
      "Epoch [1930/5000], Loss: 9531.3057\n",
      "Epoch [1940/5000], Loss: 9531.3057\n",
      "Epoch [1950/5000], Loss: 9531.3057\n",
      "Epoch [1960/5000], Loss: 9531.3057\n",
      "Epoch [1970/5000], Loss: 9531.3057\n",
      "Epoch [1980/5000], Loss: 9531.3057\n",
      "Epoch [1990/5000], Loss: 9531.3057\n",
      "Epoch [2000/5000], Loss: 9531.3057\n",
      "Epoch [2010/5000], Loss: 9531.3057\n",
      "Epoch [2020/5000], Loss: 9531.3057\n",
      "Epoch [2030/5000], Loss: 9531.3057\n",
      "Epoch [2040/5000], Loss: 9531.3057\n",
      "Epoch [2050/5000], Loss: 9531.3057\n",
      "Epoch [2060/5000], Loss: 9531.3057\n",
      "Epoch [2070/5000], Loss: 9531.3057\n",
      "Epoch [2080/5000], Loss: 9531.3057\n",
      "Epoch [2090/5000], Loss: 9531.3057\n",
      "Epoch [2100/5000], Loss: 9531.3057\n",
      "Epoch [2110/5000], Loss: 9531.3057\n",
      "Epoch [2120/5000], Loss: 9531.3057\n",
      "Epoch [2130/5000], Loss: 9531.3057\n",
      "Epoch [2140/5000], Loss: 9531.3057\n",
      "Epoch [2150/5000], Loss: 9531.3057\n",
      "Epoch [2160/5000], Loss: 9531.3057\n",
      "Epoch [2170/5000], Loss: 9531.3057\n",
      "Epoch [2180/5000], Loss: 9531.3057\n",
      "Epoch [2190/5000], Loss: 9531.3057\n",
      "Epoch [2200/5000], Loss: 9531.3057\n",
      "Epoch [2210/5000], Loss: 9531.3057\n",
      "Epoch [2220/5000], Loss: 9531.3057\n",
      "Epoch [2230/5000], Loss: 9531.3057\n",
      "Epoch [2240/5000], Loss: 9531.3057\n",
      "Epoch [2250/5000], Loss: 9531.3057\n",
      "Epoch [2260/5000], Loss: 9531.3057\n",
      "Epoch [2270/5000], Loss: 9531.3057\n",
      "Epoch [2280/5000], Loss: 9531.3057\n",
      "Epoch [2290/5000], Loss: 9531.3057\n",
      "Epoch [2300/5000], Loss: 9531.3057\n",
      "Epoch [2310/5000], Loss: 9531.3057\n",
      "Epoch [2320/5000], Loss: 9531.3057\n",
      "Epoch [2330/5000], Loss: 9531.3057\n",
      "Epoch [2340/5000], Loss: 9531.3057\n",
      "Epoch [2350/5000], Loss: 9531.3057\n",
      "Epoch [2360/5000], Loss: 9531.3057\n",
      "Epoch [2370/5000], Loss: 9531.3057\n",
      "Epoch [2380/5000], Loss: 9531.3057\n",
      "Epoch [2390/5000], Loss: 9531.3057\n",
      "Epoch [2400/5000], Loss: 9531.3057\n",
      "Epoch [2410/5000], Loss: 9531.3057\n",
      "Epoch [2420/5000], Loss: 9531.3057\n",
      "Epoch [2430/5000], Loss: 9531.3057\n",
      "Epoch [2440/5000], Loss: 9531.3057\n",
      "Epoch [2450/5000], Loss: 9531.3057\n",
      "Epoch [2460/5000], Loss: 9531.3057\n",
      "Epoch [2470/5000], Loss: 9531.3057\n",
      "Epoch [2480/5000], Loss: 9531.3057\n",
      "Epoch [2490/5000], Loss: 9531.3057\n",
      "Epoch [2500/5000], Loss: 9531.3057\n",
      "Epoch [2510/5000], Loss: 9531.3057\n",
      "Epoch [2520/5000], Loss: 9531.3057\n",
      "Epoch [2530/5000], Loss: 9531.3057\n",
      "Epoch [2540/5000], Loss: 9531.3057\n",
      "Epoch [2550/5000], Loss: 9531.3057\n",
      "Epoch [2560/5000], Loss: 9531.3057\n",
      "Epoch [2570/5000], Loss: 9531.3057\n",
      "Epoch [2580/5000], Loss: 9531.3057\n",
      "Epoch [2590/5000], Loss: 9531.3057\n",
      "Epoch [2600/5000], Loss: 9531.3057\n",
      "Epoch [2610/5000], Loss: 9531.3057\n",
      "Epoch [2620/5000], Loss: 9531.3057\n",
      "Epoch [2630/5000], Loss: 9531.3057\n",
      "Epoch [2640/5000], Loss: 9531.3057\n",
      "Epoch [2650/5000], Loss: 9531.3057\n",
      "Epoch [2660/5000], Loss: 9531.3057\n",
      "Epoch [2670/5000], Loss: 9531.3057\n",
      "Epoch [2680/5000], Loss: 9531.3057\n",
      "Epoch [2690/5000], Loss: 9531.3057\n",
      "Epoch [2700/5000], Loss: 9531.3057\n",
      "Epoch [2710/5000], Loss: 9531.3057\n",
      "Epoch [2720/5000], Loss: 9531.3057\n",
      "Epoch [2730/5000], Loss: 9531.3057\n",
      "Epoch [2740/5000], Loss: 9531.3057\n",
      "Epoch [2750/5000], Loss: 9531.3057\n",
      "Epoch [2760/5000], Loss: 9531.3057\n",
      "Epoch [2770/5000], Loss: 9531.3057\n",
      "Epoch [2780/5000], Loss: 9531.3057\n",
      "Epoch [2790/5000], Loss: 9531.3057\n",
      "Epoch [2800/5000], Loss: 9531.3057\n",
      "Epoch [2810/5000], Loss: 9531.3057\n",
      "Epoch [2820/5000], Loss: 9531.3057\n",
      "Epoch [2830/5000], Loss: 9531.3057\n",
      "Epoch [2840/5000], Loss: 9531.3057\n",
      "Epoch [2850/5000], Loss: 9531.3057\n",
      "Epoch [2860/5000], Loss: 9531.3057\n",
      "Epoch [2870/5000], Loss: 9531.3057\n",
      "Epoch [2880/5000], Loss: 9531.3057\n",
      "Epoch [2890/5000], Loss: 9531.3057\n",
      "Epoch [2900/5000], Loss: 9531.3057\n",
      "Epoch [2910/5000], Loss: 9531.3057\n",
      "Epoch [2920/5000], Loss: 9531.3057\n",
      "Epoch [2930/5000], Loss: 9531.3057\n",
      "Epoch [2940/5000], Loss: 9531.3057\n",
      "Epoch [2950/5000], Loss: 9531.3057\n",
      "Epoch [2960/5000], Loss: 9531.3057\n",
      "Epoch [2970/5000], Loss: 9531.3057\n",
      "Epoch [2980/5000], Loss: 9531.3057\n",
      "Epoch [2990/5000], Loss: 9531.3057\n",
      "Epoch [3000/5000], Loss: 9531.3057\n",
      "Epoch [3010/5000], Loss: 9531.3057\n",
      "Epoch [3020/5000], Loss: 9531.3057\n",
      "Epoch [3030/5000], Loss: 9531.3057\n",
      "Epoch [3040/5000], Loss: 9531.3057\n",
      "Epoch [3050/5000], Loss: 9531.3057\n",
      "Epoch [3060/5000], Loss: 9531.3057\n",
      "Epoch [3070/5000], Loss: 9531.3057\n",
      "Epoch [3080/5000], Loss: 9531.3057\n",
      "Epoch [3090/5000], Loss: 9531.3057\n",
      "Epoch [3100/5000], Loss: 9531.3057\n",
      "Epoch [3110/5000], Loss: 9531.3057\n",
      "Epoch [3120/5000], Loss: 9531.3057\n",
      "Epoch [3130/5000], Loss: 9531.3057\n",
      "Epoch [3140/5000], Loss: 9531.3057\n",
      "Epoch [3150/5000], Loss: 9531.3057\n",
      "Epoch [3160/5000], Loss: 9531.3057\n",
      "Epoch [3170/5000], Loss: 9531.3057\n",
      "Epoch [3180/5000], Loss: 9531.3057\n",
      "Epoch [3190/5000], Loss: 9531.3057\n",
      "Epoch [3200/5000], Loss: 9531.3057\n",
      "Epoch [3210/5000], Loss: 9531.3057\n",
      "Epoch [3220/5000], Loss: 9531.3057\n",
      "Epoch [3230/5000], Loss: 9531.3057\n",
      "Epoch [3240/5000], Loss: 9531.3057\n",
      "Epoch [3250/5000], Loss: 9531.3057\n",
      "Epoch [3260/5000], Loss: 9531.3057\n",
      "Epoch [3270/5000], Loss: 9531.3057\n",
      "Epoch [3280/5000], Loss: 9531.3057\n",
      "Epoch [3290/5000], Loss: 9531.3057\n",
      "Epoch [3300/5000], Loss: 9531.3057\n",
      "Epoch [3310/5000], Loss: 9531.3057\n",
      "Epoch [3320/5000], Loss: 9531.3057\n",
      "Epoch [3330/5000], Loss: 9531.3057\n",
      "Epoch [3340/5000], Loss: 9531.3057\n",
      "Epoch [3350/5000], Loss: 9531.3057\n",
      "Epoch [3360/5000], Loss: 9531.3057\n",
      "Epoch [3370/5000], Loss: 9531.3057\n",
      "Epoch [3380/5000], Loss: 9531.3057\n",
      "Epoch [3390/5000], Loss: 9531.3057\n",
      "Epoch [3400/5000], Loss: 9531.3057\n",
      "Epoch [3410/5000], Loss: 9531.3057\n",
      "Epoch [3420/5000], Loss: 9531.3057\n",
      "Epoch [3430/5000], Loss: 9531.3057\n",
      "Epoch [3440/5000], Loss: 9531.3057\n",
      "Epoch [3450/5000], Loss: 9531.3057\n",
      "Epoch [3460/5000], Loss: 9531.3057\n",
      "Epoch [3470/5000], Loss: 9531.3057\n",
      "Epoch [3480/5000], Loss: 9531.3057\n",
      "Epoch [3490/5000], Loss: 9531.3057\n",
      "Epoch [3500/5000], Loss: 9531.3057\n",
      "Epoch [3510/5000], Loss: 9531.3057\n",
      "Epoch [3520/5000], Loss: 9531.3057\n",
      "Epoch [3530/5000], Loss: 9531.3057\n",
      "Epoch [3540/5000], Loss: 9531.3057\n",
      "Epoch [3550/5000], Loss: 9531.3057\n",
      "Epoch [3560/5000], Loss: 9531.3057\n",
      "Epoch [3570/5000], Loss: 9531.3057\n",
      "Epoch [3580/5000], Loss: 9531.3057\n",
      "Epoch [3590/5000], Loss: 9531.3057\n",
      "Epoch [3600/5000], Loss: 9531.3057\n",
      "Epoch [3610/5000], Loss: 9531.3057\n",
      "Epoch [3620/5000], Loss: 9531.3057\n",
      "Epoch [3630/5000], Loss: 9531.3057\n",
      "Epoch [3640/5000], Loss: 9531.3057\n",
      "Epoch [3650/5000], Loss: 9531.3057\n",
      "Epoch [3660/5000], Loss: 9531.3057\n",
      "Epoch [3670/5000], Loss: 9531.3057\n",
      "Epoch [3680/5000], Loss: 9531.3057\n",
      "Epoch [3690/5000], Loss: 9531.3057\n",
      "Epoch [3700/5000], Loss: 9531.3057\n",
      "Epoch [3710/5000], Loss: 9531.3057\n",
      "Epoch [3720/5000], Loss: 9531.3057\n",
      "Epoch [3730/5000], Loss: 9531.3057\n",
      "Epoch [3740/5000], Loss: 9531.3057\n",
      "Epoch [3750/5000], Loss: 9531.3057\n",
      "Epoch [3760/5000], Loss: 9531.3057\n",
      "Epoch [3770/5000], Loss: 9531.3057\n",
      "Epoch [3780/5000], Loss: 9531.3057\n",
      "Epoch [3790/5000], Loss: 9531.3057\n",
      "Epoch [3800/5000], Loss: 9531.3057\n",
      "Epoch [3810/5000], Loss: 9531.3057\n",
      "Epoch [3820/5000], Loss: 9531.3057\n",
      "Epoch [3830/5000], Loss: 9531.3057\n",
      "Epoch [3840/5000], Loss: 9531.3057\n",
      "Epoch [3850/5000], Loss: 9531.3057\n",
      "Epoch [3860/5000], Loss: 9531.3057\n",
      "Epoch [3870/5000], Loss: 9531.3057\n",
      "Epoch [3880/5000], Loss: 9531.3057\n",
      "Epoch [3890/5000], Loss: 9531.3057\n",
      "Epoch [3900/5000], Loss: 9531.3057\n",
      "Epoch [3910/5000], Loss: 9531.3057\n",
      "Epoch [3920/5000], Loss: 9531.3057\n",
      "Epoch [3930/5000], Loss: 9531.3057\n",
      "Epoch [3940/5000], Loss: 9531.3057\n",
      "Epoch [3950/5000], Loss: 9531.3057\n",
      "Epoch [3960/5000], Loss: 9531.3057\n",
      "Epoch [3970/5000], Loss: 9531.3057\n",
      "Epoch [3980/5000], Loss: 9531.3057\n",
      "Epoch [3990/5000], Loss: 9531.3057\n",
      "Epoch [4000/5000], Loss: 9531.3057\n",
      "Epoch [4010/5000], Loss: 9531.3057\n",
      "Epoch [4020/5000], Loss: 9531.3057\n",
      "Epoch [4030/5000], Loss: 9531.3057\n",
      "Epoch [4040/5000], Loss: 9531.3057\n",
      "Epoch [4050/5000], Loss: 9531.3057\n",
      "Epoch [4060/5000], Loss: 9531.3057\n",
      "Epoch [4070/5000], Loss: 9531.3057\n",
      "Epoch [4080/5000], Loss: 9531.3057\n",
      "Epoch [4090/5000], Loss: 9531.3057\n",
      "Epoch [4100/5000], Loss: 9531.3057\n",
      "Epoch [4110/5000], Loss: 9531.3057\n",
      "Epoch [4120/5000], Loss: 9531.3057\n",
      "Epoch [4130/5000], Loss: 9531.3057\n",
      "Epoch [4140/5000], Loss: 9531.3057\n",
      "Epoch [4150/5000], Loss: 9531.3057\n",
      "Epoch [4160/5000], Loss: 9531.3057\n",
      "Epoch [4170/5000], Loss: 9531.3057\n",
      "Epoch [4180/5000], Loss: 9531.3057\n",
      "Epoch [4190/5000], Loss: 9531.3057\n",
      "Epoch [4200/5000], Loss: 9531.3057\n",
      "Epoch [4210/5000], Loss: 9531.3057\n",
      "Epoch [4220/5000], Loss: 9531.3057\n",
      "Epoch [4230/5000], Loss: 9531.3057\n",
      "Epoch [4240/5000], Loss: 9531.3057\n",
      "Epoch [4250/5000], Loss: 9531.3057\n",
      "Epoch [4260/5000], Loss: 9531.3057\n",
      "Epoch [4270/5000], Loss: 9531.3057\n",
      "Epoch [4280/5000], Loss: 9531.3057\n",
      "Epoch [4290/5000], Loss: 9531.3057\n",
      "Epoch [4300/5000], Loss: 9531.3057\n",
      "Epoch [4310/5000], Loss: 9531.3057\n",
      "Epoch [4320/5000], Loss: 9531.3057\n",
      "Epoch [4330/5000], Loss: 9531.3057\n",
      "Epoch [4340/5000], Loss: 9531.3057\n",
      "Epoch [4350/5000], Loss: 9531.3057\n",
      "Epoch [4360/5000], Loss: 9531.3057\n",
      "Epoch [4370/5000], Loss: 9531.3057\n",
      "Epoch [4380/5000], Loss: 9531.3057\n",
      "Epoch [4390/5000], Loss: 9531.3057\n",
      "Epoch [4400/5000], Loss: 9531.3057\n",
      "Epoch [4410/5000], Loss: 9531.3057\n",
      "Epoch [4420/5000], Loss: 9531.3057\n",
      "Epoch [4430/5000], Loss: 9531.3057\n",
      "Epoch [4440/5000], Loss: 9531.3057\n",
      "Epoch [4450/5000], Loss: 9531.3057\n",
      "Epoch [4460/5000], Loss: 9531.3057\n",
      "Epoch [4470/5000], Loss: 9531.3057\n",
      "Epoch [4480/5000], Loss: 9531.3057\n",
      "Epoch [4490/5000], Loss: 9531.3057\n",
      "Epoch [4500/5000], Loss: 9531.3057\n",
      "Epoch [4510/5000], Loss: 9531.3057\n",
      "Epoch [4520/5000], Loss: 9531.3057\n",
      "Epoch [4530/5000], Loss: 9531.3057\n",
      "Epoch [4540/5000], Loss: 9531.3057\n",
      "Epoch [4550/5000], Loss: 9531.3057\n",
      "Epoch [4560/5000], Loss: 9531.3057\n",
      "Epoch [4570/5000], Loss: 9531.3057\n",
      "Epoch [4580/5000], Loss: 9531.3057\n",
      "Epoch [4590/5000], Loss: 9531.3057\n",
      "Epoch [4600/5000], Loss: 9531.3057\n",
      "Epoch [4610/5000], Loss: 9531.3057\n",
      "Epoch [4620/5000], Loss: 9531.3057\n",
      "Epoch [4630/5000], Loss: 9531.3057\n",
      "Epoch [4640/5000], Loss: 9531.3057\n",
      "Epoch [4650/5000], Loss: 9531.3057\n",
      "Epoch [4660/5000], Loss: 9531.3057\n",
      "Epoch [4670/5000], Loss: 9531.3057\n",
      "Epoch [4680/5000], Loss: 9531.3057\n",
      "Epoch [4690/5000], Loss: 9531.3057\n",
      "Epoch [4700/5000], Loss: 9531.3057\n",
      "Epoch [4710/5000], Loss: 9531.3057\n",
      "Epoch [4720/5000], Loss: 9531.3057\n",
      "Epoch [4730/5000], Loss: 9531.3057\n",
      "Epoch [4740/5000], Loss: 9531.3057\n",
      "Epoch [4750/5000], Loss: 9531.3057\n",
      "Epoch [4760/5000], Loss: 9531.3057\n",
      "Epoch [4770/5000], Loss: 9531.3057\n",
      "Epoch [4780/5000], Loss: 9531.3057\n",
      "Epoch [4790/5000], Loss: 9531.3057\n",
      "Epoch [4800/5000], Loss: 9531.3057\n",
      "Epoch [4810/5000], Loss: 9531.3057\n",
      "Epoch [4820/5000], Loss: 9531.3057\n",
      "Epoch [4830/5000], Loss: 9531.3057\n",
      "Epoch [4840/5000], Loss: 9531.3057\n",
      "Epoch [4850/5000], Loss: 9531.3057\n",
      "Epoch [4860/5000], Loss: 9531.3057\n",
      "Epoch [4870/5000], Loss: 9531.3057\n",
      "Epoch [4880/5000], Loss: 9531.3057\n",
      "Epoch [4890/5000], Loss: 9531.3057\n",
      "Epoch [4900/5000], Loss: 9531.3057\n",
      "Epoch [4910/5000], Loss: 9531.3057\n",
      "Epoch [4920/5000], Loss: 9531.3057\n",
      "Epoch [4930/5000], Loss: 9531.3057\n",
      "Epoch [4940/5000], Loss: 9531.3057\n",
      "Epoch [4950/5000], Loss: 9531.3057\n",
      "Epoch [4960/5000], Loss: 9531.3057\n",
      "Epoch [4970/5000], Loss: 9531.3057\n",
      "Epoch [4980/5000], Loss: 9531.3057\n",
      "Epoch [4990/5000], Loss: 9531.3057\n",
      "Epoch [5000/5000], Loss: 9531.3057\n",
      "Mean Absolute Error (MAE): 78.0820499598525\n",
      "Mean Squared Error (MSE): 9818.68038517205\n",
      "Root Mean Squared Error (RMSE): 99.08925464030926\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "from talib import RSI, MACD, STOCH, OBV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Fetch historical data from Yahoo Finance\n",
    "ticker = 'TSLA'  # Tesla stock ticker\n",
    "start_date = '2010-01-01'  # Start date of historical data\n",
    "end_date = '2023-07-05'  # End date of historical data\n",
    "\n",
    "data = yf.download(ticker, start=start_date, end=end_date)\n",
    "\n",
    "# Calculate moving averages\n",
    "data['MA_5'] = data['Close'].rolling(window=5).mean()\n",
    "data['MA_10'] = data['Close'].rolling(window=10).mean()\n",
    "data['MA_20'] = data['Close'].rolling(window=20).mean()\n",
    "\n",
    "# Calculate RSI\n",
    "rsi_period = 14  # RSI calculation period\n",
    "data['RSI'] = RSI(data['Close'], timeperiod=rsi_period)\n",
    "\n",
    "# Calculate MACD\n",
    "macd, macd_signal, _ = MACD(data['Close'])\n",
    "data['MACD'] = macd\n",
    "\n",
    "# Calculate Stochastic Oscillator\n",
    "slowk, slowd = STOCH(data['High'], data['Low'], data['Close'])\n",
    "data['SlowK'] = slowk\n",
    "data['SlowD'] = slowd\n",
    "\n",
    "# Calculate On-Balance Volume (OBV)\n",
    "obv = OBV(data['Close'], data['Volume'])\n",
    "data['OBV'] = obv\n",
    "\n",
    "# Prepare the data\n",
    "X = data[['Open', 'Volume', 'MA_5', 'MA_10', 'MA_20', 'RSI', 'MACD', 'SlowK', 'SlowD', 'OBV']].iloc[:-1]  # Adjusted for predicting next day's price\n",
    "y = data['Close'].shift(-1)[:-1]  # Adjusted for predicting next day's price\n",
    "\n",
    "# Store column names\n",
    "column_names = X.columns\n",
    "\n",
    "# Handle missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X = imputer.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train = torch.from_numpy(X_train).float()\n",
    "y_train = torch.from_numpy(y_train.values).float()\n",
    "X_test = torch.from_numpy(X_test).float()\n",
    "\n",
    "# Define the Neural Network class\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 32)\n",
    "        self.fc2 = nn.Linear(32, 16)\n",
    "        self.fc3 = nn.Linear(16, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        out = self.fc3(x)\n",
    "        return out\n",
    "\n",
    "# Instantiate the neural network\n",
    "input_size = X_train.shape[1]\n",
    "neural_network = NeuralNetwork(input_size)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(neural_network.parameters(), lr=0.01)\n",
    "\n",
    "# Train the neural network\n",
    "num_epochs = 5000\n",
    "batch_size = 32\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(0, X_train.size(0), batch_size):\n",
    "        inputs = X_train[i:i+batch_size]\n",
    "        labels = y_train[i:i+batch_size]\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = neural_network(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Make predictions using the trained neural network\n",
    "neural_network.eval()  # Switch to evaluation mode\n",
    "predictions = neural_network(X_test)\n",
    "\n",
    "# Convert predictions tensor to numpy array\n",
    "predictions = predictions.detach().numpy().flatten()\n",
    "\n",
    "# Evaluate the model\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "\n",
    "# Save the model\n",
    "torch.save(neural_network.state_dict(), 'neural_network_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "Epoch [10/5000], Loss: 9718559744.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sami/anaconda3/envs/tf/lib/python3.10/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/sami/anaconda3/envs/tf/lib/python3.10/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([27])) that is different to the input size (torch.Size([27, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/5000], Loss: 4611330.5000\n",
      "Epoch [30/5000], Loss: 19307376.0000\n",
      "Epoch [40/5000], Loss: 4637132.5000\n",
      "Epoch [50/5000], Loss: 2134869.0000\n",
      "Epoch [60/5000], Loss: 2993317.0000\n",
      "Epoch [70/5000], Loss: 226460.4844\n",
      "Epoch [80/5000], Loss: 12976.9082\n",
      "Epoch [90/5000], Loss: 12976.6328\n",
      "Epoch [100/5000], Loss: 12976.3389\n",
      "Epoch [110/5000], Loss: 12975.9658\n",
      "Epoch [120/5000], Loss: 12975.4404\n",
      "Epoch [130/5000], Loss: 12974.6670\n",
      "Epoch [140/5000], Loss: 12973.5078\n",
      "Epoch [150/5000], Loss: 12971.7588\n",
      "Epoch [160/5000], Loss: 12969.1230\n",
      "Epoch [170/5000], Loss: 12965.1592\n",
      "Epoch [180/5000], Loss: 12959.1924\n",
      "Epoch [190/5000], Loss: 12950.2158\n",
      "Epoch [200/5000], Loss: 12936.7324\n",
      "Epoch [210/5000], Loss: 12916.5078\n",
      "Epoch [220/5000], Loss: 12886.2520\n",
      "Epoch [230/5000], Loss: 12841.1719\n",
      "Epoch [240/5000], Loss: 12774.4111\n",
      "Epoch [250/5000], Loss: 12676.5400\n",
      "Epoch [260/5000], Loss: 12535.3467\n",
      "Epoch [270/5000], Loss: 12336.9229\n",
      "Epoch [280/5000], Loss: 12069.4873\n",
      "Epoch [290/5000], Loss: 11731.1416\n",
      "Epoch [300/5000], Loss: 11338.5420\n",
      "Epoch [310/5000], Loss: 10927.0273\n",
      "Epoch [320/5000], Loss: 10537.5449\n",
      "Epoch [330/5000], Loss: 10201.3838\n",
      "Epoch [340/5000], Loss: 9934.6025\n",
      "Epoch [350/5000], Loss: 9740.4648\n",
      "Epoch [360/5000], Loss: 9613.4023\n",
      "Epoch [370/5000], Loss: 9541.9023\n",
      "Epoch [380/5000], Loss: 9510.7588\n",
      "Epoch [390/5000], Loss: 9503.9297\n",
      "Epoch [400/5000], Loss: 9508.0566\n",
      "Epoch [410/5000], Loss: 9514.8330\n",
      "Epoch [420/5000], Loss: 9520.6797\n",
      "Epoch [430/5000], Loss: 9524.7881\n",
      "Epoch [440/5000], Loss: 9527.4180\n",
      "Epoch [450/5000], Loss: 9529.0195\n",
      "Epoch [460/5000], Loss: 9529.9736\n",
      "Epoch [470/5000], Loss: 9530.5303\n",
      "Epoch [480/5000], Loss: 9530.8564\n",
      "Epoch [490/5000], Loss: 9531.0439\n",
      "Epoch [500/5000], Loss: 9531.1533\n",
      "Epoch [510/5000], Loss: 9531.2178\n",
      "Epoch [520/5000], Loss: 9531.2520\n",
      "Epoch [530/5000], Loss: 9531.2734\n",
      "Epoch [540/5000], Loss: 9531.2842\n",
      "Epoch [550/5000], Loss: 9531.2910\n",
      "Epoch [560/5000], Loss: 9531.2959\n",
      "Epoch [570/5000], Loss: 9531.2998\n",
      "Epoch [580/5000], Loss: 9531.3008\n",
      "Epoch [590/5000], Loss: 9531.3018\n",
      "Epoch [600/5000], Loss: 9531.3027\n",
      "Epoch [610/5000], Loss: 9531.3047\n",
      "Epoch [620/5000], Loss: 9531.3057\n",
      "Epoch [630/5000], Loss: 9531.3057\n",
      "Epoch [640/5000], Loss: 9531.3057\n",
      "Epoch [650/5000], Loss: 9531.3057\n",
      "Epoch [660/5000], Loss: 9531.3057\n",
      "Epoch [670/5000], Loss: 9531.3057\n",
      "Epoch [680/5000], Loss: 9531.3057\n",
      "Epoch [690/5000], Loss: 9531.3057\n",
      "Epoch [700/5000], Loss: 9531.3057\n",
      "Epoch [710/5000], Loss: 9531.3057\n",
      "Epoch [720/5000], Loss: 9531.3057\n",
      "Epoch [730/5000], Loss: 9531.3057\n",
      "Epoch [740/5000], Loss: 9531.3057\n",
      "Epoch [750/5000], Loss: 9531.3057\n",
      "Epoch [760/5000], Loss: 9531.3057\n",
      "Epoch [770/5000], Loss: 9531.3057\n",
      "Epoch [780/5000], Loss: 9531.3057\n",
      "Epoch [790/5000], Loss: 9531.3057\n",
      "Epoch [800/5000], Loss: 9531.3057\n",
      "Epoch [810/5000], Loss: 9531.3057\n",
      "Epoch [820/5000], Loss: 9531.3057\n",
      "Epoch [830/5000], Loss: 9531.3057\n",
      "Epoch [840/5000], Loss: 9531.3057\n",
      "Epoch [850/5000], Loss: 9531.3057\n",
      "Epoch [860/5000], Loss: 9531.3057\n",
      "Epoch [870/5000], Loss: 9531.3057\n",
      "Epoch [880/5000], Loss: 9531.3057\n",
      "Epoch [890/5000], Loss: 9531.3057\n",
      "Epoch [900/5000], Loss: 9531.3057\n",
      "Epoch [910/5000], Loss: 9531.3057\n",
      "Epoch [920/5000], Loss: 9531.3057\n",
      "Epoch [930/5000], Loss: 9531.3057\n",
      "Epoch [940/5000], Loss: 9531.3057\n",
      "Epoch [950/5000], Loss: 9531.3057\n",
      "Epoch [960/5000], Loss: 9531.3057\n",
      "Epoch [970/5000], Loss: 9531.3057\n",
      "Epoch [980/5000], Loss: 9531.3057\n",
      "Epoch [990/5000], Loss: 9531.3057\n",
      "Epoch [1000/5000], Loss: 9531.3057\n",
      "Epoch [1010/5000], Loss: 9531.3057\n",
      "Epoch [1020/5000], Loss: 9531.3057\n",
      "Epoch [1030/5000], Loss: 9531.3057\n",
      "Epoch [1040/5000], Loss: 9531.3057\n",
      "Epoch [1050/5000], Loss: 9531.3057\n",
      "Epoch [1060/5000], Loss: 9531.3057\n",
      "Epoch [1070/5000], Loss: 9531.3057\n",
      "Epoch [1080/5000], Loss: 9531.3057\n",
      "Epoch [1090/5000], Loss: 9531.3057\n",
      "Epoch [1100/5000], Loss: 9531.3057\n",
      "Epoch [1110/5000], Loss: 9531.3057\n",
      "Epoch [1120/5000], Loss: 9531.3057\n",
      "Epoch [1130/5000], Loss: 9531.3057\n",
      "Epoch [1140/5000], Loss: 9531.3057\n",
      "Epoch [1150/5000], Loss: 9531.3057\n",
      "Epoch [1160/5000], Loss: 9531.3057\n",
      "Epoch [1170/5000], Loss: 9531.3057\n",
      "Epoch [1180/5000], Loss: 9531.3057\n",
      "Epoch [1190/5000], Loss: 9531.3057\n",
      "Epoch [1200/5000], Loss: 9531.3057\n",
      "Epoch [1210/5000], Loss: 9531.3057\n",
      "Epoch [1220/5000], Loss: 9531.3057\n",
      "Epoch [1230/5000], Loss: 9531.3057\n",
      "Epoch [1240/5000], Loss: 9531.3057\n",
      "Epoch [1250/5000], Loss: 9531.3057\n",
      "Epoch [1260/5000], Loss: 9531.3057\n",
      "Epoch [1270/5000], Loss: 9531.3057\n",
      "Epoch [1280/5000], Loss: 9531.3057\n",
      "Epoch [1290/5000], Loss: 9531.3057\n",
      "Epoch [1300/5000], Loss: 9531.3057\n",
      "Epoch [1310/5000], Loss: 9531.3057\n",
      "Epoch [1320/5000], Loss: 9531.3057\n",
      "Epoch [1330/5000], Loss: 9531.3057\n",
      "Epoch [1340/5000], Loss: 9531.3057\n",
      "Epoch [1350/5000], Loss: 9531.3057\n",
      "Epoch [1360/5000], Loss: 9531.3057\n",
      "Epoch [1370/5000], Loss: 9531.3057\n",
      "Epoch [1380/5000], Loss: 9531.3057\n",
      "Epoch [1390/5000], Loss: 9531.3057\n",
      "Epoch [1400/5000], Loss: 9531.3057\n",
      "Epoch [1410/5000], Loss: 9531.3057\n",
      "Epoch [1420/5000], Loss: 9531.3057\n",
      "Epoch [1430/5000], Loss: 9531.3057\n",
      "Epoch [1440/5000], Loss: 9531.3057\n",
      "Epoch [1450/5000], Loss: 9531.3057\n",
      "Epoch [1460/5000], Loss: 9531.3057\n",
      "Epoch [1470/5000], Loss: 9531.3057\n",
      "Epoch [1480/5000], Loss: 9531.3057\n",
      "Epoch [1490/5000], Loss: 9531.3057\n",
      "Epoch [1500/5000], Loss: 9531.3057\n",
      "Epoch [1510/5000], Loss: 9531.3057\n",
      "Epoch [1520/5000], Loss: 9531.3057\n",
      "Epoch [1530/5000], Loss: 9531.3057\n",
      "Epoch [1540/5000], Loss: 9531.3057\n",
      "Epoch [1550/5000], Loss: 9531.3057\n",
      "Epoch [1560/5000], Loss: 9531.3057\n",
      "Epoch [1570/5000], Loss: 9531.3057\n",
      "Epoch [1580/5000], Loss: 9531.3057\n",
      "Epoch [1590/5000], Loss: 9531.3057\n",
      "Epoch [1600/5000], Loss: 9531.3057\n",
      "Epoch [1610/5000], Loss: 9531.3057\n",
      "Epoch [1620/5000], Loss: 9531.3057\n",
      "Epoch [1630/5000], Loss: 9531.3057\n",
      "Epoch [1640/5000], Loss: 9531.3057\n",
      "Epoch [1650/5000], Loss: 9531.3057\n",
      "Epoch [1660/5000], Loss: 9531.3057\n",
      "Epoch [1670/5000], Loss: 9531.3057\n",
      "Epoch [1680/5000], Loss: 9531.3057\n",
      "Epoch [1690/5000], Loss: 9531.3057\n",
      "Epoch [1700/5000], Loss: 9531.3057\n",
      "Epoch [1710/5000], Loss: 9531.3057\n",
      "Epoch [1720/5000], Loss: 9531.3057\n",
      "Epoch [1730/5000], Loss: 9531.3057\n",
      "Epoch [1740/5000], Loss: 9531.3057\n",
      "Epoch [1750/5000], Loss: 9531.3057\n",
      "Epoch [1760/5000], Loss: 9531.3057\n",
      "Epoch [1770/5000], Loss: 9531.3057\n",
      "Epoch [1780/5000], Loss: 9531.3057\n",
      "Epoch [1790/5000], Loss: 9531.3057\n",
      "Epoch [1800/5000], Loss: 9531.3057\n",
      "Epoch [1810/5000], Loss: 9531.3057\n",
      "Epoch [1820/5000], Loss: 9531.3057\n",
      "Epoch [1830/5000], Loss: 9531.3057\n",
      "Epoch [1840/5000], Loss: 9531.3057\n",
      "Epoch [1850/5000], Loss: 9531.3057\n",
      "Epoch [1860/5000], Loss: 9531.3057\n",
      "Epoch [1870/5000], Loss: 9531.3057\n",
      "Epoch [1880/5000], Loss: 9531.3057\n",
      "Epoch [1890/5000], Loss: 9531.3057\n",
      "Epoch [1900/5000], Loss: 9531.3057\n",
      "Epoch [1910/5000], Loss: 9531.3057\n",
      "Epoch [1920/5000], Loss: 9531.3057\n",
      "Epoch [1930/5000], Loss: 9531.3057\n",
      "Epoch [1940/5000], Loss: 9531.3057\n",
      "Epoch [1950/5000], Loss: 9531.3057\n",
      "Epoch [1960/5000], Loss: 9531.3057\n",
      "Epoch [1970/5000], Loss: 9531.3057\n",
      "Epoch [1980/5000], Loss: 9531.3057\n",
      "Epoch [1990/5000], Loss: 9531.3057\n",
      "Epoch [2000/5000], Loss: 9531.3057\n",
      "Epoch [2010/5000], Loss: 9531.3057\n",
      "Epoch [2020/5000], Loss: 9531.3057\n",
      "Epoch [2030/5000], Loss: 9531.3057\n",
      "Epoch [2040/5000], Loss: 9531.3057\n",
      "Epoch [2050/5000], Loss: 9531.3057\n",
      "Epoch [2060/5000], Loss: 9531.3057\n",
      "Epoch [2070/5000], Loss: 9531.3057\n",
      "Epoch [2080/5000], Loss: 9531.3057\n",
      "Epoch [2090/5000], Loss: 9531.3057\n",
      "Epoch [2100/5000], Loss: 9531.3057\n",
      "Epoch [2110/5000], Loss: 9531.3057\n",
      "Epoch [2120/5000], Loss: 9531.3057\n",
      "Epoch [2130/5000], Loss: 9531.3057\n",
      "Epoch [2140/5000], Loss: 9531.3057\n",
      "Epoch [2150/5000], Loss: 9531.3057\n",
      "Epoch [2160/5000], Loss: 9531.3057\n",
      "Epoch [2170/5000], Loss: 9531.3057\n",
      "Epoch [2180/5000], Loss: 9531.3057\n",
      "Epoch [2190/5000], Loss: 9531.3057\n",
      "Epoch [2200/5000], Loss: 9531.3057\n",
      "Epoch [2210/5000], Loss: 9531.3057\n",
      "Epoch [2220/5000], Loss: 9531.3057\n",
      "Epoch [2230/5000], Loss: 9531.3057\n",
      "Epoch [2240/5000], Loss: 9531.3057\n",
      "Epoch [2250/5000], Loss: 9531.3057\n",
      "Epoch [2260/5000], Loss: 9531.3057\n",
      "Epoch [2270/5000], Loss: 9531.3057\n",
      "Epoch [2280/5000], Loss: 9531.3057\n",
      "Epoch [2290/5000], Loss: 9531.3057\n",
      "Epoch [2300/5000], Loss: 9531.3057\n",
      "Epoch [2310/5000], Loss: 9531.3057\n",
      "Epoch [2320/5000], Loss: 9531.3057\n",
      "Epoch [2330/5000], Loss: 9531.3057\n",
      "Epoch [2340/5000], Loss: 9531.3057\n",
      "Epoch [2350/5000], Loss: 9531.3057\n",
      "Epoch [2360/5000], Loss: 9531.3057\n",
      "Epoch [2370/5000], Loss: 9531.3057\n",
      "Epoch [2380/5000], Loss: 9531.3057\n",
      "Epoch [2390/5000], Loss: 9531.3057\n",
      "Epoch [2400/5000], Loss: 9531.3057\n",
      "Epoch [2410/5000], Loss: 9531.3057\n",
      "Epoch [2420/5000], Loss: 9531.3057\n",
      "Epoch [2430/5000], Loss: 9531.3057\n",
      "Epoch [2440/5000], Loss: 9531.3057\n",
      "Epoch [2450/5000], Loss: 9531.3057\n",
      "Epoch [2460/5000], Loss: 9531.3057\n",
      "Epoch [2470/5000], Loss: 9531.3057\n",
      "Epoch [2480/5000], Loss: 9531.3057\n",
      "Epoch [2490/5000], Loss: 9531.3057\n",
      "Epoch [2500/5000], Loss: 9531.3057\n",
      "Epoch [2510/5000], Loss: 9531.3057\n",
      "Epoch [2520/5000], Loss: 9531.3057\n",
      "Epoch [2530/5000], Loss: 9531.3057\n",
      "Epoch [2540/5000], Loss: 9531.3057\n",
      "Epoch [2550/5000], Loss: 9531.3057\n",
      "Epoch [2560/5000], Loss: 9531.3057\n",
      "Epoch [2570/5000], Loss: 9531.3057\n",
      "Epoch [2580/5000], Loss: 9531.3057\n",
      "Epoch [2590/5000], Loss: 9531.3057\n",
      "Epoch [2600/5000], Loss: 9531.3057\n",
      "Epoch [2610/5000], Loss: 9531.3057\n",
      "Epoch [2620/5000], Loss: 9531.3057\n",
      "Epoch [2630/5000], Loss: 9531.3057\n",
      "Epoch [2640/5000], Loss: 9531.3057\n",
      "Epoch [2650/5000], Loss: 9531.3057\n",
      "Epoch [2660/5000], Loss: 9531.3057\n",
      "Epoch [2670/5000], Loss: 9531.3057\n",
      "Epoch [2680/5000], Loss: 9531.3057\n",
      "Epoch [2690/5000], Loss: 9531.3057\n",
      "Epoch [2700/5000], Loss: 9531.3057\n",
      "Epoch [2710/5000], Loss: 9531.3057\n",
      "Epoch [2720/5000], Loss: 9531.3057\n",
      "Epoch [2730/5000], Loss: 9531.3057\n",
      "Epoch [2740/5000], Loss: 9531.3057\n",
      "Epoch [2750/5000], Loss: 9531.3057\n",
      "Epoch [2760/5000], Loss: 9531.3057\n",
      "Epoch [2770/5000], Loss: 9531.3057\n",
      "Epoch [2780/5000], Loss: 9531.3057\n",
      "Epoch [2790/5000], Loss: 9531.3057\n",
      "Epoch [2800/5000], Loss: 9531.3057\n",
      "Epoch [2810/5000], Loss: 9531.3057\n",
      "Epoch [2820/5000], Loss: 9531.3057\n",
      "Epoch [2830/5000], Loss: 9531.3057\n",
      "Epoch [2840/5000], Loss: 9531.3057\n",
      "Epoch [2850/5000], Loss: 9531.3057\n",
      "Epoch [2860/5000], Loss: 9531.3057\n",
      "Epoch [2870/5000], Loss: 9531.3057\n",
      "Epoch [2880/5000], Loss: 9531.3057\n",
      "Epoch [2890/5000], Loss: 9531.3057\n",
      "Epoch [2900/5000], Loss: 9531.3057\n",
      "Epoch [2910/5000], Loss: 9531.3057\n",
      "Epoch [2920/5000], Loss: 9531.3057\n",
      "Epoch [2930/5000], Loss: 9531.3057\n",
      "Epoch [2940/5000], Loss: 9531.3057\n",
      "Epoch [2950/5000], Loss: 9531.3057\n",
      "Epoch [2960/5000], Loss: 9531.3057\n",
      "Epoch [2970/5000], Loss: 9531.3057\n",
      "Epoch [2980/5000], Loss: 9531.3057\n",
      "Epoch [2990/5000], Loss: 9531.3057\n",
      "Epoch [3000/5000], Loss: 9531.3057\n",
      "Epoch [3010/5000], Loss: 9531.3057\n",
      "Epoch [3020/5000], Loss: 9531.3057\n",
      "Epoch [3030/5000], Loss: 9531.3057\n",
      "Epoch [3040/5000], Loss: 9531.3057\n",
      "Epoch [3050/5000], Loss: 9531.3057\n",
      "Epoch [3060/5000], Loss: 9531.3057\n",
      "Epoch [3070/5000], Loss: 9531.3057\n",
      "Epoch [3080/5000], Loss: 9531.3057\n",
      "Epoch [3090/5000], Loss: 9531.3057\n",
      "Epoch [3100/5000], Loss: 9531.3057\n",
      "Epoch [3110/5000], Loss: 9531.3057\n",
      "Epoch [3120/5000], Loss: 9531.3057\n",
      "Epoch [3130/5000], Loss: 9531.3057\n",
      "Epoch [3140/5000], Loss: 9531.3057\n",
      "Epoch [3150/5000], Loss: 9531.3057\n",
      "Epoch [3160/5000], Loss: 9531.3057\n",
      "Epoch [3170/5000], Loss: 9531.3057\n",
      "Epoch [3180/5000], Loss: 9531.3057\n",
      "Epoch [3190/5000], Loss: 9531.3057\n",
      "Epoch [3200/5000], Loss: 9531.3057\n",
      "Epoch [3210/5000], Loss: 9531.3057\n",
      "Epoch [3220/5000], Loss: 9531.3057\n",
      "Epoch [3230/5000], Loss: 9531.3057\n",
      "Epoch [3240/5000], Loss: 9531.3057\n",
      "Epoch [3250/5000], Loss: 9531.3057\n",
      "Epoch [3260/5000], Loss: 9531.3057\n",
      "Epoch [3270/5000], Loss: 9531.3057\n",
      "Epoch [3280/5000], Loss: 9531.3057\n",
      "Epoch [3290/5000], Loss: 9531.3057\n",
      "Epoch [3300/5000], Loss: 9531.3057\n",
      "Epoch [3310/5000], Loss: 9531.3057\n",
      "Epoch [3320/5000], Loss: 9531.3057\n",
      "Epoch [3330/5000], Loss: 9531.3057\n",
      "Epoch [3340/5000], Loss: 9531.3057\n",
      "Epoch [3350/5000], Loss: 9531.3057\n",
      "Epoch [3360/5000], Loss: 9531.3057\n",
      "Epoch [3370/5000], Loss: 9531.3057\n",
      "Epoch [3380/5000], Loss: 9531.3057\n",
      "Epoch [3390/5000], Loss: 9531.3057\n",
      "Epoch [3400/5000], Loss: 9531.3057\n",
      "Epoch [3410/5000], Loss: 9531.3057\n",
      "Epoch [3420/5000], Loss: 9531.3057\n",
      "Epoch [3430/5000], Loss: 9531.3057\n",
      "Epoch [3440/5000], Loss: 9531.3057\n",
      "Epoch [3450/5000], Loss: 9531.3057\n",
      "Epoch [3460/5000], Loss: 9531.3057\n",
      "Epoch [3470/5000], Loss: 9531.3057\n",
      "Epoch [3480/5000], Loss: 9531.3057\n",
      "Epoch [3490/5000], Loss: 9531.3057\n",
      "Epoch [3500/5000], Loss: 9531.3057\n",
      "Epoch [3510/5000], Loss: 9531.3057\n",
      "Epoch [3520/5000], Loss: 9531.3057\n",
      "Epoch [3530/5000], Loss: 9531.3057\n",
      "Epoch [3540/5000], Loss: 9531.3057\n",
      "Epoch [3550/5000], Loss: 9531.3057\n",
      "Epoch [3560/5000], Loss: 9531.3057\n",
      "Epoch [3570/5000], Loss: 9531.3057\n",
      "Epoch [3580/5000], Loss: 9531.3057\n",
      "Epoch [3590/5000], Loss: 9531.3057\n",
      "Epoch [3600/5000], Loss: 9531.3057\n",
      "Epoch [3610/5000], Loss: 9531.3057\n",
      "Epoch [3620/5000], Loss: 9531.3057\n",
      "Epoch [3630/5000], Loss: 9531.3057\n",
      "Epoch [3640/5000], Loss: 9531.3057\n",
      "Epoch [3650/5000], Loss: 9531.3057\n",
      "Epoch [3660/5000], Loss: 9531.3057\n",
      "Epoch [3670/5000], Loss: 9531.3057\n",
      "Epoch [3680/5000], Loss: 9531.3057\n",
      "Epoch [3690/5000], Loss: 9531.3057\n",
      "Epoch [3700/5000], Loss: 9531.3057\n",
      "Epoch [3710/5000], Loss: 9531.3057\n",
      "Epoch [3720/5000], Loss: 9531.3057\n",
      "Epoch [3730/5000], Loss: 9531.3057\n",
      "Epoch [3740/5000], Loss: 9531.3057\n",
      "Epoch [3750/5000], Loss: 9531.3057\n",
      "Epoch [3760/5000], Loss: 9531.3057\n",
      "Epoch [3770/5000], Loss: 9531.3057\n",
      "Epoch [3780/5000], Loss: 9531.3057\n",
      "Epoch [3790/5000], Loss: 9531.3057\n",
      "Epoch [3800/5000], Loss: 9531.3057\n",
      "Epoch [3810/5000], Loss: 9531.3057\n",
      "Epoch [3820/5000], Loss: 9531.3057\n",
      "Epoch [3830/5000], Loss: 9531.3057\n",
      "Epoch [3840/5000], Loss: 9531.3057\n",
      "Epoch [3850/5000], Loss: 9531.3057\n",
      "Epoch [3860/5000], Loss: 9531.3057\n",
      "Epoch [3870/5000], Loss: 9531.3057\n",
      "Epoch [3880/5000], Loss: 9531.3057\n",
      "Epoch [3890/5000], Loss: 9531.3057\n",
      "Epoch [3900/5000], Loss: 9531.3057\n",
      "Epoch [3910/5000], Loss: 9531.3057\n",
      "Epoch [3920/5000], Loss: 9531.3057\n",
      "Epoch [3930/5000], Loss: 9531.3057\n",
      "Epoch [3940/5000], Loss: 9531.3057\n",
      "Epoch [3950/5000], Loss: 9531.3057\n",
      "Epoch [3960/5000], Loss: 9531.3057\n",
      "Epoch [3970/5000], Loss: 9531.3057\n",
      "Epoch [3980/5000], Loss: 9531.3057\n",
      "Epoch [3990/5000], Loss: 9531.3057\n",
      "Epoch [4000/5000], Loss: 9531.3057\n",
      "Epoch [4010/5000], Loss: 9531.3057\n",
      "Epoch [4020/5000], Loss: 9531.3057\n",
      "Epoch [4030/5000], Loss: 9531.3057\n",
      "Epoch [4040/5000], Loss: 9531.3057\n",
      "Epoch [4050/5000], Loss: 9531.3057\n",
      "Epoch [4060/5000], Loss: 9531.3057\n",
      "Epoch [4070/5000], Loss: 9531.3057\n",
      "Epoch [4080/5000], Loss: 9531.3057\n",
      "Epoch [4090/5000], Loss: 9531.3057\n",
      "Epoch [4100/5000], Loss: 9531.3057\n",
      "Epoch [4110/5000], Loss: 9531.3057\n",
      "Epoch [4120/5000], Loss: 9531.3057\n",
      "Epoch [4130/5000], Loss: 9531.3057\n",
      "Epoch [4140/5000], Loss: 9531.3057\n",
      "Epoch [4150/5000], Loss: 9531.3057\n",
      "Epoch [4160/5000], Loss: 9531.3057\n",
      "Epoch [4170/5000], Loss: 9531.3057\n",
      "Epoch [4180/5000], Loss: 9531.3057\n",
      "Epoch [4190/5000], Loss: 9531.3057\n",
      "Epoch [4200/5000], Loss: 9531.3057\n",
      "Epoch [4210/5000], Loss: 9531.3057\n",
      "Epoch [4220/5000], Loss: 9531.3057\n",
      "Epoch [4230/5000], Loss: 9531.3057\n",
      "Epoch [4240/5000], Loss: 9531.3057\n",
      "Epoch [4250/5000], Loss: 9531.3057\n",
      "Epoch [4260/5000], Loss: 9531.3057\n",
      "Epoch [4270/5000], Loss: 9531.3057\n",
      "Epoch [4280/5000], Loss: 9531.3057\n",
      "Epoch [4290/5000], Loss: 9531.3057\n",
      "Epoch [4300/5000], Loss: 9531.3057\n",
      "Epoch [4310/5000], Loss: 9531.3057\n",
      "Epoch [4320/5000], Loss: 9531.3057\n",
      "Epoch [4330/5000], Loss: 9531.3057\n",
      "Epoch [4340/5000], Loss: 9531.3057\n",
      "Epoch [4350/5000], Loss: 9531.3057\n",
      "Epoch [4360/5000], Loss: 9531.3057\n",
      "Epoch [4370/5000], Loss: 9531.3057\n",
      "Epoch [4380/5000], Loss: 9531.3057\n",
      "Epoch [4390/5000], Loss: 9531.3057\n",
      "Epoch [4400/5000], Loss: 9531.3057\n",
      "Epoch [4410/5000], Loss: 9531.3057\n",
      "Epoch [4420/5000], Loss: 9531.3057\n",
      "Epoch [4430/5000], Loss: 9531.3057\n",
      "Epoch [4440/5000], Loss: 9531.3057\n",
      "Epoch [4450/5000], Loss: 9531.3057\n",
      "Epoch [4460/5000], Loss: 9531.3057\n",
      "Epoch [4470/5000], Loss: 9531.3057\n",
      "Epoch [4480/5000], Loss: 9531.3057\n",
      "Epoch [4490/5000], Loss: 9531.3057\n",
      "Epoch [4500/5000], Loss: 9531.3057\n",
      "Epoch [4510/5000], Loss: 9531.3057\n",
      "Epoch [4520/5000], Loss: 9531.3057\n",
      "Epoch [4530/5000], Loss: 9531.3057\n",
      "Epoch [4540/5000], Loss: 9531.3057\n",
      "Epoch [4550/5000], Loss: 9531.3057\n",
      "Epoch [4560/5000], Loss: 9531.3057\n",
      "Epoch [4570/5000], Loss: 9531.3057\n",
      "Epoch [4580/5000], Loss: 9531.3057\n",
      "Epoch [4590/5000], Loss: 9531.3057\n",
      "Epoch [4600/5000], Loss: 9531.3057\n",
      "Epoch [4610/5000], Loss: 9531.3057\n",
      "Epoch [4620/5000], Loss: 9531.3057\n",
      "Epoch [4630/5000], Loss: 9531.3057\n",
      "Epoch [4640/5000], Loss: 9531.3057\n",
      "Epoch [4650/5000], Loss: 9531.3057\n",
      "Epoch [4660/5000], Loss: 9531.3057\n",
      "Epoch [4670/5000], Loss: 9531.3057\n",
      "Epoch [4680/5000], Loss: 9531.3057\n",
      "Epoch [4690/5000], Loss: 9531.3057\n",
      "Epoch [4700/5000], Loss: 9531.3057\n",
      "Epoch [4710/5000], Loss: 9531.3057\n",
      "Epoch [4720/5000], Loss: 9531.3057\n",
      "Epoch [4730/5000], Loss: 9531.3057\n",
      "Epoch [4740/5000], Loss: 9531.3057\n",
      "Epoch [4750/5000], Loss: 9531.3057\n",
      "Epoch [4760/5000], Loss: 9531.3057\n",
      "Epoch [4770/5000], Loss: 9531.3057\n",
      "Epoch [4780/5000], Loss: 9531.3057\n",
      "Epoch [4790/5000], Loss: 9531.3057\n",
      "Epoch [4800/5000], Loss: 9531.3057\n",
      "Epoch [4810/5000], Loss: 9531.3057\n",
      "Epoch [4820/5000], Loss: 9531.3057\n",
      "Epoch [4830/5000], Loss: 9531.3057\n",
      "Epoch [4840/5000], Loss: 9531.3057\n",
      "Epoch [4850/5000], Loss: 9531.3057\n",
      "Epoch [4860/5000], Loss: 9531.3057\n",
      "Epoch [4870/5000], Loss: 9531.3057\n",
      "Epoch [4880/5000], Loss: 9531.3057\n",
      "Epoch [4890/5000], Loss: 9531.3057\n",
      "Epoch [4900/5000], Loss: 9531.3057\n",
      "Epoch [4910/5000], Loss: 9531.3057\n",
      "Epoch [4920/5000], Loss: 9531.3057\n",
      "Epoch [4930/5000], Loss: 9531.3057\n",
      "Epoch [4940/5000], Loss: 9531.3057\n",
      "Epoch [4950/5000], Loss: 9531.3057\n",
      "Epoch [4960/5000], Loss: 9531.3057\n",
      "Epoch [4970/5000], Loss: 9531.3057\n",
      "Epoch [4980/5000], Loss: 9531.3057\n",
      "Epoch [4990/5000], Loss: 9531.3057\n",
      "Epoch [5000/5000], Loss: 9531.3057\n",
      "Mean Absolute Error (MAE): 78.07531795683708\n",
      "Mean Squared Error (MSE): 9817.85929593307\n",
      "Root Mean Squared Error (RMSE): 99.08511137367243\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "from talib import RSI, MACD, STOCH, OBV, BBANDS\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Fetch historical data from Yahoo Finance\n",
    "ticker = 'TSLA'  # Tesla stock ticker\n",
    "start_date = '2010-01-01'  # Start date of historical data\n",
    "end_date = '2023-07-05'  # End date of historical data\n",
    "\n",
    "data = yf.download(ticker, start=start_date, end=end_date)\n",
    "\n",
    "# Calculate moving averages\n",
    "data['MA_5'] = data['Close'].rolling(window=5).mean()\n",
    "data['MA_10'] = data['Close'].rolling(window=10).mean()\n",
    "data['MA_20'] = data['Close'].rolling(window=20).mean()\n",
    "\n",
    "# Calculate RSI\n",
    "rsi_period = 14  # RSI calculation period\n",
    "data['RSI'] = RSI(data['Close'], timeperiod=rsi_period)\n",
    "\n",
    "# Calculate MACD\n",
    "macd, macd_signal, _ = MACD(data['Close'])\n",
    "data['MACD'] = macd\n",
    "\n",
    "# Calculate Stochastic Oscillator\n",
    "slowk, slowd = STOCH(data['High'], data['Low'], data['Close'])\n",
    "data['SlowK'] = slowk\n",
    "data['SlowD'] = slowd\n",
    "\n",
    "# Calculate On-Balance Volume (OBV)\n",
    "obv = OBV(data['Close'], data['Volume'])\n",
    "data['OBV'] = obv\n",
    "\n",
    "# Add additional indicators\n",
    "upper_band, middle_band, lower_band = BBANDS(data['Close'], timeperiod=20)\n",
    "data['UpperBand'] = upper_band\n",
    "data['MiddleBand'] = middle_band\n",
    "data['LowerBand'] = lower_band\n",
    "\n",
    "# Prepare the data\n",
    "X = data[['Open', 'Volume', 'MA_5', 'MA_10', 'MA_20', 'RSI', 'MACD', 'SlowK', 'SlowD', 'OBV', 'UpperBand', 'MiddleBand', 'LowerBand']].iloc[:-1]  # Adjusted for predicting next day's price\n",
    "y = data['Close'].shift(-1)[:-1]  # Adjusted for predicting next day's price\n",
    "\n",
    "# Store column names\n",
    "column_names = X.columns\n",
    "\n",
    "# Handle missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X = imputer.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train = torch.from_numpy(X_train).float()\n",
    "y_train = torch.from_numpy(y_train.values).float()\n",
    "X_test = torch.from_numpy(X_test).float()\n",
    "\n",
    "# Define the Neural Network class\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 32)\n",
    "        self.fc2 = nn.Linear(32, 16)\n",
    "        self.fc3 = nn.Linear(16, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        out = self.fc3(x)\n",
    "        return out\n",
    "\n",
    "# Instantiate the neural network\n",
    "input_size = X_train.shape[1]\n",
    "neural_network = NeuralNetwork(input_size)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(neural_network.parameters(), lr=0.01)\n",
    "\n",
    "# Train the neural network\n",
    "num_epochs = 5000\n",
    "batch_size = 32\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(0, X_train.size(0), batch_size):\n",
    "        inputs = X_train[i:i+batch_size]\n",
    "        labels = y_train[i:i+batch_size]\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = neural_network(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Make predictions using the trained neural network\n",
    "neural_network.eval()  # Switch to evaluation mode\n",
    "predictions = neural_network(X_test)\n",
    "\n",
    "# Convert predictions tensor to numpy array\n",
    "predictions = predictions.detach().numpy().flatten()\n",
    "\n",
    "# Evaluate the model\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "\n",
    "# Save the model\n",
    "torch.save(neural_network.state_dict(), 'neural_network_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "Epoch [10/5000], Loss: 2539223808.0000\n",
      "Epoch [20/5000], Loss: 398233376.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sami/anaconda3/envs/tf/lib/python3.10/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([100])) that is different to the input size (torch.Size([100, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/sami/anaconda3/envs/tf/lib/python3.10/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([19])) that is different to the input size (torch.Size([19, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/5000], Loss: 393085920.0000\n",
      "Epoch [40/5000], Loss: 229916064.0000\n",
      "Epoch [50/5000], Loss: 148816896.0000\n",
      "Epoch [60/5000], Loss: 239523648.0000\n",
      "Epoch [70/5000], Loss: 2025763840.0000\n",
      "Epoch [80/5000], Loss: 1378121.6250\n",
      "Epoch [90/5000], Loss: 1269828.1250\n",
      "Epoch [100/5000], Loss: 1198941696.0000\n",
      "Epoch [110/5000], Loss: 54092764.0000\n",
      "Epoch [120/5000], Loss: 220536176.0000\n",
      "Epoch [130/5000], Loss: 744420864.0000\n",
      "Epoch [140/5000], Loss: 1500958080.0000\n",
      "Epoch [150/5000], Loss: 467614432.0000\n",
      "Epoch [160/5000], Loss: 20064174080.0000\n",
      "Epoch [170/5000], Loss: 2798568192.0000\n",
      "Epoch [180/5000], Loss: 20552770.0000\n",
      "Epoch [190/5000], Loss: 5893415.5000\n",
      "Epoch [200/5000], Loss: 1864069.1250\n",
      "Epoch [210/5000], Loss: 304032384.0000\n",
      "Epoch [220/5000], Loss: 762545.4375\n",
      "Epoch [230/5000], Loss: 8154342.5000\n",
      "Epoch [240/5000], Loss: 13128400.0000\n",
      "Epoch [250/5000], Loss: 34328140.0000\n",
      "Epoch [260/5000], Loss: 30382488.0000\n",
      "Epoch [270/5000], Loss: 22986066.0000\n",
      "Epoch [280/5000], Loss: 30886012.0000\n",
      "Epoch [290/5000], Loss: 216745344.0000\n",
      "Epoch [300/5000], Loss: 30213242.0000\n",
      "Epoch [310/5000], Loss: 4981934.5000\n",
      "Epoch [320/5000], Loss: 858323.6875\n",
      "Epoch [330/5000], Loss: 67960.5391\n",
      "Epoch [340/5000], Loss: 11343.1191\n",
      "Epoch [350/5000], Loss: 11520.0332\n",
      "Epoch [360/5000], Loss: 11519.7842\n",
      "Epoch [370/5000], Loss: 11519.4824\n",
      "Epoch [380/5000], Loss: 11519.1299\n",
      "Epoch [390/5000], Loss: 11518.7256\n",
      "Epoch [400/5000], Loss: 11518.2588\n",
      "Epoch [410/5000], Loss: 11517.7207\n",
      "Epoch [420/5000], Loss: 11517.1064\n",
      "Epoch [430/5000], Loss: 11516.3994\n",
      "Epoch [440/5000], Loss: 11515.5879\n",
      "Epoch [450/5000], Loss: 11514.6572\n",
      "Epoch [460/5000], Loss: 11513.5898\n",
      "Epoch [470/5000], Loss: 11512.3652\n",
      "Epoch [480/5000], Loss: 11510.9600\n",
      "Epoch [490/5000], Loss: 11509.3545\n",
      "Epoch [500/5000], Loss: 11507.5107\n",
      "Epoch [510/5000], Loss: 11505.3945\n",
      "Epoch [520/5000], Loss: 11502.9688\n",
      "Epoch [530/5000], Loss: 11500.1924\n",
      "Epoch [540/5000], Loss: 11497.0098\n",
      "Epoch [550/5000], Loss: 11493.3682\n",
      "Epoch [560/5000], Loss: 11489.2061\n",
      "Epoch [570/5000], Loss: 11484.4482\n",
      "Epoch [580/5000], Loss: 11479.0088\n",
      "Epoch [590/5000], Loss: 11472.7910\n",
      "Epoch [600/5000], Loss: 11465.6924\n",
      "Epoch [610/5000], Loss: 11457.5850\n",
      "Epoch [620/5000], Loss: 11448.3271\n",
      "Epoch [630/5000], Loss: 11437.7607\n",
      "Epoch [640/5000], Loss: 11425.7061\n",
      "Epoch [650/5000], Loss: 11411.9609\n",
      "Epoch [660/5000], Loss: 11396.2939\n",
      "Epoch [670/5000], Loss: 11378.4551\n",
      "Epoch [680/5000], Loss: 11358.1611\n",
      "Epoch [690/5000], Loss: 11335.0898\n",
      "Epoch [700/5000], Loss: 11308.8984\n",
      "Epoch [710/5000], Loss: 11279.1982\n",
      "Epoch [720/5000], Loss: 11245.5508\n",
      "Epoch [730/5000], Loss: 11207.5098\n",
      "Epoch [740/5000], Loss: 11164.5986\n",
      "Epoch [750/5000], Loss: 11116.2979\n",
      "Epoch [760/5000], Loss: 11061.9600\n",
      "Epoch [770/5000], Loss: 11086.3682\n",
      "Epoch [780/5000], Loss: 11065.8447\n",
      "Epoch [790/5000], Loss: 11042.5371\n",
      "Epoch [800/5000], Loss: 11016.0938\n",
      "Epoch [810/5000], Loss: 10986.1416\n",
      "Epoch [820/5000], Loss: 10952.2676\n",
      "Epoch [830/5000], Loss: 10914.0322\n",
      "Epoch [840/5000], Loss: 10870.9795\n",
      "Epoch [850/5000], Loss: 10822.6250\n",
      "Epoch [860/5000], Loss: 10768.4961\n",
      "Epoch [870/5000], Loss: 10708.1211\n",
      "Epoch [880/5000], Loss: 10641.0723\n",
      "Epoch [890/5000], Loss: 10566.9805\n",
      "Epoch [900/5000], Loss: 10485.5801\n",
      "Epoch [910/5000], Loss: 10396.7412\n",
      "Epoch [920/5000], Loss: 10300.5059\n",
      "Epoch [930/5000], Loss: 10197.1348\n",
      "Epoch [940/5000], Loss: 10087.1230\n",
      "Epoch [950/5000], Loss: 9971.2256\n",
      "Epoch [960/5000], Loss: 9850.4346\n",
      "Epoch [970/5000], Loss: 9725.9629\n",
      "Epoch [980/5000], Loss: 9599.1807\n",
      "Epoch [990/5000], Loss: 9471.5596\n",
      "Epoch [1000/5000], Loss: 9344.5879\n",
      "Epoch [1010/5000], Loss: 9219.7051\n",
      "Epoch [1020/5000], Loss: 9098.2373\n",
      "Epoch [1030/5000], Loss: 8981.3691\n",
      "Epoch [1040/5000], Loss: 8870.0986\n",
      "Epoch [1050/5000], Loss: 8765.2412\n",
      "Epoch [1060/5000], Loss: 8667.4248\n",
      "Epoch [1070/5000], Loss: 8577.1104\n",
      "Epoch [1080/5000], Loss: 8494.5938\n",
      "Epoch [1090/5000], Loss: 8420.0352\n",
      "Epoch [1100/5000], Loss: 8353.4668\n",
      "Epoch [1110/5000], Loss: 8294.8086\n",
      "Epoch [1120/5000], Loss: 8243.8848\n",
      "Epoch [1130/5000], Loss: 8200.4297\n",
      "Epoch [1140/5000], Loss: 8164.0928\n",
      "Epoch [1150/5000], Loss: 8134.4531\n",
      "Epoch [1160/5000], Loss: 8111.0151\n",
      "Epoch [1170/5000], Loss: 8093.2236\n",
      "Epoch [1180/5000], Loss: 8080.4614\n",
      "Epoch [1190/5000], Loss: 8072.0649\n",
      "Epoch [1200/5000], Loss: 8067.3379\n",
      "Epoch [1210/5000], Loss: 8065.5693\n",
      "Epoch [1220/5000], Loss: 8066.0566\n",
      "Epoch [1230/5000], Loss: 8068.1357\n",
      "Epoch [1240/5000], Loss: 8071.2070\n",
      "Epoch [1250/5000], Loss: 8074.7637\n",
      "Epoch [1260/5000], Loss: 8078.4087\n",
      "Epoch [1270/5000], Loss: 8081.8604\n",
      "Epoch [1280/5000], Loss: 8084.9468\n",
      "Epoch [1290/5000], Loss: 8087.5850\n",
      "Epoch [1300/5000], Loss: 8089.7617\n",
      "Epoch [1310/5000], Loss: 8091.5068\n",
      "Epoch [1320/5000], Loss: 8092.8760\n",
      "Epoch [1330/5000], Loss: 8093.9292\n",
      "Epoch [1340/5000], Loss: 8094.7280\n",
      "Epoch [1350/5000], Loss: 8095.3276\n",
      "Epoch [1360/5000], Loss: 8095.7749\n",
      "Epoch [1370/5000], Loss: 8096.1045\n",
      "Epoch [1380/5000], Loss: 8096.3477\n",
      "Epoch [1390/5000], Loss: 8096.5244\n",
      "Epoch [1400/5000], Loss: 8096.6553\n",
      "Epoch [1410/5000], Loss: 8096.7485\n",
      "Epoch [1420/5000], Loss: 8096.8179\n",
      "Epoch [1430/5000], Loss: 8096.8687\n",
      "Epoch [1440/5000], Loss: 8096.9043\n",
      "Epoch [1450/5000], Loss: 8096.9316\n",
      "Epoch [1460/5000], Loss: 8096.9502\n",
      "Epoch [1470/5000], Loss: 8096.9634\n",
      "Epoch [1480/5000], Loss: 8096.9746\n",
      "Epoch [1490/5000], Loss: 8096.9819\n",
      "Epoch [1500/5000], Loss: 8096.9854\n",
      "Epoch [1510/5000], Loss: 8096.9902\n",
      "Epoch [1520/5000], Loss: 8096.9932\n",
      "Epoch [1530/5000], Loss: 8096.9951\n",
      "Epoch [1540/5000], Loss: 8096.9951\n",
      "Epoch [1550/5000], Loss: 8096.9951\n",
      "Epoch [1560/5000], Loss: 8096.9951\n",
      "Epoch [1570/5000], Loss: 8096.9951\n",
      "Epoch [1580/5000], Loss: 8096.9951\n",
      "Epoch [1590/5000], Loss: 8096.9951\n",
      "Epoch [1600/5000], Loss: 8096.9951\n",
      "Epoch [1610/5000], Loss: 8096.9951\n",
      "Epoch [1620/5000], Loss: 8096.9951\n",
      "Epoch [1630/5000], Loss: 8096.9951\n",
      "Epoch [1640/5000], Loss: 8096.9951\n",
      "Epoch [1650/5000], Loss: 8096.9951\n",
      "Epoch [1660/5000], Loss: 8096.9951\n",
      "Epoch [1670/5000], Loss: 8096.9951\n",
      "Epoch [1680/5000], Loss: 8096.9951\n",
      "Epoch [1690/5000], Loss: 8096.9951\n",
      "Epoch [1700/5000], Loss: 8096.9951\n",
      "Epoch [1710/5000], Loss: 8096.9951\n",
      "Epoch [1720/5000], Loss: 8096.9951\n",
      "Epoch [1730/5000], Loss: 8096.9951\n",
      "Epoch [1740/5000], Loss: 8096.9951\n",
      "Epoch [1750/5000], Loss: 8096.9951\n",
      "Epoch [1760/5000], Loss: 8096.9951\n",
      "Epoch [1770/5000], Loss: 8096.9951\n",
      "Epoch [1780/5000], Loss: 8096.9951\n",
      "Epoch [1790/5000], Loss: 8096.9951\n",
      "Epoch [1800/5000], Loss: 8096.9951\n",
      "Epoch [1810/5000], Loss: 8096.9951\n",
      "Epoch [1820/5000], Loss: 8096.9951\n",
      "Epoch [1830/5000], Loss: 8096.9951\n",
      "Epoch [1840/5000], Loss: 8096.9951\n",
      "Epoch [1850/5000], Loss: 8096.9951\n",
      "Epoch [1860/5000], Loss: 8096.9951\n",
      "Epoch [1870/5000], Loss: 8096.9951\n",
      "Epoch [1880/5000], Loss: 8096.9951\n",
      "Epoch [1890/5000], Loss: 8096.9951\n",
      "Epoch [1900/5000], Loss: 8096.9951\n",
      "Epoch [1910/5000], Loss: 8096.9951\n",
      "Epoch [1920/5000], Loss: 8096.9951\n",
      "Epoch [1930/5000], Loss: 8096.9951\n",
      "Epoch [1940/5000], Loss: 8096.9951\n",
      "Epoch [1950/5000], Loss: 8096.9951\n",
      "Epoch [1960/5000], Loss: 8096.9951\n",
      "Epoch [1970/5000], Loss: 8096.9951\n",
      "Epoch [1980/5000], Loss: 8096.9951\n",
      "Epoch [1990/5000], Loss: 8096.9951\n",
      "Epoch [2000/5000], Loss: 8096.9951\n",
      "Epoch [2010/5000], Loss: 8096.9951\n",
      "Epoch [2020/5000], Loss: 8096.9951\n",
      "Epoch [2030/5000], Loss: 8096.9951\n",
      "Epoch [2040/5000], Loss: 8096.9951\n",
      "Epoch [2050/5000], Loss: 8096.9951\n",
      "Epoch [2060/5000], Loss: 8096.9951\n",
      "Epoch [2070/5000], Loss: 8096.9951\n",
      "Epoch [2080/5000], Loss: 8096.9951\n",
      "Epoch [2090/5000], Loss: 8096.9951\n",
      "Epoch [2100/5000], Loss: 8096.9951\n",
      "Epoch [2110/5000], Loss: 8096.9951\n",
      "Epoch [2120/5000], Loss: 8096.9951\n",
      "Epoch [2130/5000], Loss: 8096.9951\n",
      "Epoch [2140/5000], Loss: 8096.9951\n",
      "Epoch [2150/5000], Loss: 8096.9951\n",
      "Epoch [2160/5000], Loss: 8096.9951\n",
      "Epoch [2170/5000], Loss: 8096.9951\n",
      "Epoch [2180/5000], Loss: 8096.9951\n",
      "Epoch [2190/5000], Loss: 8096.9951\n",
      "Epoch [2200/5000], Loss: 8096.9951\n",
      "Epoch [2210/5000], Loss: 8096.9951\n",
      "Epoch [2220/5000], Loss: 8096.9951\n",
      "Epoch [2230/5000], Loss: 8096.9951\n",
      "Epoch [2240/5000], Loss: 8096.9951\n",
      "Epoch [2250/5000], Loss: 8096.9951\n",
      "Epoch [2260/5000], Loss: 8096.9951\n",
      "Epoch [2270/5000], Loss: 8096.9951\n",
      "Epoch [2280/5000], Loss: 8096.9951\n",
      "Epoch [2290/5000], Loss: 8096.9951\n",
      "Epoch [2300/5000], Loss: 8096.9951\n",
      "Epoch [2310/5000], Loss: 8096.9951\n",
      "Epoch [2320/5000], Loss: 8096.9951\n",
      "Epoch [2330/5000], Loss: 8096.9951\n",
      "Epoch [2340/5000], Loss: 8096.9951\n",
      "Epoch [2350/5000], Loss: 8096.9951\n",
      "Epoch [2360/5000], Loss: 8096.9951\n",
      "Epoch [2370/5000], Loss: 8096.9951\n",
      "Epoch [2380/5000], Loss: 8096.9951\n",
      "Epoch [2390/5000], Loss: 8096.9951\n",
      "Epoch [2400/5000], Loss: 8096.9951\n",
      "Epoch [2410/5000], Loss: 8096.9951\n",
      "Epoch [2420/5000], Loss: 8096.9951\n",
      "Epoch [2430/5000], Loss: 8096.9951\n",
      "Epoch [2440/5000], Loss: 8096.9951\n",
      "Epoch [2450/5000], Loss: 8096.9951\n",
      "Epoch [2460/5000], Loss: 8096.9951\n",
      "Epoch [2470/5000], Loss: 8096.9951\n",
      "Epoch [2480/5000], Loss: 8096.9951\n",
      "Epoch [2490/5000], Loss: 8096.9951\n",
      "Epoch [2500/5000], Loss: 8096.9951\n",
      "Epoch [2510/5000], Loss: 8096.9951\n",
      "Epoch [2520/5000], Loss: 8096.9951\n",
      "Epoch [2530/5000], Loss: 8096.9951\n",
      "Epoch [2540/5000], Loss: 8096.9951\n",
      "Epoch [2550/5000], Loss: 8096.9951\n",
      "Epoch [2560/5000], Loss: 8096.9951\n",
      "Epoch [2570/5000], Loss: 8096.9951\n",
      "Epoch [2580/5000], Loss: 8096.9951\n",
      "Epoch [2590/5000], Loss: 8096.9951\n",
      "Epoch [2600/5000], Loss: 8096.9951\n",
      "Epoch [2610/5000], Loss: 8096.9951\n",
      "Epoch [2620/5000], Loss: 8096.9951\n",
      "Epoch [2630/5000], Loss: 8096.9951\n",
      "Epoch [2640/5000], Loss: 8096.9951\n",
      "Epoch [2650/5000], Loss: 8096.9951\n",
      "Epoch [2660/5000], Loss: 8096.9951\n",
      "Epoch [2670/5000], Loss: 8096.9951\n",
      "Epoch [2680/5000], Loss: 8096.9951\n",
      "Epoch [2690/5000], Loss: 8096.9951\n",
      "Epoch [2700/5000], Loss: 8096.9951\n",
      "Epoch [2710/5000], Loss: 8096.9951\n",
      "Epoch [2720/5000], Loss: 8096.9951\n",
      "Epoch [2730/5000], Loss: 8096.9951\n",
      "Epoch [2740/5000], Loss: 8096.9951\n",
      "Epoch [2750/5000], Loss: 8096.9951\n",
      "Epoch [2760/5000], Loss: 8096.9951\n",
      "Epoch [2770/5000], Loss: 8096.9951\n",
      "Epoch [2780/5000], Loss: 8096.9951\n",
      "Epoch [2790/5000], Loss: 8096.9951\n",
      "Epoch [2800/5000], Loss: 8096.9951\n",
      "Epoch [2810/5000], Loss: 8096.9951\n",
      "Epoch [2820/5000], Loss: 8096.9951\n",
      "Epoch [2830/5000], Loss: 8096.9951\n",
      "Epoch [2840/5000], Loss: 8096.9951\n",
      "Epoch [2850/5000], Loss: 8096.9951\n",
      "Epoch [2860/5000], Loss: 8096.9951\n",
      "Epoch [2870/5000], Loss: 8096.9951\n",
      "Epoch [2880/5000], Loss: 8096.9951\n",
      "Epoch [2890/5000], Loss: 8096.9951\n",
      "Epoch [2900/5000], Loss: 8096.9951\n",
      "Epoch [2910/5000], Loss: 8096.9951\n",
      "Epoch [2920/5000], Loss: 8096.9951\n",
      "Epoch [2930/5000], Loss: 8096.9951\n",
      "Epoch [2940/5000], Loss: 8096.9951\n",
      "Epoch [2950/5000], Loss: 8096.9951\n",
      "Epoch [2960/5000], Loss: 8096.9951\n",
      "Epoch [2970/5000], Loss: 8096.9951\n",
      "Epoch [2980/5000], Loss: 8096.9951\n",
      "Epoch [2990/5000], Loss: 8096.9951\n",
      "Epoch [3000/5000], Loss: 8096.9951\n",
      "Epoch [3010/5000], Loss: 8096.9951\n",
      "Epoch [3020/5000], Loss: 8096.9951\n",
      "Epoch [3030/5000], Loss: 8096.9951\n",
      "Epoch [3040/5000], Loss: 8096.9951\n",
      "Epoch [3050/5000], Loss: 8096.9951\n",
      "Epoch [3060/5000], Loss: 8096.9951\n",
      "Epoch [3070/5000], Loss: 8096.9951\n",
      "Epoch [3080/5000], Loss: 8096.9951\n",
      "Epoch [3090/5000], Loss: 8096.9951\n",
      "Epoch [3100/5000], Loss: 8096.9951\n",
      "Epoch [3110/5000], Loss: 8096.9951\n",
      "Epoch [3120/5000], Loss: 8096.9951\n",
      "Epoch [3130/5000], Loss: 8096.9951\n",
      "Epoch [3140/5000], Loss: 8096.9951\n",
      "Epoch [3150/5000], Loss: 8096.9951\n",
      "Epoch [3160/5000], Loss: 8096.9951\n",
      "Epoch [3170/5000], Loss: 8096.9951\n",
      "Epoch [3180/5000], Loss: 8096.9951\n",
      "Epoch [3190/5000], Loss: 8096.9951\n",
      "Epoch [3200/5000], Loss: 8096.9951\n",
      "Epoch [3210/5000], Loss: 8096.9951\n",
      "Epoch [3220/5000], Loss: 8096.9951\n",
      "Epoch [3230/5000], Loss: 8096.9951\n",
      "Epoch [3240/5000], Loss: 8096.9951\n",
      "Epoch [3250/5000], Loss: 8096.9951\n",
      "Epoch [3260/5000], Loss: 8096.9951\n",
      "Epoch [3270/5000], Loss: 8096.9951\n",
      "Epoch [3280/5000], Loss: 8096.9951\n",
      "Epoch [3290/5000], Loss: 8096.9951\n",
      "Epoch [3300/5000], Loss: 8096.9951\n",
      "Epoch [3310/5000], Loss: 8096.9951\n",
      "Epoch [3320/5000], Loss: 8096.9951\n",
      "Epoch [3330/5000], Loss: 8096.9951\n",
      "Epoch [3340/5000], Loss: 8096.9951\n",
      "Epoch [3350/5000], Loss: 8096.9951\n",
      "Epoch [3360/5000], Loss: 8096.9951\n",
      "Epoch [3370/5000], Loss: 8096.9951\n",
      "Epoch [3380/5000], Loss: 8096.9951\n",
      "Epoch [3390/5000], Loss: 8096.9951\n",
      "Epoch [3400/5000], Loss: 8096.9951\n",
      "Epoch [3410/5000], Loss: 8096.9951\n",
      "Epoch [3420/5000], Loss: 8096.9951\n",
      "Epoch [3430/5000], Loss: 8096.9951\n",
      "Epoch [3440/5000], Loss: 8096.9951\n",
      "Epoch [3450/5000], Loss: 8096.9951\n",
      "Epoch [3460/5000], Loss: 8096.9951\n",
      "Epoch [3470/5000], Loss: 8096.9951\n",
      "Epoch [3480/5000], Loss: 8096.9951\n",
      "Epoch [3490/5000], Loss: 8096.9951\n",
      "Epoch [3500/5000], Loss: 8096.9951\n",
      "Epoch [3510/5000], Loss: 8096.9951\n",
      "Epoch [3520/5000], Loss: 8096.9951\n",
      "Epoch [3530/5000], Loss: 8096.9951\n",
      "Epoch [3540/5000], Loss: 8096.9951\n",
      "Epoch [3550/5000], Loss: 8096.9951\n",
      "Epoch [3560/5000], Loss: 8096.9951\n",
      "Epoch [3570/5000], Loss: 8096.9951\n",
      "Epoch [3580/5000], Loss: 8096.9951\n",
      "Epoch [3590/5000], Loss: 8096.9951\n",
      "Epoch [3600/5000], Loss: 8096.9951\n",
      "Epoch [3610/5000], Loss: 8096.9951\n",
      "Epoch [3620/5000], Loss: 8096.9951\n",
      "Epoch [3630/5000], Loss: 8096.9951\n",
      "Epoch [3640/5000], Loss: 8096.9951\n",
      "Epoch [3650/5000], Loss: 8096.9951\n",
      "Epoch [3660/5000], Loss: 8096.9951\n",
      "Epoch [3670/5000], Loss: 8096.9951\n",
      "Epoch [3680/5000], Loss: 8096.9951\n",
      "Epoch [3690/5000], Loss: 8096.9951\n",
      "Epoch [3700/5000], Loss: 8096.9951\n",
      "Epoch [3710/5000], Loss: 8096.9951\n",
      "Epoch [3720/5000], Loss: 8096.9951\n",
      "Epoch [3730/5000], Loss: 8096.9951\n",
      "Epoch [3740/5000], Loss: 8096.9951\n",
      "Epoch [3750/5000], Loss: 8096.9951\n",
      "Epoch [3760/5000], Loss: 8096.9951\n",
      "Epoch [3770/5000], Loss: 8096.9951\n",
      "Epoch [3780/5000], Loss: 8096.9951\n",
      "Epoch [3790/5000], Loss: 8096.9951\n",
      "Epoch [3800/5000], Loss: 8096.9951\n",
      "Epoch [3810/5000], Loss: 8096.9951\n",
      "Epoch [3820/5000], Loss: 8096.9951\n",
      "Epoch [3830/5000], Loss: 8096.9951\n",
      "Epoch [3840/5000], Loss: 8096.9951\n",
      "Epoch [3850/5000], Loss: 8096.9951\n",
      "Epoch [3860/5000], Loss: 8096.9951\n",
      "Epoch [3870/5000], Loss: 8096.9951\n",
      "Epoch [3880/5000], Loss: 8096.9951\n",
      "Epoch [3890/5000], Loss: 8096.9951\n",
      "Epoch [3900/5000], Loss: 8096.9951\n",
      "Epoch [3910/5000], Loss: 8096.9951\n",
      "Epoch [3920/5000], Loss: 8096.9951\n",
      "Epoch [3930/5000], Loss: 8096.9951\n",
      "Epoch [3940/5000], Loss: 8096.9951\n",
      "Epoch [3950/5000], Loss: 8096.9951\n",
      "Epoch [3960/5000], Loss: 8096.9951\n",
      "Epoch [3970/5000], Loss: 8096.9951\n",
      "Epoch [3980/5000], Loss: 8096.9951\n",
      "Epoch [3990/5000], Loss: 8096.9951\n",
      "Epoch [4000/5000], Loss: 8096.9951\n",
      "Epoch [4010/5000], Loss: 8096.9951\n",
      "Epoch [4020/5000], Loss: 8096.9951\n",
      "Epoch [4030/5000], Loss: 8096.9951\n",
      "Epoch [4040/5000], Loss: 8096.9951\n",
      "Epoch [4050/5000], Loss: 8096.9951\n",
      "Epoch [4060/5000], Loss: 8096.9951\n",
      "Epoch [4070/5000], Loss: 8096.9951\n",
      "Epoch [4080/5000], Loss: 8096.9951\n",
      "Epoch [4090/5000], Loss: 8096.9951\n",
      "Epoch [4100/5000], Loss: 8096.9951\n",
      "Epoch [4110/5000], Loss: 8096.9951\n",
      "Epoch [4120/5000], Loss: 8096.9951\n",
      "Epoch [4130/5000], Loss: 8096.9951\n",
      "Epoch [4140/5000], Loss: 8096.9951\n",
      "Epoch [4150/5000], Loss: 8096.9951\n",
      "Epoch [4160/5000], Loss: 8096.9951\n",
      "Epoch [4170/5000], Loss: 8096.9951\n",
      "Epoch [4180/5000], Loss: 8096.9951\n",
      "Epoch [4190/5000], Loss: 8096.9951\n",
      "Epoch [4200/5000], Loss: 8096.9951\n",
      "Epoch [4210/5000], Loss: 8096.9951\n",
      "Epoch [4220/5000], Loss: 8096.9951\n",
      "Epoch [4230/5000], Loss: 8096.9951\n",
      "Epoch [4240/5000], Loss: 8096.9951\n",
      "Epoch [4250/5000], Loss: 8096.9951\n",
      "Epoch [4260/5000], Loss: 8096.9951\n",
      "Epoch [4270/5000], Loss: 8096.9951\n",
      "Epoch [4280/5000], Loss: 8096.9951\n",
      "Epoch [4290/5000], Loss: 8096.9951\n",
      "Epoch [4300/5000], Loss: 8096.9951\n",
      "Epoch [4310/5000], Loss: 8096.9951\n",
      "Epoch [4320/5000], Loss: 8096.9951\n",
      "Epoch [4330/5000], Loss: 8096.9951\n",
      "Epoch [4340/5000], Loss: 8096.9951\n",
      "Epoch [4350/5000], Loss: 8096.9951\n",
      "Epoch [4360/5000], Loss: 8096.9951\n",
      "Epoch [4370/5000], Loss: 8096.9951\n",
      "Epoch [4380/5000], Loss: 8096.9951\n",
      "Epoch [4390/5000], Loss: 8096.9951\n",
      "Epoch [4400/5000], Loss: 8096.9951\n",
      "Epoch [4410/5000], Loss: 8096.9951\n",
      "Epoch [4420/5000], Loss: 8096.9951\n",
      "Epoch [4430/5000], Loss: 8096.9951\n",
      "Epoch [4440/5000], Loss: 8096.9951\n",
      "Epoch [4450/5000], Loss: 8096.9951\n",
      "Epoch [4460/5000], Loss: 8096.9951\n",
      "Epoch [4470/5000], Loss: 8096.9951\n",
      "Epoch [4480/5000], Loss: 8096.9951\n",
      "Epoch [4490/5000], Loss: 8096.9951\n",
      "Epoch [4500/5000], Loss: 8096.9951\n",
      "Epoch [4510/5000], Loss: 8096.9951\n",
      "Epoch [4520/5000], Loss: 8096.9951\n",
      "Epoch [4530/5000], Loss: 8096.9951\n",
      "Epoch [4540/5000], Loss: 8096.9951\n",
      "Epoch [4550/5000], Loss: 8096.9951\n",
      "Epoch [4560/5000], Loss: 8096.9951\n",
      "Epoch [4570/5000], Loss: 8096.9951\n",
      "Epoch [4580/5000], Loss: 8096.9951\n",
      "Epoch [4590/5000], Loss: 8096.9951\n",
      "Epoch [4600/5000], Loss: 8096.9951\n",
      "Epoch [4610/5000], Loss: 8096.9951\n",
      "Epoch [4620/5000], Loss: 8096.9951\n",
      "Epoch [4630/5000], Loss: 8096.9951\n",
      "Epoch [4640/5000], Loss: 8096.9951\n",
      "Epoch [4650/5000], Loss: 8096.9951\n",
      "Epoch [4660/5000], Loss: 8096.9951\n",
      "Epoch [4670/5000], Loss: 8096.9951\n",
      "Epoch [4680/5000], Loss: 8096.9951\n",
      "Epoch [4690/5000], Loss: 8096.9951\n",
      "Epoch [4700/5000], Loss: 8096.9951\n",
      "Epoch [4710/5000], Loss: 8096.9951\n",
      "Epoch [4720/5000], Loss: 8096.9951\n",
      "Epoch [4730/5000], Loss: 8096.9951\n",
      "Epoch [4740/5000], Loss: 8096.9951\n",
      "Epoch [4750/5000], Loss: 8096.9951\n",
      "Epoch [4760/5000], Loss: 8096.9951\n",
      "Epoch [4770/5000], Loss: 8096.9951\n",
      "Epoch [4780/5000], Loss: 8096.9951\n",
      "Epoch [4790/5000], Loss: 8096.9951\n",
      "Epoch [4800/5000], Loss: 8096.9951\n",
      "Epoch [4810/5000], Loss: 8096.9951\n",
      "Epoch [4820/5000], Loss: 8096.9951\n",
      "Epoch [4830/5000], Loss: 8096.9951\n",
      "Epoch [4840/5000], Loss: 8096.9951\n",
      "Epoch [4850/5000], Loss: 8096.9951\n",
      "Epoch [4860/5000], Loss: 8096.9951\n",
      "Epoch [4870/5000], Loss: 8096.9951\n",
      "Epoch [4880/5000], Loss: 8096.9951\n",
      "Epoch [4890/5000], Loss: 8096.9951\n",
      "Epoch [4900/5000], Loss: 8096.9951\n",
      "Epoch [4910/5000], Loss: 8096.9951\n",
      "Epoch [4920/5000], Loss: 8096.9951\n",
      "Epoch [4930/5000], Loss: 8096.9951\n",
      "Epoch [4940/5000], Loss: 8096.9951\n",
      "Epoch [4950/5000], Loss: 8096.9951\n",
      "Epoch [4960/5000], Loss: 8096.9951\n",
      "Epoch [4970/5000], Loss: 8096.9951\n",
      "Epoch [4980/5000], Loss: 8096.9951\n",
      "Epoch [4990/5000], Loss: 8096.9951\n",
      "Epoch [5000/5000], Loss: 8096.9951\n",
      "Mean Absolute Error (MAE): 77.99280515809096\n",
      "Mean Squared Error (MSE): 9817.986309126094\n",
      "Root Mean Squared Error (RMSE): 99.08575230135811\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "from talib import RSI, MACD, STOCH, OBV, BBANDS\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Fetch historical data from Yahoo Finance\n",
    "ticker = 'TSLA'  # Tesla stock ticker\n",
    "start_date = '2010-01-01'  # Start date of historical data\n",
    "end_date = '2023-07-05'  # End date of historical data\n",
    "\n",
    "data = yf.download(ticker, start=start_date, end=end_date)\n",
    "\n",
    "# Calculate moving averages\n",
    "data['MA_5'] = data['Close'].rolling(window=5).mean()\n",
    "data['MA_10'] = data['Close'].rolling(window=10).mean()\n",
    "data['MA_20'] = data['Close'].rolling(window=20).mean()\n",
    "\n",
    "# Calculate RSI\n",
    "rsi_period = 14  # RSI calculation period\n",
    "data['RSI'] = RSI(data['Close'], timeperiod=rsi_period)\n",
    "\n",
    "# Calculate MACD\n",
    "macd, macd_signal, _ = MACD(data['Close'])\n",
    "data['MACD'] = macd\n",
    "\n",
    "# Calculate Stochastic Oscillator\n",
    "slowk, slowd = STOCH(data['High'], data['Low'], data['Close'])\n",
    "data['SlowK'] = slowk\n",
    "data['SlowD'] = slowd\n",
    "\n",
    "# Calculate On-Balance Volume (OBV)\n",
    "obv = OBV(data['Close'], data['Volume'])\n",
    "data['OBV'] = obv\n",
    "\n",
    "# Add additional indicators\n",
    "upper_band, middle_band, lower_band = BBANDS(data['Close'], timeperiod=20)\n",
    "data['UpperBand'] = upper_band\n",
    "data['MiddleBand'] = middle_band\n",
    "data['LowerBand'] = lower_band\n",
    "\n",
    "# Prepare the data\n",
    "X = data[['Open', 'Volume', 'MA_5', 'MA_10', 'MA_20', 'RSI', 'MACD', 'SlowK', 'SlowD', 'OBV', 'UpperBand', 'MiddleBand', 'LowerBand']].iloc[:-1]  # Adjusted for predicting next day's price\n",
    "y = data['Close'].shift(-1)[:-1]  # Adjusted for predicting next day's price\n",
    "\n",
    "# Store column names\n",
    "column_names = X.columns\n",
    "\n",
    "# Handle missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X = imputer.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train = torch.from_numpy(X_train).float()\n",
    "y_train = torch.from_numpy(y_train.values).float()\n",
    "X_test = torch.from_numpy(X_test).float()\n",
    "\n",
    "# Define the Neural Network class\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 16)\n",
    "        self.fc4 = nn.Linear(16, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        out = self.fc4(x)\n",
    "        return out\n",
    "\n",
    "# Instantiate the neural network\n",
    "input_size = X_train.shape[1]\n",
    "neural_network = NeuralNetwork(input_size)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(neural_network.parameters(), lr=0.01)\n",
    "\n",
    "# Train the neural network\n",
    "num_epochs = 5000\n",
    "batch_size = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(0, X_train.size(0), batch_size):\n",
    "        inputs = X_train[i:i+batch_size]\n",
    "        labels = y_train[i:i+batch_size]\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = neural_network(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Make predictions using the trained neural network\n",
    "neural_network.eval()  # Switch to evaluation mode\n",
    "predictions = neural_network(X_test)\n",
    "\n",
    "# Convert predictions tensor to numpy array\n",
    "predictions = predictions.detach().numpy().flatten()\n",
    "\n",
    "# Evaluate the model\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "\n",
    "# Save the model\n",
    "torch.save(neural_network.state_dict(), 'neural_network_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sami/anaconda3/envs/tf/lib/python3.10/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/sami/anaconda3/envs/tf/lib/python3.10/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([27])) that is different to the input size (torch.Size([27, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/5000], Loss: 62155996.0000\n",
      "Epoch [20/5000], Loss: 20155078.0000\n",
      "Epoch [30/5000], Loss: 3936975.2500\n",
      "Epoch [40/5000], Loss: 13157598208.0000\n",
      "Epoch [50/5000], Loss: 146862576.0000\n",
      "Epoch [60/5000], Loss: 1851617.6250\n",
      "Epoch [70/5000], Loss: 523298.2812\n",
      "Epoch [80/5000], Loss: 857410.4375\n",
      "Epoch [90/5000], Loss: 641262.8750\n",
      "Epoch [100/5000], Loss: 1076417.3750\n",
      "Epoch [110/5000], Loss: 3960211.5000\n",
      "Epoch [120/5000], Loss: 28678740.0000\n",
      "Epoch [130/5000], Loss: 1983607.0000\n",
      "Epoch [140/5000], Loss: 292906.0000\n",
      "Epoch [150/5000], Loss: 15892.1738\n",
      "Epoch [160/5000], Loss: 10617.2842\n",
      "Epoch [170/5000], Loss: 17219.7676\n",
      "Epoch [180/5000], Loss: 12839.0498\n",
      "Epoch [190/5000], Loss: 12819.5020\n",
      "Epoch [200/5000], Loss: 12790.3291\n",
      "Epoch [210/5000], Loss: 12747.0742\n",
      "Epoch [220/5000], Loss: 12683.6543\n",
      "Epoch [230/5000], Loss: 12592.3828\n",
      "Epoch [240/5000], Loss: 12464.4639\n",
      "Epoch [250/5000], Loss: 12290.7324\n",
      "Epoch [260/5000], Loss: 12062.3018\n",
      "Epoch [270/5000], Loss: 11772.4844\n",
      "Epoch [280/5000], Loss: 11421.8672\n",
      "Epoch [290/5000], Loss: 11024.8994\n",
      "Epoch [300/5000], Loss: 10358.2227\n",
      "Epoch [310/5000], Loss: 9548.4414\n",
      "Epoch [320/5000], Loss: 9520.3838\n",
      "Epoch [330/5000], Loss: 9530.3926\n",
      "Epoch [340/5000], Loss: 9530.5957\n",
      "Epoch [350/5000], Loss: 9530.3740\n",
      "Epoch [360/5000], Loss: 9530.0889\n",
      "Epoch [370/5000], Loss: 9529.7754\n",
      "Epoch [380/5000], Loss: 9529.4854\n",
      "Epoch [390/5000], Loss: 9529.2812\n",
      "Epoch [400/5000], Loss: 9529.1621\n",
      "Epoch [410/5000], Loss: 9529.1055\n",
      "Epoch [420/5000], Loss: 9529.0898\n",
      "Epoch [430/5000], Loss: 9529.0918\n",
      "Epoch [440/5000], Loss: 9529.0938\n",
      "Epoch [450/5000], Loss: 9529.0938\n",
      "Epoch [460/5000], Loss: 9529.0947\n",
      "Epoch [470/5000], Loss: 9529.0957\n",
      "Epoch [480/5000], Loss: 9529.0957\n",
      "Epoch [490/5000], Loss: 9529.0977\n",
      "Epoch [500/5000], Loss: 9529.0811\n",
      "Epoch [510/5000], Loss: 9529.0898\n",
      "Epoch [520/5000], Loss: 9665.8408\n",
      "Epoch [530/5000], Loss: 9626.6924\n",
      "Epoch [540/5000], Loss: 9593.8105\n",
      "Epoch [550/5000], Loss: 9564.3555\n",
      "Epoch [560/5000], Loss: 9537.7822\n",
      "Epoch [570/5000], Loss: 9516.4668\n",
      "Epoch [580/5000], Loss: 9504.9395\n",
      "Epoch [590/5000], Loss: 9506.1230\n",
      "Epoch [600/5000], Loss: 9515.7930\n",
      "Epoch [610/5000], Loss: 9525.1670\n",
      "Epoch [620/5000], Loss: 9529.8271\n",
      "Epoch [630/5000], Loss: 9528.8994\n",
      "Epoch [640/5000], Loss: 9530.7754\n",
      "Epoch [650/5000], Loss: 9530.9102\n",
      "Epoch [660/5000], Loss: 9530.4229\n",
      "Epoch [670/5000], Loss: 9988.9277\n",
      "Epoch [680/5000], Loss: 9528.4180\n",
      "Epoch [690/5000], Loss: 9504.9854\n",
      "Epoch [700/5000], Loss: 9507.5195\n",
      "Epoch [710/5000], Loss: 9709.8516\n",
      "Epoch [720/5000], Loss: 9580.3320\n",
      "Epoch [730/5000], Loss: 9545.2129\n",
      "Epoch [740/5000], Loss: 9521.4941\n",
      "Epoch [750/5000], Loss: 9508.7744\n",
      "Epoch [760/5000], Loss: 9504.0654\n",
      "Epoch [770/5000], Loss: 9505.7891\n",
      "Epoch [780/5000], Loss: 9512.3008\n",
      "Epoch [790/5000], Loss: 9520.2568\n",
      "Epoch [800/5000], Loss: 9527.3828\n",
      "Epoch [810/5000], Loss: 9529.9229\n",
      "Epoch [820/5000], Loss: 9530.8096\n",
      "Epoch [830/5000], Loss: 9531.0537\n",
      "Epoch [840/5000], Loss: 9531.0166\n",
      "Epoch [850/5000], Loss: 9530.9727\n",
      "Epoch [860/5000], Loss: 9530.9062\n",
      "Epoch [870/5000], Loss: 9530.8027\n",
      "Epoch [880/5000], Loss: 9530.6436\n",
      "Epoch [890/5000], Loss: 9760.5479\n",
      "Epoch [900/5000], Loss: 9579.2256\n",
      "Epoch [910/5000], Loss: 9581.6240\n",
      "Epoch [920/5000], Loss: 9571.7314\n",
      "Epoch [930/5000], Loss: 10669.0059\n",
      "Epoch [940/5000], Loss: 10627.9297\n",
      "Epoch [950/5000], Loss: 10570.3701\n",
      "Epoch [960/5000], Loss: 10494.5508\n",
      "Epoch [970/5000], Loss: 10402.5703\n",
      "Epoch [980/5000], Loss: 10368.7412\n",
      "Epoch [990/5000], Loss: 10368.6396\n",
      "Epoch [1000/5000], Loss: 10368.4277\n",
      "Epoch [1010/5000], Loss: 10368.1289\n",
      "Epoch [1020/5000], Loss: 10367.6816\n",
      "Epoch [1030/5000], Loss: 10396.1504\n",
      "Epoch [1040/5000], Loss: 10395.1982\n",
      "Epoch [1050/5000], Loss: 10393.7598\n",
      "Epoch [1060/5000], Loss: 10391.5957\n",
      "Epoch [1070/5000], Loss: 10388.3486\n",
      "Epoch [1080/5000], Loss: 10383.4688\n",
      "Epoch [1090/5000], Loss: 10376.1709\n",
      "Epoch [1100/5000], Loss: 10368.1768\n",
      "Epoch [1110/5000], Loss: 10358.7510\n",
      "Epoch [1120/5000], Loss: 10438.3506\n",
      "Epoch [1130/5000], Loss: 9668.3779\n",
      "Epoch [1140/5000], Loss: 9631.2051\n",
      "Epoch [1150/5000], Loss: 9645.5781\n",
      "Epoch [1160/5000], Loss: 9647.9160\n",
      "Epoch [1170/5000], Loss: 9648.1875\n",
      "Epoch [1180/5000], Loss: 9648.2314\n",
      "Epoch [1190/5000], Loss: 9648.2246\n",
      "Epoch [1200/5000], Loss: 9648.2051\n",
      "Epoch [1210/5000], Loss: 9648.1777\n",
      "Epoch [1220/5000], Loss: 9648.1357\n",
      "Epoch [1230/5000], Loss: 9648.1553\n",
      "Epoch [1240/5000], Loss: 9647.9795\n",
      "Epoch [1250/5000], Loss: 11242.8350\n",
      "Epoch [1260/5000], Loss: 11143.6602\n",
      "Epoch [1270/5000], Loss: 11143.5508\n",
      "Epoch [1280/5000], Loss: 11143.4238\n",
      "Epoch [1290/5000], Loss: 11143.2715\n",
      "Epoch [1300/5000], Loss: 11143.0146\n",
      "Epoch [1310/5000], Loss: 11142.6270\n",
      "Epoch [1320/5000], Loss: 11142.0469\n",
      "Epoch [1330/5000], Loss: 11141.1758\n",
      "Epoch [1340/5000], Loss: 11139.8604\n",
      "Epoch [1350/5000], Loss: 11137.8838\n",
      "Epoch [1360/5000], Loss: 11134.9072\n",
      "Epoch [1370/5000], Loss: 11130.4316\n",
      "Epoch [1380/5000], Loss: 11123.7031\n",
      "Epoch [1390/5000], Loss: 11114.9062\n",
      "Epoch [1400/5000], Loss: 11114.9062\n",
      "Epoch [1410/5000], Loss: 11114.9062\n",
      "Epoch [1420/5000], Loss: 11114.9062\n",
      "Epoch [1430/5000], Loss: 11114.9062\n",
      "Epoch [1440/5000], Loss: 11114.9062\n",
      "Epoch [1450/5000], Loss: 11132.0449\n",
      "Epoch [1460/5000], Loss: 11160.0938\n",
      "Epoch [1470/5000], Loss: 11160.0938\n",
      "Epoch [1480/5000], Loss: 11160.0938\n",
      "Epoch [1490/5000], Loss: 11160.0938\n",
      "Epoch [1500/5000], Loss: 11160.0938\n",
      "Epoch [1510/5000], Loss: 11160.0938\n",
      "Epoch [1520/5000], Loss: 11160.0938\n",
      "Epoch [1530/5000], Loss: 11160.0938\n",
      "Epoch [1540/5000], Loss: 11160.0938\n",
      "Epoch [1550/5000], Loss: 11160.6504\n",
      "Epoch [1560/5000], Loss: 11160.5312\n",
      "Epoch [1570/5000], Loss: 11186.3838\n",
      "Epoch [1580/5000], Loss: 11188.7012\n",
      "Epoch [1590/5000], Loss: 11194.9736\n",
      "Epoch [1600/5000], Loss: 11194.9736\n",
      "Epoch [1610/5000], Loss: 11194.9736\n",
      "Epoch [1620/5000], Loss: 11194.9590\n",
      "Epoch [1630/5000], Loss: 11194.8809\n",
      "Epoch [1640/5000], Loss: 11194.7520\n",
      "Epoch [1650/5000], Loss: 11194.6201\n",
      "Epoch [1660/5000], Loss: 11194.4688\n",
      "Epoch [1670/5000], Loss: 11194.1230\n",
      "Epoch [1680/5000], Loss: 11193.5967\n",
      "Epoch [1690/5000], Loss: 11192.8037\n",
      "Epoch [1700/5000], Loss: 11191.6094\n",
      "Epoch [1710/5000], Loss: 11189.8115\n",
      "Epoch [1720/5000], Loss: 11187.1055\n",
      "Epoch [1730/5000], Loss: 11196.3770\n",
      "Epoch [1740/5000], Loss: 11191.0127\n",
      "Epoch [1750/5000], Loss: 11182.9570\n",
      "Epoch [1760/5000], Loss: 11170.8867\n",
      "Epoch [1770/5000], Loss: 11152.8584\n",
      "Epoch [1780/5000], Loss: 11126.0459\n",
      "Epoch [1790/5000], Loss: 11140.6904\n",
      "Epoch [1800/5000], Loss: 11140.6904\n",
      "Epoch [1810/5000], Loss: 11140.6904\n",
      "Epoch [1820/5000], Loss: 11140.6904\n",
      "Epoch [1830/5000], Loss: 11140.5918\n",
      "Epoch [1840/5000], Loss: 11140.4658\n",
      "Epoch [1850/5000], Loss: 11140.3242\n",
      "Epoch [1860/5000], Loss: 11140.0732\n",
      "Epoch [1870/5000], Loss: 11139.7080\n",
      "Epoch [1880/5000], Loss: 11139.1514\n",
      "Epoch [1890/5000], Loss: 11138.3164\n",
      "Epoch [1900/5000], Loss: 11137.0625\n",
      "Epoch [1910/5000], Loss: 11135.1699\n",
      "Epoch [1920/5000], Loss: 11132.3232\n",
      "Epoch [1930/5000], Loss: 11128.0400\n",
      "Epoch [1940/5000], Loss: 11121.6035\n",
      "Epoch [1950/5000], Loss: 11111.9531\n",
      "Epoch [1960/5000], Loss: 11097.5176\n",
      "Epoch [1970/5000], Loss: 11075.9893\n",
      "Epoch [1980/5000], Loss: 11044.0908\n",
      "Epoch [1990/5000], Loss: 10997.2432\n",
      "Epoch [2000/5000], Loss: 10985.9668\n",
      "Epoch [2010/5000], Loss: 10985.9668\n",
      "Epoch [2020/5000], Loss: 10985.9668\n",
      "Epoch [2030/5000], Loss: 10985.9668\n",
      "Epoch [2040/5000], Loss: 10985.9668\n",
      "Epoch [2050/5000], Loss: 10985.9668\n",
      "Epoch [2060/5000], Loss: 10985.9668\n",
      "Epoch [2070/5000], Loss: 10985.9668\n",
      "Epoch [2080/5000], Loss: 10985.9668\n",
      "Epoch [2090/5000], Loss: 10985.8838\n",
      "Epoch [2100/5000], Loss: 10985.7627\n",
      "Epoch [2110/5000], Loss: 10985.6328\n",
      "Epoch [2120/5000], Loss: 10985.4014\n",
      "Epoch [2130/5000], Loss: 10985.0625\n",
      "Epoch [2140/5000], Loss: 10984.5518\n",
      "Epoch [2150/5000], Loss: 10983.7822\n",
      "Epoch [2160/5000], Loss: 10982.6250\n",
      "Epoch [2170/5000], Loss: 10980.8828\n",
      "Epoch [2180/5000], Loss: 11029.9092\n",
      "Epoch [2190/5000], Loss: 11029.8662\n",
      "Epoch [2200/5000], Loss: 11029.8369\n",
      "Epoch [2210/5000], Loss: 11029.7910\n",
      "Epoch [2220/5000], Loss: 11029.7695\n",
      "Epoch [2230/5000], Loss: 11029.7041\n",
      "Epoch [2240/5000], Loss: 10982.6973\n",
      "Epoch [2250/5000], Loss: 10975.9502\n",
      "Epoch [2260/5000], Loss: 10975.7549\n",
      "Epoch [2270/5000], Loss: 10975.4639\n",
      "Epoch [2280/5000], Loss: 10975.0117\n",
      "Epoch [2290/5000], Loss: 10974.3291\n",
      "Epoch [2300/5000], Loss: 10973.3027\n",
      "Epoch [2310/5000], Loss: 10971.7549\n",
      "Epoch [2320/5000], Loss: 10969.4277\n",
      "Epoch [2330/5000], Loss: 10965.9248\n",
      "Epoch [2340/5000], Loss: 10960.6553\n",
      "Epoch [2350/5000], Loss: 10952.7510\n",
      "Epoch [2360/5000], Loss: 10940.9180\n",
      "Epoch [2370/5000], Loss: 10923.2617\n",
      "Epoch [2380/5000], Loss: 10897.0488\n",
      "Epoch [2390/5000], Loss: 10888.2393\n",
      "Epoch [2400/5000], Loss: 10888.2617\n",
      "Epoch [2410/5000], Loss: 10888.2607\n",
      "Epoch [2420/5000], Loss: 10887.5908\n",
      "Epoch [2430/5000], Loss: 10887.6719\n",
      "Epoch [2440/5000], Loss: 10887.8066\n",
      "Epoch [2450/5000], Loss: 10887.8242\n",
      "Epoch [2460/5000], Loss: 10887.6211\n",
      "Epoch [2470/5000], Loss: 10887.2314\n",
      "Epoch [2480/5000], Loss: 10883.7617\n",
      "Epoch [2490/5000], Loss: 10882.1074\n",
      "Epoch [2500/5000], Loss: 10880.8057\n",
      "Epoch [2510/5000], Loss: 10878.8184\n",
      "Epoch [2520/5000], Loss: 10875.8369\n",
      "Epoch [2530/5000], Loss: 10871.3145\n",
      "Epoch [2540/5000], Loss: 10865.3252\n",
      "Epoch [2550/5000], Loss: 10854.6846\n",
      "Epoch [2560/5000], Loss: 10823.5332\n",
      "Epoch [2570/5000], Loss: 10814.7891\n",
      "Epoch [2580/5000], Loss: 10812.1621\n",
      "Epoch [2590/5000], Loss: 10808.7217\n",
      "Epoch [2600/5000], Loss: 10802.9609\n",
      "Epoch [2610/5000], Loss: 10794.2588\n",
      "Epoch [2620/5000], Loss: 10781.7871\n",
      "Epoch [2630/5000], Loss: 10781.7119\n",
      "Epoch [2640/5000], Loss: 10781.4658\n",
      "Epoch [2650/5000], Loss: 10781.3740\n",
      "Epoch [2660/5000], Loss: 10834.6719\n",
      "Epoch [2670/5000], Loss: 10834.4404\n",
      "Epoch [2680/5000], Loss: 10834.0908\n",
      "Epoch [2690/5000], Loss: 10833.5684\n",
      "Epoch [2700/5000], Loss: 10832.7822\n",
      "Epoch [2710/5000], Loss: 10831.5977\n",
      "Epoch [2720/5000], Loss: 10829.8164\n",
      "Epoch [2730/5000], Loss: 10827.1357\n",
      "Epoch [2740/5000], Loss: 10802.4541\n",
      "Epoch [2750/5000], Loss: 10802.4512\n",
      "Epoch [2760/5000], Loss: 10802.4482\n",
      "Epoch [2770/5000], Loss: 10825.4277\n",
      "Epoch [2780/5000], Loss: 10828.5498\n",
      "Epoch [2790/5000], Loss: 10827.7305\n",
      "Epoch [2800/5000], Loss: 10827.7119\n",
      "Epoch [2810/5000], Loss: 10827.6836\n",
      "Epoch [2820/5000], Loss: 10828.2461\n",
      "Epoch [2830/5000], Loss: 10828.1182\n",
      "Epoch [2840/5000], Loss: 10827.9092\n",
      "Epoch [2850/5000], Loss: 10827.6514\n",
      "Epoch [2860/5000], Loss: 10827.2314\n",
      "Epoch [2870/5000], Loss: 10826.6270\n",
      "Epoch [2880/5000], Loss: 10831.6260\n",
      "Epoch [2890/5000], Loss: 10830.8477\n",
      "Epoch [2900/5000], Loss: 10832.5244\n",
      "Epoch [2910/5000], Loss: 10830.7910\n",
      "Epoch [2920/5000], Loss: 10828.1924\n",
      "Epoch [2930/5000], Loss: 10824.0430\n",
      "Epoch [2940/5000], Loss: 10818.3652\n",
      "Epoch [2950/5000], Loss: 10809.8877\n",
      "Epoch [2960/5000], Loss: 10797.2871\n",
      "Epoch [2970/5000], Loss: 10782.1338\n",
      "Epoch [2980/5000], Loss: 10755.1660\n",
      "Epoch [2990/5000], Loss: 10715.9043\n",
      "Epoch [3000/5000], Loss: 10659.3623\n",
      "Epoch [3010/5000], Loss: 10579.8838\n",
      "Epoch [3020/5000], Loss: 10472.2168\n",
      "Epoch [3030/5000], Loss: 10334.2148\n",
      "Epoch [3040/5000], Loss: 10170.8311\n",
      "Epoch [3050/5000], Loss: 9996.0059\n",
      "Epoch [3060/5000], Loss: 9830.3379\n",
      "Epoch [3070/5000], Loss: 9692.9111\n",
      "Epoch [3080/5000], Loss: 9594.6992\n",
      "Epoch [3090/5000], Loss: 9536.6484\n",
      "Epoch [3100/5000], Loss: 9510.8330\n",
      "Epoch [3110/5000], Loss: 9505.1172\n",
      "Epoch [3120/5000], Loss: 9504.9434\n",
      "Epoch [3130/5000], Loss: 9504.9746\n",
      "Epoch [3140/5000], Loss: 9504.9980\n",
      "Epoch [3150/5000], Loss: 9504.3135\n",
      "Epoch [3160/5000], Loss: 9504.4961\n",
      "Epoch [3170/5000], Loss: 9504.6484\n",
      "Epoch [3180/5000], Loss: 9504.7422\n",
      "Epoch [3190/5000], Loss: 9504.8936\n",
      "Epoch [3200/5000], Loss: 9504.8779\n",
      "Epoch [3210/5000], Loss: 9504.3398\n",
      "Epoch [3220/5000], Loss: 9504.1602\n",
      "Epoch [3230/5000], Loss: 9504.1602\n",
      "Epoch [3240/5000], Loss: 9504.1611\n",
      "Epoch [3250/5000], Loss: 9504.1602\n",
      "Epoch [3260/5000], Loss: 9504.1602\n",
      "Epoch [3270/5000], Loss: 9504.1602\n",
      "Epoch [3280/5000], Loss: 9504.1875\n",
      "Epoch [3290/5000], Loss: 9504.1904\n",
      "Epoch [3300/5000], Loss: 9504.1943\n",
      "Epoch [3310/5000], Loss: 9504.1992\n",
      "Epoch [3320/5000], Loss: 9504.2070\n",
      "Epoch [3330/5000], Loss: 9504.2197\n",
      "Epoch [3340/5000], Loss: 9504.2773\n",
      "Epoch [3350/5000], Loss: 9503.9990\n",
      "Epoch [3360/5000], Loss: 9503.9980\n",
      "Epoch [3370/5000], Loss: 9504.0000\n",
      "Epoch [3380/5000], Loss: 9504.0059\n",
      "Epoch [3390/5000], Loss: 9504.0215\n",
      "Epoch [3400/5000], Loss: 9504.0664\n",
      "Epoch [3410/5000], Loss: 9504.1719\n",
      "Epoch [3420/5000], Loss: 9504.4092\n",
      "Epoch [3430/5000], Loss: 9505.5908\n",
      "Epoch [3440/5000], Loss: 9505.6016\n",
      "Epoch [3450/5000], Loss: 9505.1553\n",
      "Epoch [3460/5000], Loss: 9506.6436\n",
      "Epoch [3470/5000], Loss: 9506.6875\n",
      "Epoch [3480/5000], Loss: 9506.4824\n",
      "Epoch [3490/5000], Loss: 9506.5107\n",
      "Epoch [3500/5000], Loss: 9506.5449\n",
      "Epoch [3510/5000], Loss: 9506.5840\n",
      "Epoch [3520/5000], Loss: 9506.6230\n",
      "Epoch [3530/5000], Loss: 9506.6533\n",
      "Epoch [3540/5000], Loss: 9506.6719\n",
      "Epoch [3550/5000], Loss: 9506.0820\n",
      "Epoch [3560/5000], Loss: 9506.0898\n",
      "Epoch [3570/5000], Loss: 9506.1055\n",
      "Epoch [3580/5000], Loss: 9506.1084\n",
      "Epoch [3590/5000], Loss: 9506.1318\n",
      "Epoch [3600/5000], Loss: 9507.1094\n",
      "Epoch [3610/5000], Loss: 9507.1094\n",
      "Epoch [3620/5000], Loss: 9507.1230\n",
      "Epoch [3630/5000], Loss: 9507.1533\n",
      "Epoch [3640/5000], Loss: 9507.1162\n",
      "Epoch [3650/5000], Loss: 9507.1914\n",
      "Epoch [3660/5000], Loss: 9507.3027\n",
      "Epoch [3670/5000], Loss: 9507.4717\n",
      "Epoch [3680/5000], Loss: 9507.7275\n",
      "Epoch [3690/5000], Loss: 9509.4854\n",
      "Epoch [3700/5000], Loss: 9509.5498\n",
      "Epoch [3710/5000], Loss: 9510.2148\n",
      "Epoch [3720/5000], Loss: 9510.3311\n",
      "Epoch [3730/5000], Loss: 9510.4922\n",
      "Epoch [3740/5000], Loss: 9510.2168\n",
      "Epoch [3750/5000], Loss: 9510.3389\n",
      "Epoch [3760/5000], Loss: 9510.5889\n",
      "Epoch [3770/5000], Loss: 9510.8877\n",
      "Epoch [3780/5000], Loss: 9511.1094\n",
      "Epoch [3790/5000], Loss: 9511.2432\n",
      "Epoch [3800/5000], Loss: 9511.3184\n",
      "Epoch [3810/5000], Loss: 9511.3633\n",
      "Epoch [3820/5000], Loss: 9511.3906\n",
      "Epoch [3830/5000], Loss: 9511.4111\n",
      "Epoch [3840/5000], Loss: 9511.2363\n",
      "Epoch [3850/5000], Loss: 9511.2715\n",
      "Epoch [3860/5000], Loss: 9511.3057\n",
      "Epoch [3870/5000], Loss: 9511.3418\n",
      "Epoch [3880/5000], Loss: 9511.3857\n",
      "Epoch [3890/5000], Loss: 9509.1836\n",
      "Epoch [3900/5000], Loss: 9525.3984\n",
      "Epoch [3910/5000], Loss: 9529.1143\n",
      "Epoch [3920/5000], Loss: 9530.1523\n",
      "Epoch [3930/5000], Loss: 9530.5039\n",
      "Epoch [3940/5000], Loss: 9530.6494\n",
      "Epoch [3950/5000], Loss: 9530.5889\n",
      "Epoch [3960/5000], Loss: 9530.7373\n",
      "Epoch [3970/5000], Loss: 9530.7617\n",
      "Epoch [3980/5000], Loss: 9530.8467\n",
      "Epoch [3990/5000], Loss: 9530.4072\n",
      "Epoch [4000/5000], Loss: 9528.9531\n",
      "Epoch [4010/5000], Loss: 9529.9600\n",
      "Epoch [4020/5000], Loss: 9530.3604\n",
      "Epoch [4030/5000], Loss: 9530.5215\n",
      "Epoch [4040/5000], Loss: 9531.7002\n",
      "Epoch [4050/5000], Loss: 9530.9062\n",
      "Epoch [4060/5000], Loss: 9530.7598\n",
      "Epoch [4070/5000], Loss: 9531.2061\n",
      "Epoch [4080/5000], Loss: 9530.4512\n",
      "Epoch [4090/5000], Loss: 9530.4297\n",
      "Epoch [4100/5000], Loss: 9530.6211\n",
      "Epoch [4110/5000], Loss: 9510.2588\n",
      "Epoch [4120/5000], Loss: 9511.8955\n",
      "Epoch [4130/5000], Loss: 9504.9297\n",
      "Epoch [4140/5000], Loss: 9504.4521\n",
      "Epoch [4150/5000], Loss: 9506.1182\n",
      "Epoch [4160/5000], Loss: 9506.1182\n",
      "Epoch [4170/5000], Loss: 9506.4209\n",
      "Epoch [4180/5000], Loss: 9529.9219\n",
      "Epoch [4190/5000], Loss: 9530.5703\n",
      "Epoch [4200/5000], Loss: 9530.6426\n",
      "Epoch [4210/5000], Loss: 9530.6553\n",
      "Epoch [4220/5000], Loss: 9503.9912\n",
      "Epoch [4230/5000], Loss: 9505.0059\n",
      "Epoch [4240/5000], Loss: 9505.4248\n",
      "Epoch [4250/5000], Loss: 9505.4248\n",
      "Epoch [4260/5000], Loss: 9505.4248\n",
      "Epoch [4270/5000], Loss: 9505.4258\n",
      "Epoch [4280/5000], Loss: 9505.4268\n",
      "Epoch [4290/5000], Loss: 9505.4346\n",
      "Epoch [4300/5000], Loss: 9505.4346\n",
      "Epoch [4310/5000], Loss: 9505.4453\n",
      "Epoch [4320/5000], Loss: 9505.4502\n",
      "Epoch [4330/5000], Loss: 9505.4551\n",
      "Epoch [4340/5000], Loss: 9505.4639\n",
      "Epoch [4350/5000], Loss: 9505.4746\n",
      "Epoch [4360/5000], Loss: 9505.4893\n",
      "Epoch [4370/5000], Loss: 9506.0137\n",
      "Epoch [4380/5000], Loss: 9506.0166\n",
      "Epoch [4390/5000], Loss: 9505.9365\n",
      "Epoch [4400/5000], Loss: 9506.7725\n",
      "Epoch [4410/5000], Loss: 9506.7734\n",
      "Epoch [4420/5000], Loss: 9506.7734\n",
      "Epoch [4430/5000], Loss: 9506.7734\n",
      "Epoch [4440/5000], Loss: 9507.9023\n",
      "Epoch [4450/5000], Loss: 9507.9023\n",
      "Epoch [4460/5000], Loss: 9507.9033\n",
      "Epoch [4470/5000], Loss: 9507.9043\n",
      "Epoch [4480/5000], Loss: 2434259456.0000\n",
      "Epoch [4490/5000], Loss: 9507.8887\n",
      "Epoch [4500/5000], Loss: 9507.8975\n",
      "Epoch [4510/5000], Loss: 9508.0264\n",
      "Epoch [4520/5000], Loss: 9509.5352\n",
      "Epoch [4530/5000], Loss: 9509.5381\n",
      "Epoch [4540/5000], Loss: 9509.5449\n",
      "Epoch [4550/5000], Loss: 9509.5537\n",
      "Epoch [4560/5000], Loss: 9509.5674\n",
      "Epoch [4570/5000], Loss: 9509.5889\n",
      "Epoch [4580/5000], Loss: 9509.6191\n",
      "Epoch [4590/5000], Loss: 9509.6660\n",
      "Epoch [4600/5000], Loss: 9509.5400\n",
      "Epoch [4610/5000], Loss: 9509.6348\n",
      "Epoch [4620/5000], Loss: 9509.7773\n",
      "Epoch [4630/5000], Loss: 9509.9941\n",
      "Epoch [4640/5000], Loss: 9510.3223\n",
      "Epoch [4650/5000], Loss: 9510.8164\n",
      "Epoch [4660/5000], Loss: 9511.4062\n",
      "Epoch [4670/5000], Loss: 9512.5010\n",
      "Epoch [4680/5000], Loss: 9514.0840\n",
      "Epoch [4690/5000], Loss: 9516.2637\n",
      "Epoch [4700/5000], Loss: 9519.0059\n",
      "Epoch [4710/5000], Loss: 9522.0107\n",
      "Epoch [4720/5000], Loss: 9524.7803\n",
      "Epoch [4730/5000], Loss: 9526.9365\n",
      "Epoch [4740/5000], Loss: 9529.5195\n",
      "Epoch [4750/5000], Loss: 9529.9932\n",
      "Epoch [4760/5000], Loss: 9530.3096\n",
      "Epoch [4770/5000], Loss: 9529.6543\n",
      "Epoch [4780/5000], Loss: 9530.1621\n",
      "Epoch [4790/5000], Loss: 9530.4697\n",
      "Epoch [4800/5000], Loss: 9530.6504\n",
      "Epoch [4810/5000], Loss: 9530.7598\n",
      "Epoch [4820/5000], Loss: 9530.8271\n",
      "Epoch [4830/5000], Loss: 9534.2891\n",
      "Epoch [4840/5000], Loss: 9534.2891\n",
      "Epoch [4850/5000], Loss: 9534.2891\n",
      "Epoch [4860/5000], Loss: 9534.2500\n",
      "Epoch [4870/5000], Loss: 9534.2363\n",
      "Epoch [4880/5000], Loss: 9533.4199\n",
      "Epoch [4890/5000], Loss: 9533.4170\n",
      "Epoch [4900/5000], Loss: 9532.1592\n",
      "Epoch [4910/5000], Loss: 9532.0840\n",
      "Epoch [4920/5000], Loss: 9532.0840\n",
      "Epoch [4930/5000], Loss: 9532.0840\n",
      "Epoch [4940/5000], Loss: 9532.0840\n",
      "Epoch [4950/5000], Loss: 9532.0840\n",
      "Epoch [4960/5000], Loss: 9532.0840\n",
      "Epoch [4970/5000], Loss: 9532.0840\n",
      "Epoch [4980/5000], Loss: 9532.0840\n",
      "Epoch [4990/5000], Loss: 9532.0820\n",
      "Epoch [5000/5000], Loss: 9535.5547\n",
      "Mean Absolute Error (MAE): 78.3010100535764\n",
      "Mean Squared Error (MSE): 9820.632146477976\n",
      "Root Mean Squared Error (RMSE): 99.09910265223382\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "from talib import RSI, MACD, STOCH, OBV, BBANDS\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Fetch historical data from Yahoo Finance\n",
    "ticker = 'TSLA'  # Tesla stock ticker\n",
    "start_date = '2010-01-01'  # Start date of historical data\n",
    "end_date = '2023-07-05'  # End date of historical data\n",
    "\n",
    "data = yf.download(ticker, start=start_date, end=end_date)\n",
    "\n",
    "# Calculate moving averages\n",
    "data['MA_5'] = data['Close'].rolling(window=5).mean()\n",
    "data['MA_10'] = data['Close'].rolling(window=10).mean()\n",
    "data['MA_20'] = data['Close'].rolling(window=20).mean()\n",
    "\n",
    "# Calculate RSI\n",
    "rsi_period = 14  # RSI calculation period\n",
    "data['RSI'] = RSI(data['Close'], timeperiod=rsi_period)\n",
    "\n",
    "# Calculate MACD\n",
    "macd, macd_signal, _ = MACD(data['Close'])\n",
    "data['MACD'] = macd\n",
    "\n",
    "# Calculate Stochastic Oscillator\n",
    "slowk, slowd = STOCH(data['High'], data['Low'], data['Close'])\n",
    "data['SlowK'] = slowk\n",
    "data['SlowD'] = slowd\n",
    "\n",
    "# Calculate On-Balance Volume (OBV)\n",
    "obv = OBV(data['Close'], data['Volume'])\n",
    "data['OBV'] = obv\n",
    "\n",
    "# Add additional indicators\n",
    "upper_band, middle_band, lower_band = BBANDS(data['Close'], timeperiod=20)\n",
    "data['UpperBand'] = upper_band\n",
    "data['MiddleBand'] = middle_band\n",
    "data['LowerBand'] = lower_band\n",
    "\n",
    "# Prepare the data\n",
    "X = data[['Open', 'Volume', 'MA_5', 'MA_10', 'MA_20', 'RSI', 'MACD', 'SlowK', 'SlowD', 'OBV', 'UpperBand', 'MiddleBand', 'LowerBand']].iloc[:-1]  # Adjusted for predicting next day's price\n",
    "y = data['Close'].shift(-1)[:-1]  # Adjusted for predicting next day's price\n",
    "\n",
    "# Store column names\n",
    "column_names = X.columns\n",
    "\n",
    "# Handle missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X = imputer.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train = torch.from_numpy(X_train).float()\n",
    "y_train = torch.from_numpy(y_train.values).float()\n",
    "X_test = torch.from_numpy(X_test).float()\n",
    "\n",
    "# Define the Neural Network class\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 16)\n",
    "        self.fc4 = nn.Linear(16, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        out = self.fc4(x)\n",
    "        return out\n",
    "\n",
    "# Instantiate the neural network\n",
    "input_size = X_train.shape[1]\n",
    "neural_network = NeuralNetwork(input_size)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Apply L2 regularization (weight decay)\n",
    "weight_decay = 0.001\n",
    "optimizer = optim.Adam(neural_network.parameters(), lr=0.01, weight_decay=weight_decay)\n",
    "\n",
    "# Train the neural network\n",
    "num_epochs = 5000\n",
    "batch_size = 32\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(0, X_train.size(0), batch_size):\n",
    "        inputs = X_train[i:i+batch_size]\n",
    "        labels = y_train[i:i+batch_size]\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = neural_network(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Make predictions using the trained neural network\n",
    "neural_network.eval()  # Switch to evaluation mode\n",
    "predictions = neural_network(X_test)\n",
    "\n",
    "# Convert predictions tensor to numpy array\n",
    "predictions = predictions.detach().numpy().flatten()\n",
    "\n",
    "# Evaluate the model\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "\n",
    "# Save the model\n",
    "torch.save(neural_network.state_dict(), 'neural_network_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sami/anaconda3/envs/tf/lib/python3.10/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/sami/anaconda3/envs/tf/lib/python3.10/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([27])) that is different to the input size (torch.Size([27, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/5000], Loss: 1408995712.0000\n",
      "Epoch [20/5000], Loss: 2124191872.0000\n",
      "Epoch [30/5000], Loss: 1133015680.0000\n",
      "Epoch [40/5000], Loss: 747984384.0000\n",
      "Epoch [50/5000], Loss: 17977496.0000\n",
      "Epoch [60/5000], Loss: 23529152.0000\n",
      "Epoch [70/5000], Loss: 108761600.0000\n",
      "Epoch [80/5000], Loss: 9664280.0000\n",
      "Epoch [90/5000], Loss: 6352159744.0000\n",
      "Epoch [100/5000], Loss: 140377.5156\n",
      "Epoch [110/5000], Loss: 26391.6484\n",
      "Epoch [120/5000], Loss: 26655.1816\n",
      "Epoch [130/5000], Loss: 116536.4922\n",
      "Epoch [140/5000], Loss: 1039736.3750\n",
      "Epoch [150/5000], Loss: 13004.6143\n",
      "Epoch [160/5000], Loss: 13002.1484\n",
      "Epoch [170/5000], Loss: 12999.3740\n",
      "Epoch [180/5000], Loss: 12995.1943\n",
      "Epoch [190/5000], Loss: 12988.9062\n",
      "Epoch [200/5000], Loss: 12979.4551\n",
      "Epoch [210/5000], Loss: 12965.2686\n",
      "Epoch [220/5000], Loss: 12946.4873\n",
      "Epoch [230/5000], Loss: 12921.9893\n",
      "Epoch [240/5000], Loss: 12885.8125\n",
      "Epoch [250/5000], Loss: 12833.3291\n",
      "Epoch [260/5000], Loss: 12759.6172\n",
      "Epoch [270/5000], Loss: 12661.4238\n",
      "Epoch [280/5000], Loss: 12539.6025\n",
      "Epoch [290/5000], Loss: 12399.2031\n",
      "Epoch [300/5000], Loss: 12246.1787\n",
      "Epoch [310/5000], Loss: 12084.2666\n",
      "Epoch [320/5000], Loss: 11914.1436\n",
      "Epoch [330/5000], Loss: 11734.1357\n",
      "Epoch [340/5000], Loss: 11540.7461\n",
      "Epoch [350/5000], Loss: 11329.5186\n",
      "Epoch [360/5000], Loss: 11096.1914\n",
      "Epoch [370/5000], Loss: 10839.0703\n",
      "Epoch [380/5000], Loss: 10562.7490\n",
      "Epoch [390/5000], Loss: 10281.7285\n",
      "Epoch [400/5000], Loss: 10019.8350\n",
      "Epoch [410/5000], Loss: 9802.5625\n",
      "Epoch [420/5000], Loss: 9646.5469\n",
      "Epoch [430/5000], Loss: 9553.7520\n",
      "Epoch [440/5000], Loss: 9512.5947\n",
      "Epoch [450/5000], Loss: 9503.9570\n",
      "Epoch [460/5000], Loss: 9509.4209\n",
      "Epoch [470/5000], Loss: 9517.3955\n",
      "Epoch [480/5000], Loss: 9523.4883\n",
      "Epoch [490/5000], Loss: 9527.2295\n",
      "Epoch [500/5000], Loss: 9529.2871\n",
      "Epoch [510/5000], Loss: 9530.3496\n",
      "Epoch [520/5000], Loss: 9530.8643\n",
      "Epoch [530/5000], Loss: 9531.0908\n",
      "Epoch [540/5000], Loss: 9531.1689\n",
      "Epoch [550/5000], Loss: 9531.1719\n",
      "Epoch [560/5000], Loss: 9531.1318\n",
      "Epoch [570/5000], Loss: 9531.0566\n",
      "Epoch [580/5000], Loss: 9530.9434\n",
      "Epoch [590/5000], Loss: 9530.7734\n",
      "Epoch [600/5000], Loss: 9530.5244\n",
      "Epoch [610/5000], Loss: 9530.1631\n",
      "Epoch [620/5000], Loss: 9529.6504\n",
      "Epoch [630/5000], Loss: 9528.9590\n",
      "Epoch [640/5000], Loss: 9528.0869\n",
      "Epoch [650/5000], Loss: 9527.0791\n",
      "Epoch [660/5000], Loss: 9526.0693\n",
      "Epoch [670/5000], Loss: 9525.2686\n",
      "Epoch [680/5000], Loss: 9524.7871\n",
      "Epoch [690/5000], Loss: 9524.5508\n",
      "Epoch [700/5000], Loss: 9524.4482\n",
      "Epoch [710/5000], Loss: 9524.4102\n",
      "Epoch [720/5000], Loss: 9524.3994\n",
      "Epoch [730/5000], Loss: 9524.4014\n",
      "Epoch [740/5000], Loss: 9524.4082\n",
      "Epoch [750/5000], Loss: 9524.4160\n",
      "Epoch [760/5000], Loss: 9524.4258\n",
      "Epoch [770/5000], Loss: 9524.4365\n",
      "Epoch [780/5000], Loss: 9524.4473\n",
      "Epoch [790/5000], Loss: 9524.4570\n",
      "Epoch [800/5000], Loss: 9524.4678\n",
      "Epoch [810/5000], Loss: 9524.4785\n",
      "Epoch [820/5000], Loss: 9524.4883\n",
      "Epoch [830/5000], Loss: 9524.4990\n",
      "Epoch [840/5000], Loss: 9524.5107\n",
      "Epoch [850/5000], Loss: 9524.5195\n",
      "Epoch [860/5000], Loss: 9524.5303\n",
      "Epoch [870/5000], Loss: 9524.5410\n",
      "Epoch [880/5000], Loss: 9524.5508\n",
      "Epoch [890/5000], Loss: 9524.5615\n",
      "Epoch [900/5000], Loss: 9524.5703\n",
      "Epoch [910/5000], Loss: 9524.5811\n",
      "Epoch [920/5000], Loss: 9524.5938\n",
      "Epoch [930/5000], Loss: 9524.6035\n",
      "Epoch [940/5000], Loss: 9524.6133\n",
      "Epoch [950/5000], Loss: 9524.6240\n",
      "Epoch [960/5000], Loss: 9524.6338\n",
      "Epoch [970/5000], Loss: 9524.6436\n",
      "Epoch [980/5000], Loss: 9524.6543\n",
      "Epoch [990/5000], Loss: 9524.6650\n",
      "Epoch [1000/5000], Loss: 9524.6748\n",
      "Epoch [1010/5000], Loss: 9524.6855\n",
      "Epoch [1020/5000], Loss: 9524.6953\n",
      "Epoch [1030/5000], Loss: 9524.7061\n",
      "Epoch [1040/5000], Loss: 9524.7168\n",
      "Epoch [1050/5000], Loss: 9524.7266\n",
      "Epoch [1060/5000], Loss: 9524.7363\n",
      "Epoch [1070/5000], Loss: 9524.7451\n",
      "Epoch [1080/5000], Loss: 9524.7568\n",
      "Epoch [1090/5000], Loss: 9524.7666\n",
      "Epoch [1100/5000], Loss: 9524.7764\n",
      "Epoch [1110/5000], Loss: 9524.7881\n",
      "Epoch [1120/5000], Loss: 9524.7979\n",
      "Epoch [1130/5000], Loss: 9524.8076\n",
      "Epoch [1140/5000], Loss: 9524.8184\n",
      "Epoch [1150/5000], Loss: 9524.8281\n",
      "Epoch [1160/5000], Loss: 9524.8398\n",
      "Epoch [1170/5000], Loss: 9524.8477\n",
      "Epoch [1180/5000], Loss: 9524.8584\n",
      "Epoch [1190/5000], Loss: 9524.8701\n",
      "Epoch [1200/5000], Loss: 9524.8789\n",
      "Epoch [1210/5000], Loss: 9524.8887\n",
      "Epoch [1220/5000], Loss: 9524.8994\n",
      "Epoch [1230/5000], Loss: 9524.9092\n",
      "Epoch [1240/5000], Loss: 9524.9189\n",
      "Epoch [1250/5000], Loss: 9524.9297\n",
      "Epoch [1260/5000], Loss: 9524.9395\n",
      "Epoch [1270/5000], Loss: 9524.9482\n",
      "Epoch [1280/5000], Loss: 9524.9590\n",
      "Epoch [1290/5000], Loss: 9524.9697\n",
      "Epoch [1300/5000], Loss: 9524.9785\n",
      "Epoch [1310/5000], Loss: 9524.9893\n",
      "Epoch [1320/5000], Loss: 9524.9990\n",
      "Epoch [1330/5000], Loss: 9525.0078\n",
      "Epoch [1340/5000], Loss: 9525.0176\n",
      "Epoch [1350/5000], Loss: 9525.0283\n",
      "Epoch [1360/5000], Loss: 9525.0381\n",
      "Epoch [1370/5000], Loss: 9525.0498\n",
      "Epoch [1380/5000], Loss: 9525.0586\n",
      "Epoch [1390/5000], Loss: 9525.0684\n",
      "Epoch [1400/5000], Loss: 9525.0791\n",
      "Epoch [1410/5000], Loss: 9525.0879\n",
      "Epoch [1420/5000], Loss: 9525.0977\n",
      "Epoch [1430/5000], Loss: 9525.1074\n",
      "Epoch [1440/5000], Loss: 9525.1182\n",
      "Epoch [1450/5000], Loss: 9525.1260\n",
      "Epoch [1460/5000], Loss: 9525.1367\n",
      "Epoch [1470/5000], Loss: 9525.1465\n",
      "Epoch [1480/5000], Loss: 9525.1553\n",
      "Epoch [1490/5000], Loss: 9525.1650\n",
      "Epoch [1500/5000], Loss: 9525.1758\n",
      "Epoch [1510/5000], Loss: 9525.1855\n",
      "Epoch [1520/5000], Loss: 9525.1943\n",
      "Epoch [1530/5000], Loss: 9525.2051\n",
      "Epoch [1540/5000], Loss: 9525.2148\n",
      "Epoch [1550/5000], Loss: 9525.2236\n",
      "Epoch [1560/5000], Loss: 9525.2334\n",
      "Epoch [1570/5000], Loss: 9525.2422\n",
      "Epoch [1580/5000], Loss: 9525.2520\n",
      "Epoch [1590/5000], Loss: 9525.2617\n",
      "Epoch [1600/5000], Loss: 9525.2715\n",
      "Epoch [1610/5000], Loss: 9525.2812\n",
      "Epoch [1620/5000], Loss: 9525.2900\n",
      "Epoch [1630/5000], Loss: 9525.2988\n",
      "Epoch [1640/5000], Loss: 9525.3096\n",
      "Epoch [1650/5000], Loss: 9525.3193\n",
      "Epoch [1660/5000], Loss: 9525.3281\n",
      "Epoch [1670/5000], Loss: 9525.3379\n",
      "Epoch [1680/5000], Loss: 9525.3467\n",
      "Epoch [1690/5000], Loss: 9525.3564\n",
      "Epoch [1700/5000], Loss: 9525.3662\n",
      "Epoch [1710/5000], Loss: 9525.3760\n",
      "Epoch [1720/5000], Loss: 9525.3838\n",
      "Epoch [1730/5000], Loss: 9525.3936\n",
      "Epoch [1740/5000], Loss: 9525.4023\n",
      "Epoch [1750/5000], Loss: 9525.4111\n",
      "Epoch [1760/5000], Loss: 9525.4199\n",
      "Epoch [1770/5000], Loss: 9525.4307\n",
      "Epoch [1780/5000], Loss: 9525.4385\n",
      "Epoch [1790/5000], Loss: 9525.4502\n",
      "Epoch [1800/5000], Loss: 9525.4580\n",
      "Epoch [1810/5000], Loss: 9525.4668\n",
      "Epoch [1820/5000], Loss: 9525.4756\n",
      "Epoch [1830/5000], Loss: 9525.4854\n",
      "Epoch [1840/5000], Loss: 9525.4941\n",
      "Epoch [1850/5000], Loss: 9525.5039\n",
      "Epoch [1860/5000], Loss: 9525.5137\n",
      "Epoch [1870/5000], Loss: 9525.5215\n",
      "Epoch [1880/5000], Loss: 9525.5312\n",
      "Epoch [1890/5000], Loss: 9525.5391\n",
      "Epoch [1900/5000], Loss: 9525.5498\n",
      "Epoch [1910/5000], Loss: 9525.5586\n",
      "Epoch [1920/5000], Loss: 9525.5664\n",
      "Epoch [1930/5000], Loss: 9525.5762\n",
      "Epoch [1940/5000], Loss: 9525.5840\n",
      "Epoch [1950/5000], Loss: 9525.5938\n",
      "Epoch [1960/5000], Loss: 9525.6025\n",
      "Epoch [1970/5000], Loss: 9525.6113\n",
      "Epoch [1980/5000], Loss: 9525.6191\n",
      "Epoch [1990/5000], Loss: 9525.6289\n",
      "Epoch [2000/5000], Loss: 9525.6377\n",
      "Epoch [2010/5000], Loss: 9525.6475\n",
      "Epoch [2020/5000], Loss: 9525.6543\n",
      "Epoch [2030/5000], Loss: 9525.6650\n",
      "Epoch [2040/5000], Loss: 9525.6738\n",
      "Epoch [2050/5000], Loss: 9525.6826\n",
      "Epoch [2060/5000], Loss: 9525.6904\n",
      "Epoch [2070/5000], Loss: 9525.6992\n",
      "Epoch [2080/5000], Loss: 9525.7080\n",
      "Epoch [2090/5000], Loss: 9525.7178\n",
      "Epoch [2100/5000], Loss: 9525.7246\n",
      "Epoch [2110/5000], Loss: 9525.7334\n",
      "Epoch [2120/5000], Loss: 9525.7412\n",
      "Epoch [2130/5000], Loss: 9525.7520\n",
      "Epoch [2140/5000], Loss: 9525.7598\n",
      "Epoch [2150/5000], Loss: 9525.7686\n",
      "Epoch [2160/5000], Loss: 9525.7773\n",
      "Epoch [2170/5000], Loss: 9525.7852\n",
      "Epoch [2180/5000], Loss: 9525.7939\n",
      "Epoch [2190/5000], Loss: 9525.8018\n",
      "Epoch [2200/5000], Loss: 9525.8105\n",
      "Epoch [2210/5000], Loss: 9525.8193\n",
      "Epoch [2220/5000], Loss: 9525.8271\n",
      "Epoch [2230/5000], Loss: 9525.8350\n",
      "Epoch [2240/5000], Loss: 9525.8438\n",
      "Epoch [2250/5000], Loss: 9525.8525\n",
      "Epoch [2260/5000], Loss: 9525.8604\n",
      "Epoch [2270/5000], Loss: 9525.8691\n",
      "Epoch [2280/5000], Loss: 9525.8789\n",
      "Epoch [2290/5000], Loss: 9525.8857\n",
      "Epoch [2300/5000], Loss: 9525.8955\n",
      "Epoch [2310/5000], Loss: 9525.9033\n",
      "Epoch [2320/5000], Loss: 9525.9111\n",
      "Epoch [2330/5000], Loss: 9525.9189\n",
      "Epoch [2340/5000], Loss: 9525.9268\n",
      "Epoch [2350/5000], Loss: 9525.9346\n",
      "Epoch [2360/5000], Loss: 9525.9434\n",
      "Epoch [2370/5000], Loss: 9525.9521\n",
      "Epoch [2380/5000], Loss: 9525.9600\n",
      "Epoch [2390/5000], Loss: 9525.9678\n",
      "Epoch [2400/5000], Loss: 9525.9766\n",
      "Epoch [2410/5000], Loss: 9525.9854\n",
      "Epoch [2420/5000], Loss: 9525.9932\n",
      "Epoch [2430/5000], Loss: 9526.0010\n",
      "Epoch [2440/5000], Loss: 9526.0088\n",
      "Epoch [2450/5000], Loss: 9526.0176\n",
      "Epoch [2460/5000], Loss: 9526.0254\n",
      "Epoch [2470/5000], Loss: 9526.0322\n",
      "Epoch [2480/5000], Loss: 9526.0410\n",
      "Epoch [2490/5000], Loss: 9526.0488\n",
      "Epoch [2500/5000], Loss: 9526.0566\n",
      "Epoch [2510/5000], Loss: 9526.0645\n",
      "Epoch [2520/5000], Loss: 9526.0723\n",
      "Epoch [2530/5000], Loss: 9526.0801\n",
      "Epoch [2540/5000], Loss: 9526.0889\n",
      "Epoch [2550/5000], Loss: 9526.0967\n",
      "Epoch [2560/5000], Loss: 9526.1055\n",
      "Epoch [2570/5000], Loss: 9526.1113\n",
      "Epoch [2580/5000], Loss: 9526.1191\n",
      "Epoch [2590/5000], Loss: 9526.1279\n",
      "Epoch [2600/5000], Loss: 9526.1357\n",
      "Epoch [2610/5000], Loss: 9526.1436\n",
      "Epoch [2620/5000], Loss: 9526.1514\n",
      "Epoch [2630/5000], Loss: 9526.1582\n",
      "Epoch [2640/5000], Loss: 9526.1660\n",
      "Epoch [2650/5000], Loss: 9526.1748\n",
      "Epoch [2660/5000], Loss: 9526.1826\n",
      "Epoch [2670/5000], Loss: 9526.1885\n",
      "Epoch [2680/5000], Loss: 9526.1973\n",
      "Epoch [2690/5000], Loss: 9526.2041\n",
      "Epoch [2700/5000], Loss: 9526.2109\n",
      "Epoch [2710/5000], Loss: 9526.2197\n",
      "Epoch [2720/5000], Loss: 9526.2275\n",
      "Epoch [2730/5000], Loss: 9526.2354\n",
      "Epoch [2740/5000], Loss: 9526.2432\n",
      "Epoch [2750/5000], Loss: 9526.2500\n",
      "Epoch [2760/5000], Loss: 9526.2578\n",
      "Epoch [2770/5000], Loss: 9526.2646\n",
      "Epoch [2780/5000], Loss: 9526.2734\n",
      "Epoch [2790/5000], Loss: 9526.2803\n",
      "Epoch [2800/5000], Loss: 9526.2881\n",
      "Epoch [2810/5000], Loss: 9526.2949\n",
      "Epoch [2820/5000], Loss: 9526.3027\n",
      "Epoch [2830/5000], Loss: 9526.3115\n",
      "Epoch [2840/5000], Loss: 9526.3164\n",
      "Epoch [2850/5000], Loss: 9526.3252\n",
      "Epoch [2860/5000], Loss: 9526.3330\n",
      "Epoch [2870/5000], Loss: 9526.3398\n",
      "Epoch [2880/5000], Loss: 9526.3467\n",
      "Epoch [2890/5000], Loss: 9526.3535\n",
      "Epoch [2900/5000], Loss: 9526.3604\n",
      "Epoch [2910/5000], Loss: 9526.3691\n",
      "Epoch [2920/5000], Loss: 9526.3760\n",
      "Epoch [2930/5000], Loss: 9526.3828\n",
      "Epoch [2940/5000], Loss: 9526.3906\n",
      "Epoch [2950/5000], Loss: 9526.3975\n",
      "Epoch [2960/5000], Loss: 9526.4043\n",
      "Epoch [2970/5000], Loss: 9526.4131\n",
      "Epoch [2980/5000], Loss: 9526.4180\n",
      "Epoch [2990/5000], Loss: 9526.4258\n",
      "Epoch [3000/5000], Loss: 9526.4346\n",
      "Epoch [3010/5000], Loss: 9526.4414\n",
      "Epoch [3020/5000], Loss: 9526.4482\n",
      "Epoch [3030/5000], Loss: 9526.4561\n",
      "Epoch [3040/5000], Loss: 9526.4619\n",
      "Epoch [3050/5000], Loss: 9526.4688\n",
      "Epoch [3060/5000], Loss: 9526.4756\n",
      "Epoch [3070/5000], Loss: 9526.4834\n",
      "Epoch [3080/5000], Loss: 9526.4912\n",
      "Epoch [3090/5000], Loss: 9526.4980\n",
      "Epoch [3100/5000], Loss: 9526.5039\n",
      "Epoch [3110/5000], Loss: 9526.5107\n",
      "Epoch [3120/5000], Loss: 9526.5186\n",
      "Epoch [3130/5000], Loss: 9526.5244\n",
      "Epoch [3140/5000], Loss: 9526.5332\n",
      "Epoch [3150/5000], Loss: 9526.5391\n",
      "Epoch [3160/5000], Loss: 9526.5459\n",
      "Epoch [3170/5000], Loss: 9526.5518\n",
      "Epoch [3180/5000], Loss: 9526.5605\n",
      "Epoch [3190/5000], Loss: 9526.5654\n",
      "Epoch [3200/5000], Loss: 9526.5732\n",
      "Epoch [3210/5000], Loss: 9526.5791\n",
      "Epoch [3220/5000], Loss: 9526.5869\n",
      "Epoch [3230/5000], Loss: 9526.5938\n",
      "Epoch [3240/5000], Loss: 9526.6006\n",
      "Epoch [3250/5000], Loss: 9526.6084\n",
      "Epoch [3260/5000], Loss: 9526.6143\n",
      "Epoch [3270/5000], Loss: 9526.6211\n",
      "Epoch [3280/5000], Loss: 9526.6279\n",
      "Epoch [3290/5000], Loss: 9526.6348\n",
      "Epoch [3300/5000], Loss: 9526.6416\n",
      "Epoch [3310/5000], Loss: 9526.6484\n",
      "Epoch [3320/5000], Loss: 9526.6553\n",
      "Epoch [3330/5000], Loss: 9526.6611\n",
      "Epoch [3340/5000], Loss: 9526.6680\n",
      "Epoch [3350/5000], Loss: 9526.6748\n",
      "Epoch [3360/5000], Loss: 9526.6816\n",
      "Epoch [3370/5000], Loss: 9526.6875\n",
      "Epoch [3380/5000], Loss: 9526.6943\n",
      "Epoch [3390/5000], Loss: 9526.7012\n",
      "Epoch [3400/5000], Loss: 9526.7090\n",
      "Epoch [3410/5000], Loss: 9526.7148\n",
      "Epoch [3420/5000], Loss: 9526.7197\n",
      "Epoch [3430/5000], Loss: 9526.7285\n",
      "Epoch [3440/5000], Loss: 9526.7334\n",
      "Epoch [3450/5000], Loss: 9526.7402\n",
      "Epoch [3460/5000], Loss: 9526.7451\n",
      "Epoch [3470/5000], Loss: 9526.7529\n",
      "Epoch [3480/5000], Loss: 9526.7598\n",
      "Epoch [3490/5000], Loss: 9526.7666\n",
      "Epoch [3500/5000], Loss: 9526.7725\n",
      "Epoch [3510/5000], Loss: 9526.7793\n",
      "Epoch [3520/5000], Loss: 9526.7842\n",
      "Epoch [3530/5000], Loss: 9526.7910\n",
      "Epoch [3540/5000], Loss: 9526.7969\n",
      "Epoch [3550/5000], Loss: 9526.8047\n",
      "Epoch [3560/5000], Loss: 9526.8105\n",
      "Epoch [3570/5000], Loss: 9526.8174\n",
      "Epoch [3580/5000], Loss: 9526.8242\n",
      "Epoch [3590/5000], Loss: 9526.8301\n",
      "Epoch [3600/5000], Loss: 9526.8369\n",
      "Epoch [3610/5000], Loss: 9526.8428\n",
      "Epoch [3620/5000], Loss: 9526.8486\n",
      "Epoch [3630/5000], Loss: 9526.8545\n",
      "Epoch [3640/5000], Loss: 9526.8604\n",
      "Epoch [3650/5000], Loss: 9526.8672\n",
      "Epoch [3660/5000], Loss: 9526.8740\n",
      "Epoch [3670/5000], Loss: 9526.8809\n",
      "Epoch [3680/5000], Loss: 9526.8857\n",
      "Epoch [3690/5000], Loss: 9526.8926\n",
      "Epoch [3700/5000], Loss: 9526.8994\n",
      "Epoch [3710/5000], Loss: 9526.9053\n",
      "Epoch [3720/5000], Loss: 9526.9111\n",
      "Epoch [3730/5000], Loss: 9526.9160\n",
      "Epoch [3740/5000], Loss: 9526.9238\n",
      "Epoch [3750/5000], Loss: 9526.9297\n",
      "Epoch [3760/5000], Loss: 9526.9355\n",
      "Epoch [3770/5000], Loss: 9526.9414\n",
      "Epoch [3780/5000], Loss: 9526.9482\n",
      "Epoch [3790/5000], Loss: 9526.9551\n",
      "Epoch [3800/5000], Loss: 9526.9600\n",
      "Epoch [3810/5000], Loss: 9526.9668\n",
      "Epoch [3820/5000], Loss: 9526.9717\n",
      "Epoch [3830/5000], Loss: 9526.9785\n",
      "Epoch [3840/5000], Loss: 9526.9834\n",
      "Epoch [3850/5000], Loss: 9526.9902\n",
      "Epoch [3860/5000], Loss: 9526.9961\n",
      "Epoch [3870/5000], Loss: 9527.0020\n",
      "Epoch [3880/5000], Loss: 9527.0078\n",
      "Epoch [3890/5000], Loss: 9527.0146\n",
      "Epoch [3900/5000], Loss: 9527.0195\n",
      "Epoch [3910/5000], Loss: 9527.0244\n",
      "Epoch [3920/5000], Loss: 9527.0312\n",
      "Epoch [3930/5000], Loss: 9527.0381\n",
      "Epoch [3940/5000], Loss: 9527.0439\n",
      "Epoch [3950/5000], Loss: 9527.0488\n",
      "Epoch [3960/5000], Loss: 9527.0557\n",
      "Epoch [3970/5000], Loss: 9527.0615\n",
      "Epoch [3980/5000], Loss: 9527.0664\n",
      "Epoch [3990/5000], Loss: 9527.0723\n",
      "Epoch [4000/5000], Loss: 9527.0791\n",
      "Epoch [4010/5000], Loss: 9527.0850\n",
      "Epoch [4020/5000], Loss: 9527.0898\n",
      "Epoch [4030/5000], Loss: 9527.0967\n",
      "Epoch [4040/5000], Loss: 9527.1016\n",
      "Epoch [4050/5000], Loss: 9527.1074\n",
      "Epoch [4060/5000], Loss: 9527.1143\n",
      "Epoch [4070/5000], Loss: 9527.1191\n",
      "Epoch [4080/5000], Loss: 9527.1250\n",
      "Epoch [4090/5000], Loss: 9527.1309\n",
      "Epoch [4100/5000], Loss: 9527.1367\n",
      "Epoch [4110/5000], Loss: 9527.1426\n",
      "Epoch [4120/5000], Loss: 9527.1484\n",
      "Epoch [4130/5000], Loss: 9527.1533\n",
      "Epoch [4140/5000], Loss: 9527.1592\n",
      "Epoch [4150/5000], Loss: 9527.1650\n",
      "Epoch [4160/5000], Loss: 9527.1709\n",
      "Epoch [4170/5000], Loss: 9527.1758\n",
      "Epoch [4180/5000], Loss: 9527.1807\n",
      "Epoch [4190/5000], Loss: 9527.1875\n",
      "Epoch [4200/5000], Loss: 9527.1924\n",
      "Epoch [4210/5000], Loss: 9527.1992\n",
      "Epoch [4220/5000], Loss: 9527.2051\n",
      "Epoch [4230/5000], Loss: 9527.2100\n",
      "Epoch [4240/5000], Loss: 9527.2158\n",
      "Epoch [4250/5000], Loss: 9527.2227\n",
      "Epoch [4260/5000], Loss: 9527.2266\n",
      "Epoch [4270/5000], Loss: 9527.2324\n",
      "Epoch [4280/5000], Loss: 9527.2373\n",
      "Epoch [4290/5000], Loss: 9527.2432\n",
      "Epoch [4300/5000], Loss: 9527.2490\n",
      "Epoch [4310/5000], Loss: 9527.2549\n",
      "Epoch [4320/5000], Loss: 9527.2598\n",
      "Epoch [4330/5000], Loss: 9527.2666\n",
      "Epoch [4340/5000], Loss: 9527.2715\n",
      "Epoch [4350/5000], Loss: 9527.2764\n",
      "Epoch [4360/5000], Loss: 9527.2812\n",
      "Epoch [4370/5000], Loss: 9527.2881\n",
      "Epoch [4380/5000], Loss: 9527.2930\n",
      "Epoch [4390/5000], Loss: 9527.2979\n",
      "Epoch [4400/5000], Loss: 9527.3037\n",
      "Epoch [4410/5000], Loss: 9527.3086\n",
      "Epoch [4420/5000], Loss: 9527.3164\n",
      "Epoch [4430/5000], Loss: 9527.3203\n",
      "Epoch [4440/5000], Loss: 9527.3252\n",
      "Epoch [4450/5000], Loss: 9527.3311\n",
      "Epoch [4460/5000], Loss: 9527.3369\n",
      "Epoch [4470/5000], Loss: 9527.3418\n",
      "Epoch [4480/5000], Loss: 9527.3467\n",
      "Epoch [4490/5000], Loss: 9527.3516\n",
      "Epoch [4500/5000], Loss: 9527.3564\n",
      "Epoch [4510/5000], Loss: 9527.3623\n",
      "Epoch [4520/5000], Loss: 9527.3682\n",
      "Epoch [4530/5000], Loss: 9527.3730\n",
      "Epoch [4540/5000], Loss: 9527.3770\n",
      "Epoch [4550/5000], Loss: 9527.3838\n",
      "Epoch [4560/5000], Loss: 9527.3887\n",
      "Epoch [4570/5000], Loss: 9527.3936\n",
      "Epoch [4580/5000], Loss: 9527.3994\n",
      "Epoch [4590/5000], Loss: 9527.4043\n",
      "Epoch [4600/5000], Loss: 9527.4092\n",
      "Epoch [4610/5000], Loss: 9527.4160\n",
      "Epoch [4620/5000], Loss: 9527.4209\n",
      "Epoch [4630/5000], Loss: 9527.4248\n",
      "Epoch [4640/5000], Loss: 9527.4316\n",
      "Epoch [4650/5000], Loss: 9527.4355\n",
      "Epoch [4660/5000], Loss: 9527.4414\n",
      "Epoch [4670/5000], Loss: 9527.4473\n",
      "Epoch [4680/5000], Loss: 9527.4512\n",
      "Epoch [4690/5000], Loss: 9527.4570\n",
      "Epoch [4700/5000], Loss: 9527.4609\n",
      "Epoch [4710/5000], Loss: 9527.4668\n",
      "Epoch [4720/5000], Loss: 9527.4727\n",
      "Epoch [4730/5000], Loss: 9527.4756\n",
      "Epoch [4740/5000], Loss: 9527.4814\n",
      "Epoch [4750/5000], Loss: 9527.4873\n",
      "Epoch [4760/5000], Loss: 9527.4932\n",
      "Epoch [4770/5000], Loss: 9527.4980\n",
      "Epoch [4780/5000], Loss: 9527.5029\n",
      "Epoch [4790/5000], Loss: 9527.5078\n",
      "Epoch [4800/5000], Loss: 9527.5127\n",
      "Epoch [4810/5000], Loss: 9527.5176\n",
      "Epoch [4820/5000], Loss: 9527.5225\n",
      "Epoch [4830/5000], Loss: 9527.5283\n",
      "Epoch [4840/5000], Loss: 9527.5322\n",
      "Epoch [4850/5000], Loss: 9527.5381\n",
      "Epoch [4860/5000], Loss: 9527.5430\n",
      "Epoch [4870/5000], Loss: 9527.5469\n",
      "Epoch [4880/5000], Loss: 9527.5537\n",
      "Epoch [4890/5000], Loss: 9527.5586\n",
      "Epoch [4900/5000], Loss: 9527.5635\n",
      "Epoch [4910/5000], Loss: 9527.5684\n",
      "Epoch [4920/5000], Loss: 9527.5723\n",
      "Epoch [4930/5000], Loss: 9527.5781\n",
      "Epoch [4940/5000], Loss: 9527.5820\n",
      "Epoch [4950/5000], Loss: 9527.5869\n",
      "Epoch [4960/5000], Loss: 9527.5918\n",
      "Epoch [4970/5000], Loss: 9527.5967\n",
      "Epoch [4980/5000], Loss: 9527.6025\n",
      "Epoch [4990/5000], Loss: 9527.6074\n",
      "Epoch [5000/5000], Loss: 9527.6113\n",
      "Mean Absolute Error (MAE): 77.9267639192916\n",
      "Mean Squared Error (MSE): 9817.505358348242\n",
      "Root Mean Squared Error (RMSE): 99.08332532948337\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "from talib import RSI, MACD, STOCH, OBV, BBANDS\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Fetch historical data from Yahoo Finance\n",
    "ticker = 'TSLA'  # Tesla stock ticker\n",
    "start_date = '2010-01-01'  # Start date of historical data\n",
    "end_date = '2023-07-05'  # End date of historical data\n",
    "\n",
    "data = yf.download(ticker, start=start_date, end=end_date)\n",
    "\n",
    "# Calculate moving averages\n",
    "data['MA_5'] = data['Close'].rolling(window=5).mean()\n",
    "data['MA_10'] = data['Close'].rolling(window=10).mean()\n",
    "data['MA_20'] = data['Close'].rolling(window=20).mean()\n",
    "\n",
    "# Calculate RSI\n",
    "rsi_period = 14  # RSI calculation period\n",
    "data['RSI'] = RSI(data['Close'], timeperiod=rsi_period)\n",
    "\n",
    "# Calculate MACD\n",
    "macd, macd_signal, _ = MACD(data['Close'])\n",
    "data['MACD'] = macd\n",
    "\n",
    "# Calculate Stochastic Oscillator\n",
    "slowk, slowd = STOCH(data['High'], data['Low'], data['Close'])\n",
    "data['SlowK'] = slowk\n",
    "data['SlowD'] = slowd\n",
    "\n",
    "# Calculate On-Balance Volume (OBV)\n",
    "obv = OBV(data['Close'], data['Volume'])\n",
    "data['OBV'] = obv\n",
    "\n",
    "# Add additional indicators\n",
    "upper_band, middle_band, lower_band = BBANDS(data['Close'], timeperiod=20)\n",
    "data['UpperBand'] = upper_band\n",
    "data['MiddleBand'] = middle_band\n",
    "data['LowerBand'] = lower_band\n",
    "\n",
    "# Prepare the data\n",
    "X = data[['Open', 'Volume', 'MA_5', 'MA_10', 'MA_20', 'RSI', 'MACD', 'SlowK', 'SlowD', 'OBV', 'UpperBand', 'MiddleBand', 'LowerBand']].iloc[:-1]  # Adjusted for predicting next day's price\n",
    "y = data['Close'].shift(-1)[:-1]  # Adjusted for predicting next day's price\n",
    "\n",
    "# Store column names\n",
    "column_names = X.columns\n",
    "\n",
    "# Handle missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X = imputer.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train = torch.from_numpy(X_train).float()\n",
    "y_train = torch.from_numpy(y_train.values).float()\n",
    "X_test = torch.from_numpy(X_test).float()\n",
    "\n",
    "# Define the Neural Network class\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 16)\n",
    "        self.fc4 = nn.Linear(16, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        out = self.fc4(x)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# Instantiate the neural network\n",
    "input_size = X_train.shape[1]\n",
    "neural_network = NeuralNetwork(input_size)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(neural_network.parameters(), lr=0.01)\n",
    "\n",
    "# Train the neural network\n",
    "num_epochs = 5000\n",
    "batch_size = 32\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(0, X_train.size(0), batch_size):\n",
    "        inputs = X_train[i:i+batch_size]\n",
    "        labels = y_train[i:i+batch_size]\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = neural_network(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Make predictions using the trained neural network\n",
    "neural_network.eval()  # Switch to evaluation mode\n",
    "predictions = neural_network(X_test)\n",
    "\n",
    "# Convert predictions tensor to numpy array\n",
    "predictions = predictions.detach().numpy().flatten()\n",
    "\n",
    "# Evaluate the model\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "\n",
    "# Save the model\n",
    "torch.save(neural_network.state_dict(), 'neural_network_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
